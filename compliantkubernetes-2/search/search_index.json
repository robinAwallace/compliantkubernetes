{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Elastisys Compliant Kubernetes \u00b6 Like vanilla Kubernetes - but with security and observability built in. Elastisys Compliant Kubernetes is an open source, Certified Kubernetes distribution designed according to the ISO27001 controls: providing you with security tooling and observability from day one. Components of Elastisys Compliant Kubernetes \u00b6 How do I get started? \u00b6 Getting started guides: for application developers for Kubernetes administrators for CISOs (Chief Information Security Officers) Would you like to contribute? \u00b6 We want to build the next generation of cloud native technology where data security and privacy is the default setting. Join us on our mission as a contributor? Go to the guide for contributors .","title":"Overview"},{"location":"architecture/","text":"Architecture \u00b6 Below we present the architecture of Compliant Kubernetes, using the C4 model . For the nitty-gritty details, see Architectural Decision Records . Level 1: System Context \u00b6 Let us start with the system context. Compliance imposes restrictions on all levels of the tech stack. Your compliance focus should mostly lie on your application. Compliant Kubernetes ensures that the platform hosting your application is compliant. Finally, you need the whole software stack on a hardware that is managed in a compliant way, either via an ISO 27001-certified cloud provider or using on-prem hardware. Level 2: Clusters \u00b6 Most regulations require logging to a tamper-proof environment. This is usually interpreted as an attacker gaining access to your application should not be able to delete logs showing their attack and the harm caused by their attack. To achieve this, Compliant Kubernetes is implemented as two Kubernetes clusters A workload cluster , which hosts your application, and A service cluster , which hosts services for monitoring, logging and vulnerability management. Level 3: Individual Components \u00b6 Click on the diagram below to see the nuts-and-bolts of Compliant Kubernetes. Note Due to technical limitations, some compliance-related components still need to run in the workload cluster. These are visible when inspecting the workload cluster, for example, via the Kubernetes API . Currently, these components are: Falco, for intrusion detection; Prometheus, for collecting metrics; Fluentd, for collecting logs; OpenPolicyAgent, for enforcing Kubernetes API policies. Note that, the logs, metrics and alerts produced by these components are immediately pushed into the tamper-proof logging environment, hence this technical limitation does not weaken compliance. Level 3: Authentication \u00b6 Click on the diagram below to see the nuts-and-bolts of Compliant Kubernetes authentication. Level 3: Backup \u00b6 Click on the diagram below to see the nuts-and-bolts of Compliant Kubernetes backup. Level 3: Metrics and Metrics-based Alerting \u00b6 Level 3: Logs and Log-based Alerting \u00b6 Level 3: Access Control \u00b6","title":"Architecture"},{"location":"compliance/","text":"Compliance Basics \u00b6 Compliance will vary widely depending on: Jurisdiction (e.g., US vs. EU); Industry regulation (e.g., MedTech vs. FinTech); Company policies (e.g., log retention based on cost-risk analysis). The following is meant to offer an overview of compliance focusing on information security, and how Compliant Kubernetes reduces compliance burden. Click on the revelant blue text to find out more: Compliance: The Societal Perspective \u00b6 Organizations in certain sectors, such as BioTech, FinTech, MedTech, and those processing personal data, need public trust to operate. Such companies are allowed to handle sensitive data, create and destroy money, etc., in exchange for being compliant with certain regulations \u2014 in devtalk put, sticking to some rules set by regulators. For example: Any organization dealing with personal data is scrutinized by the Swedish Data Protection Authority (Datainspektionen) and needs to comply with GDPR. Any organization handling patient data needs to comply with Patientdatalagen (PDL) in Sweden. Such regulation is not only aspirational, but is actually checked as often as yearly by an external auditor. If an organization is found to be non-compliant it may pay heavy fines or even lose its license to operate. Compliance: The Engineering Perspective \u00b6 Translating legalese into code involves several steps. First a Compliance Officer will identify what regulations apply to the company. Based on those regulations, they will draft policies to clarify how the company should operate \u2014 i.e., run its daily business \u2014 in the most efficient manner while complying with regulations. To ensure the policies do not have gaps, are non-overlapping and consistent, they will generally follow an information security standard , such as ISO/IEC 27001. Such information security standards list a set of controls , i.e., \"points\" in the organization where a process and a check needs to be put in place. The resulting policies need to be interpreted and implemented by each department. Some of these can be supported by, or entirely implemented by, technology. Compliant Kubernetes includes software to do just that, and thus, Compliant Kubernetes addresses the needs of the infrastructure team. In essence, Compliant Kubernetes are carefully configured Kubernetes clusters together with other open-source components. They reduce compliance burden by allowing an organization to focus on making their processes and application compliant, knowing that the underlying platform is compliant. As far as getting certification, a key aspect is the ability to point to documentation that clearly states that your tech stack fulfils all stipulated requirements. By relying on Compliant Kubernetes, the majority of this work is already done for you.","title":"Compliance Basics"},{"location":"release-notes/","text":"Release Notes \u00b6 Compliant Kubernetes \u00b6 v0.25.0 - 2022-08-25 v0.24.1 - 2022-08-01 v0.24.0 - 2022-07-25 v0.23.0 - 2022-07-06 v0.22.0 - 2022-06-01 v0.21.0 - 2022-05-04 v0.20.0 - 2022-03-21 v0.19.1 - 2022-03-01 v0.19.0 - 2022-02-01 v0.18.2 - 2021-12-16 v0.17.2 - 2021-12-16 v0.18.1 - 2021-12-08 v0.17.1 - 2021-12-08 v0.18.0 - 2021-11-04 v0.17.0 - 2021-06-29 v0.16.0 - 2021-05-27 Note For a more detailed look check out the full changelog . Robin v4 \u00b6 iiiiiiii Robin v3 \u00b6 iiiii robin v2 \u00b6 robin v1 \u00b6 v0.25.0 \u00b6 Released 2022-08-25 Added \u00b6 Added Hierarchical Namespace Controller Allowing users to create and manage subnamespaces, namespaces within namespaces. You can read more about this in our FAQ . Added support for custom solvers in cluster issuers Allowing DNS01 challenges for certificate requests. Added support for running Harbor in High Availability Updated \u00b6 Updated cert-manager from v1.6.1 to v1.8.2 API versions v1alpha2 , v1alpha3 , and v1beta1 have been removed from the custom resource definitions (CRDs), certificate rotation policy will now be validated. See their changelog for more details. Updated OpenSearch with new usability improvements and features Checkout their launch announcement . Changed \u00b6 New additions to the Kubernetes cluster status Grafana dashboard It now shows information about resource requests and limits per node, and resource usage vs request per pod. v0.24.1 \u00b6 Released 2022-08-01 Required patch to be able to use release v0.24.0 Fixed \u00b6 Fixed a formatting issue with harbor s3 configuration. v0.24.0 \u00b6 Released 2022-07-25 Updated \u00b6 Upgraded Helm stack Upgrades for Helm, Helmfile and Helm-secrets. Image upgrade to node-local-dns Changed \u00b6 Improved stability to automatic node reboots Added \u00b6 Further configurability to ingress-nginx v0.23.0 \u00b6 Released 2022-07-06 Updated \u00b6 Updated the ingress controller ingress-nginx to image version v1.2.1 You can find the changelog here . Changed \u00b6 Added support for accessing Alertmanager via port-forward Added \u00b6 Backups can now be encrypted before they are replicated to an off-site S3 service. Improved metrics and alerting for OpenSearch. Fixed \u00b6 The deployment of Dex is now properly configured to be HA, ensuring that the Dex instances are placed on different Kubernetes worker nodes. v0.22.0 \u00b6 Released 2022-06-01 Added \u00b6 Added support for Elastx and UpCloud! New 'Welcoming' dashboard in OpenSearch and Grafana. Users can now access public docs and different urls to the services provided by Compliant Kubernetes. Improved availability of metrics and alerting. Alertmanager now runs with two replicas by default, Prometheus can now be run in HA mode. Added Falco rules to reduce alerts for services in Compliant Kubernetes. Falco now alerts less on operations that are expected out of these services. Fixed \u00b6 Fixed a bug where users couldn't silence alerts when portforwarding to alertmanager. Improved logging stack and fixed a number of issues to ensure reliability. v0.21.0 \u00b6 Released 2022-05-04 Changed \u00b6 Users can now view ClusterIssuers. User admins can now add users to the ClusterRole user-view. This is done by adding users to the ClusterRoleBinding extra-user-view . User can now get ClusterIssuers. Ensured all CISO dashboards are available to users. All the grafana dashboards in our CISO docs are now available. Better stability for dex Dex now runs with two replicas and has been updated. Updated \u00b6 Image upgrades to reduce number of vulnerabilities Upgrades for fluentd, grafana, and harbor chartmuseum. v0.20.0 \u00b6 Released 2022-03-21 Added \u00b6 Added kured - Kubernetes Reboot Daemon. This enables automatic node reboots and security patching of the underlying base Operating System image, container runtime and Kubernetes cluster components. Added fluentd grafana dashboard and alerts. Added RBAC for admin users. Admin users can now list pods cluster wide and run the kubectl top command. Added containerd support for fluentd. Changed \u00b6 Added the new OPA policy. To disallow the latest image tag. Persist Dex state in Kubernetes. This ensure the JWT token received from an OpenID provider is valid even after security patching of Kubernetes cluster components. Add ingressClassName in ingresses where that configuration option is available. Thanos is now enabled by default. Updated \u00b6 Upgraded nginx-ingress helm chart to v4.0.17 This upgrades nginx-ingress to v1.1.1. When upgrading an ingressClass object called nginx will be installed, this class has been set as the default class in Kubernetes. Ingress-nginx has been configured to still handle existing ingress objects that do not specify any ingressClassName. Upgraded starboard-operator helm chart to v0.9.1 This is upgrading starboard-operator to v0.14.1 Removed \u00b6 Removed influxDB and dependent helm charts. v0.19.1 \u00b6 Released 2022-03-01 Fixed \u00b6 Fixed critical stability issue related to Prometheus rules being evaluated without metrics. v0.19.0 \u00b6 Released 2022-02-01 Added \u00b6 Added Thanos as a new metrics backend. Provides a much more efficient and reliable platform for long-term metrics, with the capabilities to keep metrics for much longer time periods than previously possible. InfluxDB will still be supported in this release. Added a new feature to enable off-site replication of backups. Synchronizes S3 buckets across regions or clouds to keep an off-site backup. Added a new feature to create and log into separate indices per namespace. Currently considered to be an alpha feature. Changed \u00b6 Replacing Open Distro for Elasticsearch with OpenSearch. In this release, since the Open Distro project has reached end of life , Elasticsearch is replaced with OpenSearch and Kibana with OpenSearch Dashboards. OpenSearch is a fully open source fork of Elasticsearch with a compatible API and familiar User Experience. Note that recent versions of official Elasticsearch clients and tools will not work with OpenSearch as they employ a product check, compatible versions can be found here . Enforcing OPA policies by default. Provides strict safeguards by default. Allowing viewers to inspect and temporarily edit panels in Grafana. Gives more insight to the metrics and data shown. Setting Fluentd to log the reason why when it can't push logs to OpenSearch. Updated \u00b6 Large number of application and service updates, keeping up to date with new security fixes and changes. v0.18.2 \u00b6 Released 2021-12-16. Changes: Updated Open Distro for Elasticsearch to 1.13.3 to mitigate CVE-2021-44228 & CVE-2021-45046 v0.17.2 \u00b6 Released 2021-12-16. Changes: Updated Open Distro for Elasticsearch to 1.13.3 to mitigate CVE-2021-44228 & CVE-2021-45046 v0.18.1 \u00b6 Released 2021-12-08. Changes: updated Grafana to 8.0.7 in order to fix CVE-2021-43798 v0.17.1 \u00b6 Released 2021-12-08. Changes: updated Grafana to 8.0.7 in order to fix CVE-2021-43798 v0.18.0 \u00b6 Released 2021-11-04. Changes: Ingress-nginx-controller has been updated from v0.28.0 to v0.49.3, bringing various updates. Additionally, the configuration option allow-snippet-annotations has been set to false to mitigate known security issue CVE-2021-25742 Fixes, minor version upgrades, improvements to resource requests and limits for applications, improvements to stability. v0.17.0 \u00b6 Released 2021-06-29. Changes: The dashboard tool Grafana has been updated to a new major version of 8.x.x. This introduces new features and fixes, as well as some possibly breaking changes. See their release notes for more information. The single-sign-on service Dex has been updated, bringing small changes and better consistency to the UI. Fixes, improvements to resource limits, resource usage, and stability. v0.16.0 \u00b6 Released 2021-05-27. Changes: The default retention values have been changed and streamlined for authlog* and other* . The former will be kept for a longer period of time while the latter for shorter, both have reduced sized according to their actual usage. Updates, fixes, and features to improve the security of the platform.","title":"Release notes"},{"location":"roadmap/","text":"Roadmap \u00b6 GPU support \u00b6 Support for GPU nodes in Compliant Kubernetes workload clusters, subject to GPU availability on supported cloud providers. Secrets management service \u00b6 Convenient solution to manage secrets for Compliant Kubernetes application developers, such as SOPS or Sealed Secrets. SAML support \u00b6 Support for SAML based Identity Providers (IDPs) as a complement to currently supported OpenID format. ArgoCD configation that adhere to security safeguards \u00b6 Locked down security profile for ArgoCD that adheres to Compliant Kubernetes security practices. By default, pull-based CD solutions such as ArgoCD (and Flux, for that matter) require too extensive security privileges. Additional dashboards \u00b6 User experience for Compliant Kubernetes operators, application developers, and CISOs will be continuously improved, including addition of single pane of glass dashboards that give overviews of all relevant services. Non-Goals \u00b6 Opinionated CI/CD \u00b6 Compliant Kubernetes can be used with a wide range of CI/CD pipelines, including traditional push-style tools and pull-style solutions such as GitOps operators. Compliant Kubernetes will not dictate the use of one specific CI/CD technology.","title":"Roadmap"},{"location":"vocabulary/","text":"Vocabulary \u00b6 This page introduces terminology used in the Compliant Kubernetes project. We assume that you are familiar with Kubernetes concepts . Administrator : A person or automation process (i.e., CI/CD pipeline) that creates, destoys, updates or otherwise maintains a Compliant Kubernetes installation. Control : \"Points\" in an organization that need a clear policy in order to comply with regulation. Regulation : Law or contractual requirements that an organization is required to follow to be allowed to operate. Operator : Software extension to Kubernetes (see Operator pattern ). Rarely used to mean \"administrator\". Service cluster : Kubernetes cluster that hosts monitoring, logging and technical vulnerability management components. These components are separated from the workload cluster to give an extra layer of security, as is required by some regulations. Workload cluster : Kubernetes cluster hosting the application that exposes end-user -- front-office or back-office -- functionality. User : A person or an automation process (i.e., CI/CD pipeline) that interacts with Compliant Kubernetes for the purpose of running and monitoring an application hosted by Compliant Kubernetes.","title":"Vocabulary"},{"location":"watch-demos/","text":"Watch Demos \u00b6 Redirecting to Youtube Playlist ... window.location.replace(\"https://www.youtube-nocookie.com/embed/videoseries?list=PL1Bzo6mJqRXkJZ5fSoItF2LcNZY4G_iqC&rel=0\");","title":"Watch Demos"},{"location":"adr/","tags":["ISO 27001 A.14.1.1"],"text":"Architectural Decision Log \u00b6 Mapping to ISO 27001 Controls \u00b6 A.14.1.1 \"Information security requirements analysis and specification\" What are architectural decisions? \u00b6 Architectural decisions are high-level technical decisions that affect most stakeholders, in particular Compliant Kubernetes developers, administrators and users. A non-exhaustive list of architectural decisions is as follows: adding or removing tools; adding or removing components; changing what component talks to what other component; major (in the SemVer sense) component upgrades. Architectural decisions should be taken as directions to follow for future development and not issues to be fixed immediately. What triggers an architectural decision? \u00b6 An architectural decision generally starts with one of the following: A new features was requested by product management. An improvement was requested by engineering management. A new risk was discovered, usually by the architect, but also by any stakeholder. A new technology was discovered, that may help with a new feature, an improvement or to mitigate a risk. How are architectural decisions captured? \u00b6 Architectural decisions are captured via Architectural Decision Records or the tech radar . Both are stored in Git, hence a decision log is also captured as part of the Git commit messages. How are architectural decisions taken? \u00b6 Architectural decisions need to mitigate the following information security risks: a component might not fulfill advertised expectations; a component might be abandoned; a component might change direction and deviate from expectations; a component might require a lot of (initial or ongoing) training; a component might not take security seriously; a component might change its license, prohibiting its reuse or making its use expensive. The Compliant Kubernetes architect is overall responsible for this risk. How are these risks mitigated? \u00b6 Before taking in any new component to Compliant Kubernetes, we investigate and evaluate them. We prefer components that are: community-driven open-source projects , to reduce the risk of a component becoming abandoned, changing its license or changing direction in the interest of a single entity; as far as possible, we choose CNCF projects (preferably graduated ones) or projects which are governed by at least 3 different entities; projects with a good security track record , to avoid unexpected security vulnerabilities or delays in fixing security vulnerabilities; as far as possible, we choose projects with a clear security disclosure process and a clear security announcement process; projects that are popular , both from a usage and contribution perspective; as far as possible, we choose projects featuring well-known users and many contributors; projects that rely on technologies that our team is already trained on , to reduce the risk of requiring a lot of (initial or ongoing) training; as far as possible, we choose projects that overlap with the projects already on our tech radar ; projects that are simple to install and manage , to reduce required training and burden on administrators. Often, it is not possible to fulfill the above criteria. In that case, we take the following mitigations: Architectural Decision Records include recommendations on training to be taken by administrators. Closed-source or \"as-a-Service\" alternatives are used, if they are easy to replace thanks to broad API compatibility or standardization. These mitigations may be relaxed for components that are part of alpha or beta features, as these features -- and required components -- can be removed at our discretion. ADRs \u00b6 This log lists the architectural decisions for Compliant Kubernetes. ADR-0000 - Use Markdown Architectural Decision Records ADR-0001 - Use Rook for Storage Orchestrator ADR-0002 - Use Kubespray for Cluster Life-cycle ADR-0003 - [Superseded by ADR-0019 ] Push Metrics via InfluxDB ADR-0004 - Plan for Usage without Wrapper Scripts ADR-0005 - Use Individual SSH Keys ADR-0006 - Use Standard Kubeconfig Mechanisms ADR-0007 - Make Monitoring Forwarders Storage Independent ADR-0008 - Use HostNetwork or LoadBalancer for Ingress ADR-0009 - Use ClusterIssuers for LetsEncrypt ADR-0010 - Run managed services in workload cluster ADR-0011 - Let upstream projects handle CRDs ADR-0012 - [Superseded by ADR-0017 ] Do not persist Dex ADR-0013 - Configure Alerts in On-call Management Tool (e.g., Opsgenie) ADR-0014 - Use bats for testing bash wrappers ADR-0015 - We believe in community-driven open source ADR-0016 - gid=0 is okay, but not by default ADR-0017 - Persist Dex ADR-0018 - Use Probe to Measure Uptime of Internal Compliant Kubernetes Services ADR-0019 - Push Metrics via Thanos ADR-0020 - Filter by cluster label then data source ADR-0021 - Default to TLS for performance-insensitive additional services ADR-0022 - Use Dedicated Nodes for Additional Services For new ADRs, please use template.md as basis. More information on MADR is available at https://adr.github.io/madr/ . General information about architectural decision records is available at https://adr.github.io/ . Index Regeneration \u00b6 Pre-requisites: Install npm Install adr-log Install make Run make -C docs/adr","title":"Architectural Decision Log"},{"location":"adr/0000-use-markdown-architectural-decision-records/","text":"Use Markdown Architectural Decision Records \u00b6 Context and Problem Statement \u00b6 We want to record architectural decisions made in this project. Which format and structure should these records follow? Considered Options \u00b6 MADR 2.1.2 \u2013 The Markdown Architectural Decision Records Formless \u2013 No conventions for file format and structure Decision Outcome \u00b6 Chosen option: \"MADR 2.1.2\", because We need to start somewhere, and it's better to have some format than no format. MADR seems to be good enough for our current needs.","title":"Use Markdown Architectural Decision Records"},{"location":"adr/0001-use-rook-storage-orchestrator/","text":"Use Rook for Storage Orchestrator \u00b6 Status: accepted Deciders: Cristian Klein, Lars Larsson, Pradyumna Kashyap, Daniel Harr, Viktor Forsberg, Fredrik Liv Date: 2020-11-16 Context and Problem Statement \u00b6 Compliant Kubernetes has the vision to reduce the compliance burden on multiple clouds (\"Multi-cloud. Open source. Compliant.\"). Many of the cloud providers we target do not have a storage provider or do not have a storage provider that integrates with Kubernetes. How should we support PersistentVolumeClaims in such cases? Decision Drivers \u00b6 Storage Orchestrator needs to be popular and well maintained, so that developer can focus on adding value on top of Kubernetes clusters. Storage Orchestrator needs to be easy to set up, easy to operate and battle-tested, so on-call administrators are not constantly woken up. Storage Orchestrator needs to have reasonable performance. (A local storage provider can deal with high-performance use-cases.) Considered Options \u00b6 Rook GlusterFS Longhorn NFS Storage Provider Decision Outcome \u00b6 Chosen option: \"Rook\", because it is CNCF graduated, hence it is most likely to drive development and adoption long-term. Prady tested it and showed it was easy to use. It supports Ceph as a backend, making it battle-tested. It has reasonable performance. Positive Consequences \u00b6 We no longer need to worry about cloud provider without native storage. Negative Consequences \u00b6 We need to deprecate our NFS storage provider. Some manual steps are required to set up partitions for Rook. These will be automated when the burden justifies it. Pros and Cons of the Options \u00b6 Longhorn \u00b6 Good, because it is a CNCF project. Good, because it is well integrated with Kubernetes. Bad, because it is not the most mature CNCF project in the storage class. Bad, because it was not easy to set up. GlusterFS \u00b6 Good, because it is battle-tested. Bad, because it is not as well integrated with Kubernetes as other projects. Bad, because it is not a CNCF project (driven by Red Hat). NFS Storage Provider \u00b6 Good, because we used it before and we have experience. Bad, because it is a non-redundant, snowflake, brittle solution.","title":"Use Rook for Storage Orchestrator"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/","text":"Use Kubespray for Cluster Life-cycle \u00b6 Status: accepted Deciders: Lars, Johan, Cristian, Emil, Viktor, Geoff, Ewnetu, Fredrik (potentially others who attended the architecture meeting, but I can't remember) Date: 2020-11-17 Context and Problem Statement \u00b6 Compliant Kubernetes promises: \"Multi-cloud. Open source. Compliant\". So far, we delivered on our multi-cloud promise by using our in-house ck8s-cluster implementation. This strategy feels unsustainable for two reasons: First, we don't have the resources to catch up and keep up with open source projects in the cluster life-cycle space. Second, we don't want to differentiate on how to set up vanilla Kubernetes cluster, i.e., lower in the Kubernetes stack. Rather we want to differentiate on services on top of vanilla Kubernetes clusters. Decision Drivers \u00b6 We want to differentiate on top of vanilla Kubernetes cluster. We want to be able to run Compliant Kubernetes on top of as many cloud providers as possible. We promise building on top of best-of-breeds open source projects. We want to reduce burden with developing and maintaining our in-house tooling for cluster life-cycle management. Considered Options \u00b6 Rancher kubeadm via in-house tools (ck8s-cluster) kubespray kops Decision Outcome \u00b6 We chose kubespray, because it is best aligned with our interests, both feature- and roadmap-wise. It has a large community and is expected to be well maintained in the future. It uses kubeadm for domain knowledge on how to set up Kubernetes clusters. Positive Consequences \u00b6 We learn how to use a widely-used tool for cluster lifecycle management. We support many cloud providers. We can differentiate on top of vanilla Kubernetes. Negative Consequences \u00b6 We need training on kubespray. We need to port our tooling and practices to kubespray. We need to port compliantkubernetes-apps to work on kubespray. Pros and Cons of the Options \u00b6 Rancher \u00b6 Good, because it provides cluster life-cycle management at scale. Bad, because it creates clusters in an opinionated way, which is insufficiently flexible for our needs. Bad, because it is not a community project, hence entails long-term licensing uncertainty. kubeadm via in-house tool (ck8s-cluster) \u00b6 Good, because we know it and we built it. Good, because it works well for current use-cases. Bad, because it entails a lot of effort to develop and maintain. Bad, because it is lagging behind feature-wise with other cluster life-cycle solutions. kops \u00b6 Good, because it integrates well with the underlying cloud provider (e.g., AWS). Bad, because it supports fewer cloud providers than kubespray. NOTE: In the future, we might want to support compliantkubernetes-apps on top of both kops and kubespray, but this does not seem to bring value just now.","title":"Use Kubespray for Cluster Life-cycle"},{"location":"adr/0003-push-metrics-via-influxdb/","text":"[Superseded by ADR-0019 ] Push Metrics via InfluxDB \u00b6 Status: superseded by ADR-0019 Deciders: Johan, Cristian, Viktor, Emil, Olle, Fredrik Date: 2020-11-19 Context and Problem Statement \u00b6 We want to support workload multi-tenancy, i.e., one service cluster -- hosting the tamper-proof logging environment -- and multiple workload clusters. Currently, the service cluster exposes two end-points for workload clusters: Dex, for authentication; Elastisearch, for pushing logs (append-only). Currently, the service cluster pulls metrics from the workload cluster. This makes it difficult to have multiple workload clusters connected to the same service cluster. Decision Drivers \u00b6 We want to support workload multi-tenancy. We want to untangle the life-cycle of the service cluster and workload cluster. The service cluster acts as a tamper-proof logging environment, hence it should be difficult to tamper with metrics from the workload cluster. Considered Options \u00b6 Service cluster exposes InfluxDB; workload cluster pushes metrics into InfluxDB. Migrate from InfluxDB to Thanos Migrate from InfluxDB to Cortex Decision Outcome \u00b6 We chose to push metrics from the workload cluster to the service cluster via InfluxDB, because it involves the least amount of effort and is sufficient for the current use-cases that we want to support. InfluxDB supports a writer role, which makes overwriting metrics difficult -- unfortunately, not impossible. Positive Consequences \u00b6 All of *.$opsDomain can point to the service cluster workers -- optionally fronted by a load-balancer -- which considerably simplifies setup. Multiple workload clusters can push metrics to the service cluster, which paves the path to workload multi-tenancy. The service cluster can be set up first, followed by one-or-more workload clusters. Workload clusters become more \"cattle\"-ish. Negative Consequences \u00b6 Existing Compliant Kubernetes clusters will need some manual migration steps, in particular changing the prometheus.$opsDomain DNS entry. The service cluster exposes yet another endpoint, which should only be available to workload clusters and not the Internet. HTTP authentication (over HTTPS) feels sufficient for now, but we need a follow-up decision on how to add another layer of protection to these endpoints. The workload clusters will have to properly label their metrics. Although not easy, metrics can be overwritten from the workload cluster. We will improve on this when (a) demand for closing this risk increases, (b) we re-evaluate long-term metrics storage. Pros and Cons of the Options \u00b6 Both Thanos and Cortex seems worthy projects to replace InfluxDB. At the time of this writing, they were both having CNCF Incubating status. The two projects feature a healthy collaboration and are likely to merge in the future. However, right now, migrating away from InfluxDB feels like it adds more cost than benefits. We will reevaluate this decision when InfluxDB is no longer sufficient for our needs.","title":"[Superseded by [ADR-0019](0019-push-metrics-via-thanos.md)] Push Metrics via InfluxDB"},{"location":"adr/0004-plan-for-usage-without-wrapper-scripts/","text":"Plan for Usage without Wrapper Scripts \u00b6 Status: accepted Deciders: Architecture Meeting Date: 2020-11-24 Context and Problem Statement \u00b6 We frequently write wrapper scripts. They bring the following value: They bind together several tools and make them work together as a whole, e.g., sops and kubectl . They encode domain knowledge and standard operating procedures, e.g., how to add a node, how a cluster should look like, where to find configuration files. They enforce best practices, e.g., encrypt secrets consumed or produced by tools. Unfortunately, wrapper scripts can also bring disadvantages: They make usages that are deviating from the \"good way\" difficult. They risk adding opacity and raise the adoption barrier. People used to the underlying tools may find it difficult to follow how those tools are invoked. They add overhead when adding new features or supporting new use-cases. They raise the learning curve, i.e., newcomers need to learn the wrapper scripts in addition to the underlying tools. Completely abstracting away the underlying tools is unlikely, due to the Law of Leaky Abstractions . Decision Drivers \u00b6 We want to make operations simple, predictable, resilient to human error and scalable. We want to have some predictability in how an environment is set up. We want to make Compliant Kubernetes flexible and agile. Considered Options \u00b6 On one extreme, we can enforce wrapper scripts as the only way forward. This would require significant investment, as these scripts would need to be very powerful and well documented. On the other extreme, we completely \"ban\" wrapper scripts. Decision Outcome \u00b6 We have chosen to keep wrapper scripts in general. However, they need to be written in a way that ensures that our artefacts (e.g., Terraform scripts, Ansible roles, Helmfiles and Helm Charts) are usable without wrapper scripts. Wrapper scripts should also be simple enough so they can be inspected and useful commands can be copy-pasted out. This ensures that said scripts do not need to be \"too\" powerful and \"too\" well documented, but at the same time they do brings the sought after value. This decision applies for new wrapper scripts. We will not rework old wrapper scripts. Positive Consequences \u00b6 The operations team can encode standard operating procedures and scale ways of working. Customer-facing developers can easily reuse artefacts for new use-cases, without significant development effort. Newcomers will (hopefully) find the right trade-off of barriers, depending on whether they are looking for flexibility or predictability. Negative Consequences \u00b6 There will be a constant temptation to do things outside wrapper scripts, which will complicated knowledge sharing, operations and support. When this becomes a significant issue, we will need to draft clear guidelines on what should belong in a wrapper scripts and what not.","title":"Plan for Usage without Wrapper Scripts"},{"location":"adr/0005-use-individual-ssh-keys/","text":"Use Individual SSH Keys \u00b6 Status: accepted Deciders: Cristian, Fredrik, Olle, Johan Date: 2021-01-28 Technical Story: Do not fiddle with the SSH key Create a process of how we should move to use personal SSH keys Context and Problem Statement \u00b6 Currently, we create per-cluster SSH key pairs, which are shared among administrators. This is problematic from an information security perspective for a few reasons: It reduces the auditability of various actions, e.g., who SSH-ed into the Kubernetes control plane Nodes. It makes credential management challenging, e.g., when onboarding/offboarding administrators. It makes credential rotation challenging, e.g., the new SSH key pair needs to be transmitted to all administrators. It encourages storing the SSH key pair without password protection. It makes it difficult to store SSH key pairs on an exfiltration-proof medium, such as a YubiKey. It violates the Principle of Least Astonishment. Decision Drivers \u00b6 We need to stick to information security best-practices. Considered Options \u00b6 Inject SSH keys via cloud-init. Manage SSH keys via an Ansible role. Decision Outcome \u00b6 We will manage SSH keys via an Ansible role, since it allows rotating/adding/deleting keys without rebooting nodes. Also, it caters to more environments, e.g., BYO-VMs and BYO-metal. The compliantkubernetes-kubespray project will make it easy to configure SSH keys. Bootstrapping \u00b6 The above decision raises a chicken-and-egg problem: Ansible needs SSH access to the nodes, but the SSH access is managed via Ansible. This issue is solved as follows. For cloud deployments, all Terraform providers support injecting at least one public SSH key via cloud-init: AWS Exoscale GCP OpenStack The administrator who creates the cluster bootstraps SSH access by providing their own public SSH key via cloud-init. Then, the Ansible role adds the public SSH keys of the other administrators. BYO-VM and BYO-metal deployments are handled similarly, except that the initial public SSH key is delivered by email/Slack to the VM/metal administrator. Recommendations to Operators \u00b6 Operators should devise procedures for onboarding and offboarding member of the on-call team, as well as rotating SSH keys. The public SSH keys of all on-call administrators could be stored in a repository in a single file with one key per line. The comment of the key should clearly identify the owner. Operator logs (be it stand-alone documents, git or GitOps-like repositories) should clearly list the SSH keys and identities of the administrators configured for each environment. Links \u00b6 ansible.posix.authorized_key Ansible Module","title":"Use Individual SSH Keys"},{"location":"adr/0006-use-standard-kubeconfig-mechanisms/","text":"Use Standard Kubeconfig Mechanisms \u00b6 Status: accepted Deciders: Compliant Kubernetes Architecture Meeing Date: 2021-02-02 Context and Problem Statement \u00b6 To increase adoption of Compliant Kubernetes, we were asked to observe the Principle of Least Astonishment . Currently, Compliant Kubernetes's handing of kubeconfig is astonishing. Most tools in the ecosystem use the standard KUBECONFIG environment variable and kubecontext implemented in the client-go library. These tools leave it up to the user to set KUBECONFIG or use the default ~/.kube/config . Similarly, there is a default kubecontext which can be overwritten via command-line. Tools that get cluster credentials generate a context related to the name of the cluster. Tools that behave as such include: gcloud container clusters get-credentials az aks get-credentials kops helmfile helm kubectl fluxctl Decision Drivers \u00b6 Compliant Kubernetes needs to observe the Principle of Least Astonishment. Compliant Kubernetes needs to be compatible with various \"underlying\" vanilla Kubernetes tools. Compliant Kubernetes needs to be usable with various tools \"on top\". Considered Options \u00b6 Current solution, i.e., scripts wrapping kubeconfigs in sops which then execute \"fixed\" commands, like helmfile , helm and kubectl . \"Lighter\" scripts wrapping and unwrapping kubeconfig, allowing administrators to run helmfile , helm and kubectl as the administrator sees fit. Use standard kubeconfig mechanism. Decision Outcome \u00b6 We chose using standard kubeconfig mechanism, because it improves integration both with tools \"below\" Compliant Kubernetas and \"on top\" of Compliant Kubernetes. Tools that produce Kubernetes contexts are expected to use an approach similar to kubectl config set-cluster , set-credentials and set-context . The name of the cluster, user and context should be derived from the name of the cluster. Tools that consume Kubernetes contexts are expected to use an approach similar to kubectl , helm or helmfile (see links below). Links \u00b6 Organizing Cluster Access Using kubeconfig Files kubectx / kubens","title":"Use Standard Kubeconfig Mechanisms"},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/","text":"Make Monitoring Forwarders Storage Independent \u00b6 Status: accepted Deciders: Axel, Cristian, Fredrik, Johan, Olle, Viktor Date: 2021-02-09 Context and Problem Statement \u00b6 In the context of this ADR, forwarders refers to any components that are necessary to forward monitoring information -- specifically traces, metrics and logs -- to some monitoring database. As of February 2021, Compliant Kubernetes employs two projects as forwarders: Prometheus for metrics forwarding; fluentd for log forwarding. Similarly, two projects are employed as monitoring databases: InfluxDB for metrics; Elasticsearch for logs. Overall, the monitoring system needs to be one order of magnitude more resilient than the monitored system. Forwarders improve the resilience of the monitoring system by providing buffering: In case the database is under maintenance or down, the buffer of the forwarders will ensure that no monitoring information is lost. Hence, forwarders are subject to the following tensions: More buffering implies storage, which make the forwarders vulnerable to storage outages (e.g., disk full, CSI hiccups); Less buffering implies higher risk of losing monitoring information when the database is under maintenance or down. Decision Drivers \u00b6 We want a robust monitoring system. We want to monitor the storage system. We want VM-template-based rendering of the workload cluster, which implies no cloud native storage integration. We want to make it easier to \"cleanup and start from a known good state\". We want to have self-healing and avoid manual actions after failure. We want to be able to find the root cause of an incident quickly. We want to run as many components non-root as possible and tightly integrate with securityContext . Considered Options \u00b6 Use underlying storage provider for increased buffering resilience ( current approach ). Use Local Persistent Volumes . Use emptyDir volumes. Use hostPath volumes. Decision Outcome \u00b6 Chosen option: emptyDir for Prometheus as forwarder, because it allows monitoring of the storage system in some cases (e.g. Rook) and can redeploy automatically after node failure. It also keeps the complexity down without much risk of data loss. Fluentd as forwarder is deployed via DaemonSet. Both, emptyDir and hostPath can be used. Positive Consequences \u00b6 We can monitor the storage system. Failure of the storage system does not affect monitoring forwarder. Forwarder can be easier deployed \"fresh\". Negative Consequences \u00b6 Buffered monitoring information is lost if node is lost. emptyDir can cause disk pressure. This can be handled by alerting on low disk space. Pros and Cons of the Options \u00b6 Use underlying storage provider \u00b6 Good, because the forwarder can be restarted on any node. Good, because the buffer can be large. Good, because no buffered monitoring information is lost if a node goes down. Good, because buffered monitoring information is preserved if the forwarder is redeployed. Bad, because non-node-local storage is generally slower. Note, however, that at least SafeSpring and CityCloud use a central Ceph storage cluster for the VM's boot disk, which wipes out node-local's storage advantage.) Bad, because the forwarder will fail if storage provider goes down. This is especially problematic for Exoscale, bare-metal and BYO-VMs. Bad, because the forwarder cannot monitor the storage provider (circular dependency). Bad, because setting right ownership requires init containers or alpha features . Use Local Persistent Volumes \u00b6 Bad, because the forwarder cannot be restarted on any node without manual action: \"if a node becomes unhealthy, then the local volume becomes inaccessible by the pod. The pod using this volume is unable to run.\". Bad, because the amount of forwarding depends on the node's local disk size. Bad, because buffered monitoring information is lost if the forwarder's node goes down. Good, because buffered monitoring information is preserved if the forwarder is redeployed. Good, because node-local storage is generally faster. Good, because the forwarder will survive failure of storage provider. Good, because the forwarder can monitor the storage provider (no circular dependency). Bad, because local persistent storage requires an additional configuration step. Bad, because setting right ownership requires init containers or alpha features . Use emptyDir \u00b6 Good, because the forwarder can be restarted on any node without manual action. Bad, because the amount of forwarding depends on the node's local disk size. Bad, because buffered monitoring information is lost if the forwarder's node goes down. Bad, because buffered monitoring information is lost if the forwarder is (not carefully enough) redeployed. Good, because node-local storage is generally faster. Good, because the forwarder will survive failure of storage provider. Good, because the forwarder can monitor the storage provider (no circular dependency). Good, because works out of the box. Good, because it integrates nicely with securityContext . Use hostPath \u00b6 Similar to Local Persistent Volumes, but Worse, because if the forwarder is redeployed on a new node, buffering information may appear/disappear. Better, because it requires no extra storage provider configuration. Links \u00b6 Prometheus Operator Storage","title":"Make Monitoring Forwarders Storage Independent"},{"location":"adr/0008-use-hostnetwork-or-loadbalancer-for-ingress/","text":"Use HostNetwork or LoadBalancer for Ingress \u00b6 Status: accepted Deciders: Axel, Cristian, Fredrik, Johan, Olle, Viktor Date: 2021-02-09 Technical Story: Ingress configuration Context and Problem Statement \u00b6 Many regulations require traffic to be encrypted over public Internet. Compliant Kubernetes solves this problem via an Ingress controller and cert-manager . As of February 2021, Compliant Kubernetes comes by default with nginx-ingress , but Ambassador is planned as an alternative. The question is, how does traffic arrive at the Ingress controller? Decision Drivers \u00b6 We want to obey the Principle of Least Astonishment . We want to cater to hybrid cloud deployments, including bare-metal ones, which might lack support for Kubernetes-controlled load balancer . Some deployments, e.g., Bring-Your-Own VMs, might not allow integration with the underlying load balancer. We want to keep things simple. Considered Options \u00b6 Via the host network , i.e., some workers expose the Ingress controller on their port 80 and 443. Over a NodePort service , i.e., kube-proxy exposes the Ingress controller on a port between 30000-32767 on each worker. As a Service Type LoadBalancer , i.e., above plus Kubernetes provisions a load balancer via Service controller . Decision Outcome \u00b6 Chosen options: Use host network if Kubernetes-controlled load balancer is unavailable or undesired. If necessary, front the worker nodes with a manual or Terraform-controlled load-balancer. This includes: Where load-balancing does not add value, e.g., if a deployment is planned to have only a single-node or single-worker for the foreseeable future: Point the DNS entry to the worker IP instead. Exoscale currently falls in this category, due to its Kubernetes integration being rather recent. SafeSpring falls in this category, since it is missing load balancers. If the cloud provider is missing a storage controller, it might be undesirable to perform integration \"just\" for load-balancing. Use Service Type LoadBalancer when available. This includes: AWS, Azure, GCP and CityCloud. Additional considerations: This means that, generally, it will not be possible to set up the correct DNS entries until after we apply Compliant Kubernetes Apps. There is a risk for \"the Internet\" -- LetsEncrypt specifically -- to perform DNS lookups too soon and cause negative DNS caches with a long lifetime. Therefore, placeholder IP addresses must be used, e.g.: *.$BASE_DOMAIN 60s A 203.0.113.123 *.ops.$BASE_DOMAIN 60s A 203.0.113.123 203.0.113.123 is in TEST-NET-3 and okay to use as placeholder. This approach is inspired by kops and should not feel astonishing. Positive Consequences \u00b6 We make the best of each cloud provider. Obeys principle of least astonishment. We do not add a load balancer \"just because\". Negative Consequences \u00b6 Complexity is a bit increased, however, this feels like essential complexity. Links \u00b6 Cloud Controller Manager Ingress Nginx: Bare Metal Considerations","title":"Use HostNetwork or LoadBalancer for Ingress"},{"location":"adr/0009-use-cluster-issuers-for-letsencrypt/","text":"Use ClusterIssuers for LetsEncrypt \u00b6 Status: accepted Deciders: Cristian, Lennart Date: 2021-02-26 Technical Story: Make apps less fragile Context and Problem Statement \u00b6 Data protection regulations require encrypting network traffic over public networks, e.g., via HTTPS. This requires provisioning and rotating TLS certificates. To automate this task, we use the cert-manager , which automates provisioning and rotation of TLS certificates from Let's Encrypt . There are two ways to configure Let's Encrypt as an issuers for cert-manager: Issuer and ClusterIssuer . The former is namespaced, whereas the latter is cluster-wide. Should we use Issuer or ClusterIssuer? Decision Drivers \u00b6 We want to make compliantkubernetes-apps less fragile, and LetsEncrypt ratelimiting is a cause of fragility. We want to make it easy for users to get started with Compliant Kubernetes in a \"secure by default\" manner. We want to have a clear separation between user and administrator resources, responsibilities and privileges. We want to keep the option open for \"light\" renderings, i.e., a single Kubernetes clusters that hosts both service cluster and workload cluster components. Considered Options \u00b6 Use one Issuer per namespace; users need to install their own Issuers in the workload clusters. Use ClusterIssuer in service cluster; let users install Issuers in the workload clusters as required. Use ClusterIssuer in both service cluster and workload cluster(s). Decision Outcome \u00b6 Chosen option: \"Use ClusterIssuers in the service cluster; optionally enable ClusterIssuers in the workload cluster(s)\", because it reduces fragility, clarifies responsibilities, makes it easy to get started securely. Each cluster is configured with an optional ClusterIssuer called letsencrypt-prod for LetsEncrypt production and letsencrypt-staging for LetsEncrypt staging. The email address for the ClusterIssuers is configured by the administrator. Recommendations to Operators \u00b6 Direct LetsEncrypt emails to a \"logging\" mailbox \u00b6 Although LetsEncrypt does not require an email address, cert-managers seems to require all ClusterIssuers/Issuers to be configured with a syntactically valid email address. Said email address will receive notifications when certificates are close to expiry. Given that Compliant Kubernetes comes with Cryptography dashboards, these emails do not seem useful. Hence, ClusterIssuer emails should be directed to an address that has \"logging\" but not \"alerting\" status. Separate registered domains \u00b6 LetsEncrypt production has a rate limit of 50 certificates per week per registered domain . For example, if awesome-website.workload-cluster.environment.elastisys.se points to the workload cluster's Ingress controller, then an excessive creation and destruction of Ingress resources may trigger rate limiting for all of elastisys.se . It is therefore advisable to: Use separate registered domains for development and production environments. Use separate registered domains for workload cluster(s) and the service cluster, or restrict which Ingress resources can be created by the user. Note that, the rate limiting risk exists with both Issuers and ClusterIssuers and was not introduced by this ADR.","title":"Use ClusterIssuers for LetsEncrypt"},{"location":"adr/0010-run-managed-services-in-workload-cluster/","text":"Run managed services in workload cluster \u00b6 Status: proposed. This ADR did not reach consensus, with strong arguments on both sides. However, due to needing a decision in a timely manner, this ADR is actually followed. Therefore, this ADR serves both for visibility and to document a fait accompli . Deciders: Cristian Date: 2021-04-29 Context and Problem Statement \u00b6 To truly offer our users an option to run containerized workloads in EU jurisdiction, they also need additional managed services, like databases, message queues, caches, etc. Where should these run? Decision Drivers \u00b6 Some of these services are chatty and need low latency. Some of these services might assume trusted clients over a trusted network. We want to make it easy to run these services with regulatory compliance in mind, e.g., we should be able to reuse Compliant Kubernetes features around monitoring, logging, access control and network segregation. We want to make it difficult for Compliant Kubernetes users to negatively affect managed services. We want to keep support for multiple workload cluster, i.e., application multi-tenancy. Many cloud providers do not support Service Type LoadBalancer, which complicates exposing non-HTTP services outside a Kubernetes cluster. Service cluster might not exist in a future packaging of Compliant Kubernetes. Considered Options \u00b6 Run managed services in workload cluster Run managed services in service cluster Run managed services in yet another cluster Decision Outcome \u00b6 Chosen option: \"run managed services in workload cluster\". Positive Consequences \u00b6 Latency is minimized: The application consuming the managed service is close to the managed service, without needing to go through intermediate software components, such as a TCP Ingress controller. NetworkPolicies can be reused for communication segregation. OpenID and RBAC in the workload cluster can be reused for user access control. Kubernetes audit log can be re-used for auditing user access managed services. Such access is required, e.g., for manual database migrations and \"rare\" operations like GDPR data correction requests. Ease of exposition: No need for Service Type LoadBalancer, which is not supported on all cloud providers. Negative Consequences \u00b6 Blurs separation of responsibilities between user and administrator. The managed service is easier impacted by user misusage, e.g., bringing a Node into OOM. Workload cluster can no longer be deployed with \u201cfree for all\u201d security. Operators need to push and fight back against loose access control. Links \u00b6 Service Type LoadBalancer Exposing TCP and UDP services with ingress-nginx Redis Security","title":"Run managed services in workload cluster"},{"location":"adr/0011-let-upstream-projects-handle-crds/","text":"Let upstream projects handle CRDs \u00b6 Status: accepted Deciders: Compliant Kubernetes Arch Meeting Date: 2021-04-29 Technical Story: #446 #369 #391 #402 #436 . Context and Problem Statement \u00b6 CustomResourceDefinitions (CRDs) are tricky. They are essentially a mechanism to change the API of Kubernetes. Helm 2 had zero support for CRDs. Helm 3 has support for installing CRDs , but not upgrading them. How should we handle CRDs? Decision Drivers \u00b6 CRDs add complexity and need to be treated specially. Generally need to \u201ctrim fat\u201d and rely on upstream. Considered Options \u00b6 Install and upgrade CRDs as part of the bootstrap step, which is a Helm 2 legacy. Rely on whatever mechanism is proposed by upstream Helm Charts. Decision Outcome \u00b6 Chosen option: \"Rely on upstream\", because it trims fat and reduces astonishment. At installation, rely on upstream's approach to install CRDs (see below). At upgrade, propagate upstream migration steps in CK8s migration steps in each release notes. An issue template was created to ensure we won't forget. Since we \"vendor in\" all Charts, CRDs can be discovered using: grep -R 'kind: CustomResourceDefinition' Positive Consequences \u00b6 Less astonishing, compared to installing Chart \"by hand\". Less maintenance, i.e., there is only one source of truth for CRDs. Negative Consequences \u00b6 None really. Detailed Audit \u00b6 A detailed audit was performed of all CRDs in Compliant Kubernetes on 2021-04-27. As a summary, all projects encourage installing CRDs as part of standard helm install . Most projects encourage following manual migration steps to handle CRDs. Some projects handle CRD upgrades. A detailed analysis is listed below: cert-manager \u00b6 Installation: The cert-manager Helm Chart includes the installCRDs value -- by default it is set to false . If set to true , then CRDs are automatically installed when installing cert-manager, albeit not using the CRDs mechanism provided by Helm. Upgrade: CRDs are supposed to be upgraded manually . dex \u00b6 Dex can be configured without CRDs. ADR-0012 argues for that approach. gatekeeper \u00b6 Installation: Gatekeeper installs CRDs using the mechanism provided by Helm . Upgrade: Gatekeeper wants you to either uninstall-install or run a helm_migrate.sh . Prometheus (kube-prometheus-stack) \u00b6 Installation: kube-prometheus-stack installs CRDs using standard Helm mechanism . Upgrade: kube-prometheus-stack expects you to run manual upgrade steps . Velero \u00b6 Installation: Velero install CRDs using standard Helm mechanism . Upgrade: Velero includes magic to upgrade CRDs .","title":"Let upstream projects handle CRDs"},{"location":"adr/0012-do-not-persist-dex/","text":"[Superseded by ADR-0017 ] Do not persist Dex \u00b6 Status: superseded by ADR-0017 Deciders: Compliant Kubernetes Architecture Meeting Date: 2021-04-29 Technical Story: Reduce Helmfile concurrency for improved predictability Context and Problem Statement \u00b6 Dex requires persisting state to perform various tasks such as track refresh tokens, preventing replays, and rotating keys. What persistence option should we use? Decision Drivers \u00b6 CRDs add complexity Storage adds complexity Considered Options \u00b6 Use \"memory\" storage Use CRD-based storage Decision Outcome \u00b6 Chosen option: \"use memory\", because it simplified operations with little negative impact. Positive Consequences \u00b6 Dex brings no additional CRDs, which simplified upgrades. Dex brings no state, which simplified upgrades. Negative Consequences \u00b6 The authentication flow is disrupted, if Dex is rebooted exactly during an authentication flow. There is no user impact if Dex is restarted after the JWT was issued. Cristian tested this with kubectl and Grafana. Since we will only reboot Dex during maintenance windows, this is unlikely to be an issue in the foreseeable future. Other Considerations \u00b6 If Dex becomes a bottleneck and needs replication, or if we want to avoid disrupting authentication flows during operations on Dex, we will have to revisit this ADR.","title":"[Superseded by [ADR-0017](0017-persist-dex.md)] Do not persist Dex"},{"location":"adr/0013-configure-alerts-in-omt/","text":"Configure Alerts in On-call Management Tool (e.g., Opsgenie) \u00b6 Status: accepted Deciders: Compliant Kubernetes Architecture Meeting Date: 2021-06-03 Technical Story: See \"Investigate how to systematically work with alerts\" Context and Problem Statement \u00b6 Alerts are some noteworthy IT event, like a Node becoming un-ready, login failure or a disk getting full. Terminology differs across tooling and organizations, but one generally cares about: P1 (critical) alerts, which require immediate human attention -- the person on-call needs to be notified immediately -- and; P2 (high) alerts which require human attention with 24 hours -- the person on-call needs to be notified next morning; P3 (moderate) alerts which do not require immediate human attention, but should be regularly reviewed. Other priorities (e.g., P4 and below) are generally used for informational purposes. Dealing with alerts correctly entails prioritizing them (e.g., P1, P2, P3), deciding if someone should be notified, who should be notified, how they should be notified (e.g., SMS or email) and when. \"Who\", \"how\" and \"when\" should include escalation, if the previous notification was not acknowledged within a pre-configured time interval, then the same person if notified via a different channel or a new person is notified. Under-alerting -- e.g., notifying an on-call person too late -- may lead to Service Level Agreement (SLA) violations and a general feeling of administrator anxiety: \"Is everything okay, or is alerting not working?\". Over-alerting -- e.g., notifying a person too often about low-priority alerts -- leads to alert fatigue and \"crying wolf\" where even important alerts are eventually ignored. Hence, configuring the right level of alerting -- in particular notifications -- is extremely important both for SLA fulfillment and a happy on-call team. Where should alerting be configured, so as to quickly converge to the optimal alerting level? Decision Drivers \u00b6 Allow to quickly silence, un-silence and re-prioritize alerts. Allow arbitrary flexibility, e.g., who should be notified, when should notification happen, when should escalation happen, for what cluster and namespaces should notification happen, etc. Leverage existing tools and processes. Considered Options \u00b6 Configure alerting in Compliant Kubernetes, specifically alertmanager. Configure alerting in an On-call Management Tool (OMT), e.g., Opsgenie, PagerDuty. Decision Outcome \u00b6 Chosen option: Compliant Kubernetes \u201cover-alerts\u201d, i.e., forwards all alerts and all relevant information to an On-Call Management Tool (OMT, e.g., Opsgenie). Configuration of alerts happens in the OMT. Positive Consequences \u00b6 Clear separation of concerns. Alerting does not require per-customer configuration of Compliant Kubernetes. Leverages existing tools and processes. We do not need to implement complex alert filtering in Compliant Kubernetes, e.g., silence alerts during maintenance windows, silence alerts during Swedish holidays, etc. Negative Consequences \u00b6 Does not capture alerting know-how in Compliant Kubernetes. Migration to a new OMT means all alerting configuration needs to be migrated to the new tool. Fortunately, this can be done incrementally. Recommendations to Operators \u00b6 Operators should familiarize themselves with the capabilities of OMT, e.g., OpsGenie. This should be first done using a web UI, since that improves discoverability of such capabilities. When alerting configuration becomes too complex and/or repetitive, administrators should employ a configuration management tools, such as Terraform, to configure the OMT. Links \u00b6 Opsgenie documentation Alertmanager documentation Terraform Opsgenie provider Pulumni Opsgenie module","title":"Configure Alerts in On-call Management Tool (e.g., Opsgenie)"},{"location":"adr/0014-use-bats-for-testing-bash-wrappers/","text":"Use bats for testing bash wrappers \u00b6 Status: accepted Deciders: Compliant Kubernetes Architecture Meeting Date: 2021-06-03 Context and Problem Statement \u00b6 We write wrapper scripts for simpler and consistent operations. How should we test these scripts? Decision Drivers \u00b6 We want to use the best tools out there. We want to reduce tools sprawl, i.e., the collective cost (e.g., training) of adding a new tool should outweigh the collective benefit of the new tool. We want to make contributions inviting. Considered Options \u00b6 Do not test bash scripts. (We write perfect scripts 100% of the time, right? :smile:) Use alias for mocking, diff and test for assertions. Use bats Decision Outcome \u00b6 Chosen option: \"bats\", because the benefit of using a standard and rather light tool outweighs the cost of collective training on the new tool. Positive Consequences \u00b6 We use a pretty standard tool for testing in the bash universe. We do not risk re-inventing the while by writing our own wrappers around alias , diff and test . Negative Consequences \u00b6 We need to learn another tool, fortunately, it seems pretty light. Other Considerations \u00b6 Be very mindful about not overusing bash. Generally bash should only be used for things that you would do in the terminal, but got tired of copy-pasting, like: Running commands Copying files Setting environment variables Minor path translations For more advanced functionality prefer upstreaming into Ansible roles/libraries, Helm Charts, upstream source code, etc.","title":"Use bats for testing bash wrappers"},{"location":"adr/0015-we-believe-in-community-driven-open-source/","tags":["ISO 27001 A.17"],"text":"We believe in community-driven open source \u00b6 Status: accepted Deciders: Rob, Johan, Cristian (a.k.a., Product Management working group) Date: 2021-08-17 Context and Problem Statement \u00b6 We often get bombarded with questions like \"Why don't you use X?\" or \"Why don't you build on top of Y?\", sometimes preceded by \"product/project X already has feature Y\". Needless to say, this can cause a \"Simpsons Already Did It\" feeling. This ADR clarifies one of the core values of the Compliant Kubernetes project, namely our belief in community-driven open source. The ADR is useful to clarify both to internal and external stakeholders the choices we make. Decision Drivers \u00b6 We do not want to depend on the interests of any single company, be it small or large. Our customers need to have a business continuity plan, see ISO 27001, Annex A.17 . Therefore, we want to make it easy to \"exit\" Compliant Kubernetes and take over platform management. We want to use the best tools out there. Considered Options \u00b6 Prefer closed source solutions. Prefer single-company open source solutions. Prefer community-drive open source solutions. Decision Outcome \u00b6 Chosen option: \"prefer community-driven open source solutions\". Positive Consequences \u00b6 We do not depend on the interests of any single company. Our customers do not depend on the interests of any single company. Business continuity is significantly simplified for our customers. We have better chances at influencing projects in a direction that is useful to us and our customers. The smaller the project, the easier to influence. Negative Consequences \u00b6 Sometimes we might need to give up \"that cool new feature\" until the community-driven open source solution catches up with their closed source or single-company open source alternative. Alternatively, we might need to put extra time and effort to develop \"that cool new feature\" ourselves. As they are not bound by vendor liability -- e.g., end-of-life promises -- community-driven projects present a greater risk of being abandoned. The smaller the project, the higher the risk.","title":"We believe in community-driven open source"},{"location":"adr/0016-gid-0-is-okey-but-not-by-default/","text":"gid=0 is okay, but not by default \u00b6 Status: accepted Deciders: Cristian, Lars, Olle Date: 2021-08-23 Context and Problem Statement \u00b6 OpenShift likes to shift (pun intended) the UID -- i.e., assign arbitrary UIDs -- to containers. They do this as an additional security feature, given that OpenShift is a multi-tentant Kubernetes solution. Each OpenShift project received a non-overlapping UID range. Hence, in case an attacker escapes a container, it will be more difficult to interfere with other processes. However, this shifting of UIDs introduces an additional complexity: What if a process wants to write to the filesystem? What uid, gid and permissions should the files and folders have? To solve this problem, the OpenShift documentation (see \"Support arbitrary user ids\" ) recommends setting gid=0 on those files and folders. Specifically, the Dockerfiles of the container images should contain: RUN chgrp -R 0 /some/directory && chmod -R g = u /some/directory During execution, OpenShift assigns gid=0 as a supplementary group to containers, so as to give them access to the required files. In contrast to OpenShift, Compliant Kubernetes is not a multi-tenant solution. Given previous vulnerabilities in Kubernetes that affected tenant isolation (e.g., CVE-2020-8554 ), we believe that non-trusting users should not share a workload cluster. Hence, we do not assign arbitrary UIDs to containers and do not need to assign gid=0 as a supplementary group. The gid=0 practice above seems to have made its way in quite a few Dockerfiles , however, it is far from being the default outside OpenShift. What should Compliant Kubernetes do with the gid=0 practice? Decision Drivers \u00b6 For user expectations, we want to make it easy to start with Compliant Kubernetes. For better security and easier audits, we do not want to add unnecessary permissions. ID mapping in mounts has landed in Linux 5.12. Once this feature is used in container runtimes and Kubernetes, the gid=0 problem will go away. Considered Options \u00b6 Allow gid=0 by default. Disallow gid=0 by default -- this is what Kubespray does. Never allow gid=0 . Decision Outcome \u00b6 Chosen option: \"disallow gid=0 by default\". Enabling it on a case-by-case basis is okay. Positive Consequences \u00b6 We do not unnecessarily add a permission to containers. Negative Consequences \u00b6 Some users will complain about their container images not starting, and we will need to add a less restricted PodSecurityPolicy in their cluster. Other Considerations \u00b6 PodSecurityPolicies are deprecated in favor of PodSecurity Admission . This decision will have to be revisited once PodSecurity Admission is stable. In case we notice that the gid=0 practice is gaining significant uptake, we will have to revisit this decision to allow gid=0 by default. In case ID mapping is implemented in container runtimes and Kubernetes, this problem will likely go away. In that case, this decision might be revisited to never allow gid=0 .","title":"gid=0 is okay, but not by default"},{"location":"adr/0017-persist-dex/","text":"Persist Dex \u00b6 Status: accepted Deciders: Compliant Kubernetes Architecture Meeting Date: 2021-11-16 Technical Story: Enable Dex persistence Context and Problem Statement \u00b6 Dex requires persisting state to perform various tasks such as track refresh tokens, preventing replays, and rotating keys. What persistence option should we use? Decision Drivers \u00b6 CRDs add complexity Storage adds complexity We want to frequently reboot Nodes for security patching We want to deliver excellent user experience Considered Options \u00b6 Use \"memory\" storage Use CRD-based storage Decision Outcome \u00b6 Chosen option: \"use CRD-based storage\", because it improves user experience when Nodes are rebooted. With \"memory\" storage, Dex loses the OpenID keys when restarted, which leads to the user being forced to eventually re-login. Worst off, this forced re-login happens unexpectedly from the user's perspective, when the Kubernetes apiserver chooses to refresh the OpenID keys. Here is the experiment to illustrate the issue: $ curl https://dex. $DOMAIN /.well-known/openid-configuration > before-openid-configuration.json $ curl https://dex. $DOMAIN /keys > before-keys.json $ kubectl delete pods -n dex -l app.kubernetes.io/instance = dex $ curl https://dex. $DOMAIN /.well-known/openid-configuration > after-openid-configuration.json $ curl https://dex. $DOMAIN /keys > after-keys.json $ diff -y before-openid-configuration.json after-openid-configuration.json [empty output, no differences] $ diff -y before-keys.json after-keys.json [all keys are replaced] Positive Consequences \u00b6 Nodes which host Dex can be rebooted for security patching User experience is optimized Negative Consequences \u00b6 Dex will have a more permissions in the Service Cluster (see rbac.yaml ) We will need to closely monitor migration steps for Dex","title":"Persist Dex"},{"location":"adr/0018-use-probe-to-measure-internal-uptime/","text":"Use Probe to Measure Uptime of Internal Compliant Kubernetes Services \u00b6 Status: accepted Deciders: Cristian, Lucian, Ravi Date: 2021-11-25 Context and Problem Statement \u00b6 We need to measure uptime for at least two reasons: To serve as feedback on what needs to be improved next. To demonstrate compliance with our SLAs. How exactly should we measure uptime? Decision Drivers \u00b6 We want to reduce tools sprawl. We want to be mindful about capacity and infrastructure costs. We want to measure uptime as observed by a consumer -- i.e., application or user -- taking into account business continuity measures, such as redundancy, fail-over time, etc. Considered Options \u00b6 Blackbox exporter kubelet prober metrics Prometheus Operator Probe , which essentially wraps the Blackbox exporter in a Probe CustomResource. Decision Outcome \u00b6 Chosen option: \"use Probe for measuring uptime of internal Compliant Kubernetes services\", because it measures uptime as observed by a consumer. Although this requires a bit of extra capacity for running Blackbox, the costs are worth the benefits. Instead of configuring Blackbox directly, Probe is a cleaner abstraction provided by the Prometheus Operator. The following is an example for a Probe: apiVersion : monitoring.coreos.com/v1 kind : Probe metadata : name : google-is-up labels : probe : google release : kube-prometheus-stack spec : interval : 60s module : http_2xx prober : url : blackbox-prometheus-blackbox-exporter.monitoring.svc.cluster.local:9115 targets : staticConfig : static : - https://www.google.com This will generate a metric as follows: probe_success{cluster=\"ckdemo-wc\", instance=\"https://www.google.com\", job=\"probe/demo1/google-is-up\", namespace=\"demo1\"} . Positive Consequences \u00b6 We measure uptime as observed by a consumer. Increasing redundancy, reducing failure time, etc. will contribute positively to our uptime, as desired. Negative Consequences \u00b6 We don't currently run Blackbox in the workload cluster, so we'll need a bit of extra capacity. Recommendations to Operators \u00b6 Blackbox should only be used for measuring uptime of internal services, i.e., those that are only exposed within the Kubernetes cluster. Examples include additional services, such as PostgreSQL, Redis and RabbitMQ. For external endpoints -- specifically, Dex, Grafana, Kibana, Harbor and Ingress Controllers -- prefer using an external uptime service which integrates with an On-Call Management Tool, e.g., Uptime Cloud Monitor Integration for Opsgenie . External uptime measurement should achieve the similar effect as the commands below: curl --head https://dex.$DOMAIN/healthz curl --include https://harbor.$DOMAIN/api/v2.0/health curl --head https://grafana.$DOMAIN/healthz curl --head https://kibana.$DOMAIN/api/status curl --head some-domain.$DOMAIN/healthz # Pokes the WC Ingress Controller","title":"Use Probe to Measure Uptime of Internal Compliant Kubernetes Services"},{"location":"adr/0019-push-metrics-via-thanos/","text":"Push Metrics via Thanos \u00b6 Status: accepted Deciders: arch meeting Date: 2022-01-20 Context and Problem Statement \u00b6 Currently, the service cluster exposes several end-points for workload clusters: Dex, for authentication; OpenSearch, for pushing logs (append-only); InfluxDB, for pusing metrics; Harbor, for pulling container images. InfluxDB has served us really well over the years. However, as we enter a new era of growth, it no longer satisfies our needs. In particular: It is not community-driven (see ADR-0015 We believe in community-driven open source ). The open-source version cannot be run replicated, hence it is a single point of failure. It is rather capacity hungry, eating as much as 2 CPUs and 15 Gi in a standard package environment. It is unsuitable for long-term metrics storage, which we need -- among others -- for proper capacity management. We decided to migrate from InfluxDB to Thanos , which can both push and pull metrics. Shall we push or pull metrics using Thanos? Decision Drivers \u00b6 We want to support multiple workload clusters. We want to untangle the life-cycle of the service cluster and workload cluster. The service cluster acts as a tamper-proof logging environment, hence it should be difficult to tamper with metrics from the workload cluster. Considered Options \u00b6 Push metrics from workload cluster to service cluster. Pull metrics from workload cluster to service cluster. Decision Outcome \u00b6 We chose to push metrics from the workload cluster to the service cluster via via Thanos Receive , because it keeps the \"direction\" of metrics flow. Hence, we keep support for multiple workload clusters without any changes. At the time of this writing, pulling metrics via Thanos sidecar seems to be the preferred way to deploy Thanos. We will monitor the ecosystem and our needs, and -- if needed -- move to pulling metrics. At any rate, even if we end up with Thanos Sidecar, migrating in two steps -- first from InfluxDB to Thanos Receive, then to Thanos Sidecar -- feels less risky. Positive Consequences \u00b6 All of *.$opsDomain can point to the service cluster workers -- optionally fronted by a load-balancer -- which considerably simplifies setup. Multiple workload clusters can push metrics to the service cluster, which paves the path to workload multi-tenancy. The service cluster can be set up first, followed by one-or-more workload clusters. Workload clusters become more \"cattle\"-ish. Negative Consequences \u00b6 Metrics are less protected than in a pull architecture. E.g., compromising the workload cluster can easier be used to mount an attack against long-term metrics storage.","title":"Push Metrics via Thanos"},{"location":"adr/0020-filter-by-cluster-label-then-data-source/","text":"Filter by cluster label then data source \u00b6 Status: accepted Deciders: arch meeting Date: 2021-01-27 Technical Story: https://github.com/elastisys/compliantkubernetes-apps/issues/742 Context and Problem Statement \u00b6 Compliant Kubernetes allows multiple workload clusters to be connected to a single service cluster. This allows the metrics of multiple workload clusters to be inspected via the same dashboards. How should we organise metrics to allow users and admins to select for which clusters they want to see metrics? Decision Drivers \u00b6 We want to be able to see metrics for a single cluster, for multiple cluster, and even for all clusters. We want to be able to reuse upstream dashboards, and some are missing filters for the cluster variable. We want to stay flexible. Considered Options \u00b6 Use only the cluster label and expose a single data source. Expose multiple data sources and ignore the cluster label. Filter primarily by cluster label, but allow filtering by data source. Filter primarily by data source, but allow filtering by cluster label. Decision Outcome \u00b6 Chosen option: \"Filter primarily by cluster label, but allow filtering by data source\", because it fulfills the all decision drivers with little complexity. Prom-label-enforcer can be used to create multiple data sources from a single data store, discriminating by cluster label. To simplify Thanos configuration, we can also discriminate based on tenant_id , which will always contain the same value as cluster . In general, we will aim to fix dashboards missing the cluster variable upstream. However, by also providing filtering based on data source, we facilitate our users to reuse their dashboards, which might not be cluster-aware. Positive Consequences \u00b6 We support both dashboards with cluster filter and without We can enforce metrics multi-tenancy, i.e., map Grafana users/orgs to datasources, to filter some metrics out. Negative Consequences \u00b6 [Minor] We need to configure data sources in sc-config.yaml For example, if we forget to add the name of a workload cluster, the data source will be missing, but filtering based on cluster label is still possible. [Minor] Label enforcer uses a bit of resources. However, we already saved a lot by migrating from InfluxDB to Thanos, so we can afford go back a bit.","title":"Filter by cluster label then data source"},{"location":"adr/0021-tls-for-additional-services/","text":"Default to TLS for performance-insensitive additional services \u00b6 Status: accepted Deciders: arch meeting Date: 2022-02-16 Context and Problem Statement \u00b6 We run additional services in the workload cluster, currently databases (PostgreSQL), in-memory caches (Redis) and message queues (RabbitMQ). Traditionally, when these services are provided as managed services, they are exposed via a TLS-encrypted endpoint. See examples for: Redis -- notice rediss:// ; RabbitMQ ; PostgreSQL . In Compliant Kubernetes, the network is assumed trusted, either because we performed a provider audit or because we enabled Pod-to-Pod encryption via WireGuard . Hence, TLS does not improve data security. How should we expose additional services in Compliant Kubernetes? With or without TLS? Decision Drivers \u00b6 We want to stick to best practices and sane defaults. We want to make it easy to port applications to Compliant Kubernetes and its additional services. Some services are performance-sensitive: Redis suffers a significant performance drop with TLS . The Spotahome Redis Operator does not support TLS . Some services are performance-insensitive: PostgreSQL and RabbitMQ feature negligible performance impact with TLS. Considered Options \u00b6 Always disable TLS, since the network in Compliant Kubernetes is trusted. Always enable TLS. By default, enable TLS for performance-insensitive services and disable TLS for performance-sensitive services. Allow TLS to be disabled if the user requests it. Decision Outcome \u00b6 Chosen option: \"By default, enable TLS for performance-insensitive services and disable TLS for performance-sensitive services. Allow TLS to be disabled if the user requests it.\" Specifically: Never enable TLS for Redis: Performance impact is huge and the network is already trusted. Furthermore, the Spotahome Redis Operator does not support TLS . Enable TLS by default for PostgreSQL and RabbitMQ: Performance impact is negligible and most application are already configured for it. Allow TLS to be disabled if requested for PostgreSQL and RabbitMQ.","title":"Default to TLS for performance-insensitive additional services"},{"location":"adr/0022-use-dedicated-nodes-for-additional-services/","tags":["BSI IT-Grundschutz APP.4.4.A14"],"text":"Use Dedicated Nodes for Additional Services \u00b6 Status: accepted Deciders: arch meeting Date: 2022-03-03 Context and Problem Statement \u00b6 We run additional services in the workload cluster, currently databases (PostgreSQL), in-memory caches (Redis) and message queues (RabbitMQ). On what Nodes should they run? Decision Drivers \u00b6 We want to deliver a stable and secure platform. Considered Options \u00b6 Spread additional services on application Nodes. Run additional services on dedicated Nodes. Decision Outcome \u00b6 Chosen option: \"run additional service on dedicated Nodes\", because it improves the stability and security of the platform. Specifically, use the following Node labels elastisys.io/node-type=postgresql elastisys.io/node-type=redis elastisys.io/node-type=rabbitmq and taints: elastisys.io/node-type=postgresql:NoSchedule elastisys.io/node-type=redis:NoSchedule elastisys.io/node-type=rabbitmq:NoSchedule Important Dedicated Nodes still contain some workload cluster components for logging, monitoring, intrusion detection, etc., so not all their capacity is available to the service. Positive Consequences \u00b6 Performance is more predictable. Responsibility is more clearly separated, i.e., application Nodes vs. additional services Nodes. Security and stability of additional services is somewhat improved, e.g., SystemOOM due to an application won't impact PostgreSQL. Negative Consequences \u00b6 Forces additional services to be sized based on available Node sizes. While some commonality exists, Node sizes are specific to each infrastructure provider. Latency is somewhat increased. This is an issue mostly for Redis, as other services are a bit more latency tolerant. Recommendations to Operators \u00b6 For better application performance and security, run system Deployments and StatefulSets -- such as Ingress Controllers, Prometheus, Velero, Gatekeeper and Starboard -- onto dedicated Nodes. Specifically, use the following Node label: elastisys.io/node-type=elastisys and taint: elastisys.io/node-type=elastisys:NoSchedule Remember to also add tolerations and Node affinity to all affected Pods. Links \u00b6 Taints and Tolerations Well-Known Labels, Annotations and Taints Kubespray node_labels and node_taints","title":"Use Dedicated Nodes for Additional Services"},{"location":"adr/template/","text":"[short title of solved problem and solution] \u00b6 Status: [proposed | rejected | accepted | deprecated | \u2026 | superseded by ADR-0005 Deciders: [list everyone involved in the decision] Date: [YYYY-MM-DD when the decision was last updated] Technical Story: [description | ticket/issue URL] Context and Problem Statement \u00b6 [Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.] Decision Drivers \u00b6 [driver 1, e.g., a force, facing concern, \u2026] [driver 2, e.g., a force, facing concern, \u2026] \u2026 Considered Options \u00b6 [option 1] [option 2] [option 3] \u2026 Decision Outcome \u00b6 Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. Positive Consequences \u00b6 [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026] \u2026 Negative Consequences \u00b6 [e.g., compromising quality attribute, follow-up decisions required, \u2026] \u2026 Pros and Cons of the Options \u00b6 [option 1] \u00b6 [example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026 [option 2] \u00b6 [example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026 [option 3] \u00b6 [example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026 Links \u00b6 [Link type] [Link to ADR] \u2026","title":"[short title of solved problem and solution]"},{"location":"ciso-guide/","text":"CISO Guide Overview \u00b6 This guide is for the Chief Information Security Officer (CISO) who needs to prove to an internal or external auditor that the application runs on top of a compliant platform. The CISO can be described via the following user stories: As an information security officer, I want to audit the Compliant Kubernetes cluster, so as to comply with continuous compliance policies. As an information security officer, I want to quickly identify compliance violation and convert them into actionable tasks for developers. The CISO only needs: a modern browser (recent versions of Chrome, Firefox or Edge will do); the URL to the Compliant Kubernetes dashboard (usually https://grafana.example.com); credentials for the Compliant Kubernetes cluster. If in doubt, contact the Compliant Kubernetes administrator.","title":"Overview"},{"location":"ciso-guide/audit-logs/","tags":["ISO 27001 A.12.4.3"],"text":"Audit Logs \u00b6 Note This section helps you implement ISO 27001, specifically: A.12.4.3 Administrator & Operator Logs Compliant Kubernetes comes built in with audit logs, which can be accessed via OpenSearch Dashboard . The audit logs are stored in the kubeaudit* index pattern. The audit logs cover calls to the Kubernetes API, specifically who did what and when on which Kubernetes cluster. Thanks to integration with your Identity Provider (IdP), if who is a person, their email address will be shown. If who is a system -- e.g., a CI/CD pipeline -- the name of the ServiceAccount is recorded. Your change management or incident management process should ensure that you also cover why . Both users (application developers) and administrators will show in the audit log. The former will change resources related to their application, whereas the latter will change Compliant Kubernetes system components. Note It might be tempting to enable audit logging for \"everything\", e.g., service cluster Kubernetes API, Harbor, Grafana, Kibana, etc. Compliant Kubernetes takes a risk-reward approach and captures audit logs for the events that pose the highest risk to personal data. Don't forget that, at the end of the day, logs are only as useful as someone looks at them . SSH Access Logs \u00b6 Compliant Kubernetes also captures highly privileged SSH access to the worker Nodes in the authlog* index pattern. Only administrators should have such access. Note This section helps you implement ISO 27001, specifically: A.9.2.1 User Registration and Deregistration Many data protection regulation will require you to individually identify administrators , hence individual SSH keys. This allows you to individually identify administrators in the SSH access log. Audit Logs for Additional Services \u00b6 The Kubernetes Audit Logs capture user access to additional services, i.e., kubectl exec or kubectl port-forward commands. Additional services usually do not have audit logging enabled, since that generates a lot of log entries. Too often the extra bandwidth, storage capacity, performance loss comes with little benefit to data security. Prefer audit logs in your application to capture audit-worthy events , such as login, logout, patient record access, patient record change, etc. Resist the temptation to enable audit logging too \"low\" in the stack. Messages like \"Redis client connected\" are plenty and add little value to your data protection posture. Out of all additional services, audit logging for the database makes the most sense. It can be enabled via pgaudit . Make sure you discuss your auditing requirements with the service-specific administrator, to ensure you find the best risk-reduction-to-implementation-cost trade-off. Typically, you want to discuss: which databases and tables are audited: e.g., audit app.users , but not app.emailsSent ; what operations are audited: e.g., audit INSERT/UPDATE/DELETE , but not SELECT ; by which users: e.g., audit person access, but not application access. Further Reading \u00b6 Kubernetes Auditing pgaudit","title":"Audit Logs"},{"location":"ciso-guide/backup/","tags":["ISO 27001 A.12.3.1","ISO 27001 A.17.1.1","BSI IT-Grundschutz APP.4.4.A5"],"text":"Backup Dashboard \u00b6 Relevant Regulations \u00b6 GDPR \u00b6 GDPR Article 32 : In assessing the appropriate level of security account shall be taken in particular of the risks that are presented by processing, in particular from accidental or unlawful destruction, loss , alteration, unauthorised disclosure of, or access to personal data transmitted, stored or otherwise processed. [highlights added] Swedish Patient Data Law \u00b6 Note This regulation is only available in Swedish. To avoid confusion, we decided not to produce an unofficial translation. HSLF-FS 2016:40 : S\u00e4kerhetskopiering 12 \u00a7 V\u00e5rdgivaren ska s\u00e4kerst\u00e4lla att personuppgifter som behandlas i informationssystem s\u00e4kerhetskopieras med en fastst\u00e4lld periodicitet. S\u00e4kerhetskopiorna ska f\u00f6rvaras p\u00e5 ett s\u00e4kert s\u00e4tt, v\u00e4l \u00e5tskilda fr\u00e5n originaluppgifterna. 13 \u00a7 V\u00e5rdgivaren ska besluta om hur l\u00e4nge s\u00e4kerhetskopiorna ska sparas och hur ofta \u00e5terl\u00e4sningstester av kopiorna ska g\u00f6ras. Allm\u00e4nna r\u00e5d: Hur ofta \u00e5terl\u00e4sningstester ska g\u00f6ras b\u00f6r styras av resultaten av \u00e5terkommande riskanalyser. Mapping to ISO 27001 Controls \u00b6 A.12.3.1 Information Backup A.17.1.1 Planning Information Security Continuity Compliant Kubernetes Backup Dashboard \u00b6 The Compliant Kubernetes Backup Dashboard allows to quickly audit the status of backups and ensure the Recovery Point Objective are met. Handling Non-Compliance \u00b6 In case there is a violation of backup policies: Ask the administrator to check the status of the backup jobs . Ask the developers to check if they correctly marked Kubernetes resources with the necessary backup annotations .","title":"Backup"},{"location":"ciso-guide/capacity-management/","tags":["ISO 27001 A.12.1.3"],"text":"Capacity Management (Kubernetes Status) Dashboard \u00b6 Relevant Regulations \u00b6 GDPR \u00b6 GDPR Article 32 : The ability to ensure the ongoing confidentiality, integrity, availability and resilience of processing systems and services; [highlights added] Swedish Patient Data Law \u00b6 Note This regulation is only available in Swedish. To avoid confusion, we decided not to produce an unofficial translation. HSLF-FS 2016:40 : 10 \u00a7 V\u00e5rdgivaren ska vid utveckling, idrifttagande och \u00e4ndring av informationssystem som anv\u00e4nds f\u00f6r behandling av personuppgifter s\u00e4kerst\u00e4lla att personuppgifternas tillg\u00e4nglighet, riktighet, konfidentialitet och sp\u00e5rbarhet inte riskeras. Mapping to ISO 27001 Controls \u00b6 A.12.1.3 Capacity Management The use of resources must be monitored, tuned and projections made of future capacity requirements to ensure the required system performance to meet the business objectives. Compliant Kubernetes Status Dashboard \u00b6 The Compliant Kubernetes Status Dashboard shows a quick overview of the status of your kubernetes cluster. This includes: Unhealthy pods Unhealthy nodes Resource requested of the total resources in the cluster Pods with missing resource requests This makes it easy to identify when your cluster is not working correctly and helps you identify configuration that isn't following best practise.","title":"Capacity Management"},{"location":"ciso-guide/controls/","text":"ISO 27001 and BSI IT-Grundschutz Controls \u00b6 Click on the links below to navigate the documentation by ISO 27001 control: BSI IT-Grundschutz APP.4.4.A10 \u00b6 CI/CD BSI IT-Grundschutz APP.4.4.A14 \u00b6 Use Dedicated Nodes for Additional Services BSI IT-Grundschutz APP.4.4.A16 \u00b6 Overview BSI IT-Grundschutz APP.4.4.A18 \u00b6 Network Model Enforce NetworkPolicies BSI IT-Grundschutz APP.4.4.A2 \u00b6 CI/CD BSI IT-Grundschutz APP.4.4.A21 \u00b6 Maintenance Prepare Your Application BSI IT-Grundschutz APP.4.4.A3 \u00b6 How to Delegate Demarcation BSI IT-Grundschutz APP.4.4.A5 \u00b6 Backup Backups BSI IT-Grundschutz APP.4.4.A7 \u00b6 Network Model Enforce NetworkPolicies ISO 27001 A.10.1.2 \u00b6 Cryptography Network Model ISO 27001 A.12.1.3 \u00b6 Capacity Management Metrics Enforce Resources ISO 27001 A.12.2.1 \u00b6 Intrusion Detection ISO 27001 A.12.3.1 \u00b6 Backup Disaster Recovery Backups ISO 27001 A.12.4.1 \u00b6 Log Review Logs ISO 27001 A.12.4.3 \u00b6 Audit Logs Log Review Logs ISO 27001 A.12.4.4 \u00b6 Provider Audit ISO 27001 A.12.6.1 \u00b6 Intrusion Detection Vulnerability Management Overview Maintenance Prepare Your Application Enforce No Root Enforce Trusted Registries ISO 27001 A.13 \u00b6 Network Security ISO 27001 A.13.1.1 \u00b6 Network Model Enforce NetworkPolicies ISO 27001 A.13.1.2 \u00b6 Network Model Enforce NetworkPolicies ISO 27001 A.13.1.3 \u00b6 Network Model Enforce NetworkPolicies ISO 27001 A.14.1.1 \u00b6 Architectural Decision Log ISO 27001 A.14.2.5 \u00b6 Enforce No Root ISO 27001 A.15 \u00b6 Provider Audit ISO 27001 A.16 \u00b6 Metric Alerts Logs ISO 27001 A.16.1 \u00b6 Metrics ISO 27001 A.16.1.7 \u00b6 Intrusion Detection ISO 27001 A.17 \u00b6 We believe in community-driven open source ISO 27001 A.17.1.1 \u00b6 Backup Disaster Recovery ISO 27001 A.18.2.2 \u00b6 Policy-as-Code ISO 27001 A.18.2.3 \u00b6 Policy-as-Code ISO 27001 A.9.4.1 \u00b6 How to Delegate ISO 27001 A.9.4.4 \u00b6 Enforce No Root Other IT-Grundschutz Controls \u00b6 APP.4.4.A17 Attestierung von Nodes (H) \u00b6 The Kubespray layer in Compliant Kubernetes ensures that Data Plane Nodes and Control Plane Nodes are mutually authenticated via mutual TLS. BSI IT-Grundschutz Controls outside the scope of Compliant Kubernetes \u00b6 Pending official translation into English, the controls are written in German. APP.4.4.A1 Planung der Separierung der Anwendungen (B) \u00b6 Compliant Kubernetes recommends to setting up at least two separate environment: one for testing and one for production. APP.4.4.A6 Initialisierung von Pods (S) \u00b6 Application developers must make sure that initialization happens in init containers . APP.4.4.A11 \u00dcberwachung der Container (S) \u00b6 Application developers must ensure that their application has a liveliness and readiness probe, which are configured in the Deployment. This is illustrated by our user demo . APP.4.4.A12 Absicherung der Infrastruktur-Anwendungen (S) \u00b6 This requirement essentially states that the Compliant Kubernetes environments are only as secure as the infrastructure around them. Make sure you have a proper IT policy in place. Regularly review the systems where you store backups and configuration of Compliant Kubernetes. APP.4.4.A13 Automatisierte Auditierung der Konfiguration (H) \u00b6 Compliant Kubernetes administrators must regularly audit the configuration of their environments. We recommend doing this on a quarterly basis. APP.4.4.A20 Verschl\u00fcsselte Datenhaltung bei Pods (H) \u00b6 Compliant Kubernetes recommends disk encryption to be provided at the infrastructure level. If you have this requirement, check for full-disk encryption via the provider audit .","title":"ISO 27001 and BSI IT-Grundschutz"},{"location":"ciso-guide/cryptography/","tags":["ISO 27001 A.10.1.2"],"text":"Cryptography Dashboard \u00b6 Relevant Regulations \u00b6 GDPR \u00b6 GDPR Article 32 : Taking into account the state of the art [...] the controller and the processor shall implement [...] as appropriate [...] encryption of personal data; In assessing the appropriate level of security account shall be taken in particular of the risks that are presented by processing, in particular from accidental or unlawful destruction, loss, alteration, unauthorised disclosure of, or access to personal data transmitted , stored or otherwise processed. [highlights added] Swedish Patient Data Law \u00b6 Note This regulation is only available in Swedish. To avoid confusion, we decided not to produce an unofficial translation. HSLF-FS 2016:40 : Behandling av personuppgifter i \u00f6ppna n\u00e4t 15 \u00a7 Om v\u00e5rdgivaren anv\u00e4nder \u00f6ppna n\u00e4t vid behandling av personuppgifter, ska denne ansvara f\u00f6r att \u00f6verf\u00f6ring av uppgifterna g\u00f6rs p\u00e5 ett s\u00e5dant s\u00e4tt att inte obeh\u00f6riga kan ta del av dem, och elektronisk \u00e5tkomst eller direkt\u00e5tkomst till uppgifterna f\u00f6reg\u00e5s av stark autentisering. Mapping to ISO 27001 Controls \u00b6 A.10.1.2 Key Management Compliant Kubernetes Cryptography Dashboard \u00b6 The Compliant Kubernetes Cryptography Dashboard allows to quickly audit the status of cryptography. It shows, amongst others, the public Internet endpoints (Ingresses) that are encrypted and the expiry time. Default Compliant Kubernetes configurations automatically renew certificates before expiry. Handling Non-Compliance \u00b6 In case there is a violation of cryptography policies: If a certificate is expired and was not renewed, ask the administrator to check the status of cert-manager and ingress-controller component. If an endpoint is not encrypted, ask the developers to set the necessary Ingress annotations .","title":"Cryptography"},{"location":"ciso-guide/faq/","text":"CISO FAQ \u00b6 Will GrafanaLabs change to AGPL licenses affect Compliant Kubernetes? \u00b6 TL;DR Users and administrators of Compliant Kubernetes are unaffected. Part of Compliant Kubernetes -- specifically the CISO dashboards -- are built on top of Grafana, which recently changed its license to AGPLv3 . In brief, if Grafana is exposed via a network connection -- as is the case with Compliant Kubernetes -- then AGPLv3 requires all source code including modifications to be made available. The exact difference between \"aggregate\" and \"modified version\" is somewhat unclear . Compliant Kubernetes only configures Grafana and does not change its source code. Hence, we determined that Compliant Kubernetes is an \"aggregate\" work and is unaffected by the \"viral\" clauses of AGPLv3. As a result, Compliant Kubernetes continues to be distributed under Apache 2.0 as before. Will Min.io change to AGPL licenses affect Compliant Kubernetes? \u00b6 TL;DR Users and administrators of Compliant Kubernetes are unaffected. Min.io recently changed its license to AGPLv3 . Certain installations of Compliant Kubernetes may use Min.io for accessing object storage on Azure or GCP. However, Compliant Kubernetes does not currently include Min.io. In brief, if Min.io is exposed via a network connection, then AGPLv3 requires all source code including modifications to be made available. The exact difference between \"aggregate\" and \"modified version\" is somewhat unclear . When using Min.io with Compliant Kubernetes, we only use Min.io via its S3-compatible API. Hence, we determined that Compliant Kubernetes is an \"aggregate\" work and is unaffected by the \"viral\" clauses of AGPLv3. As a result, Compliant Kubernetes continues to be distributed under Apache 2.0 as before.","title":"FAQ"},{"location":"ciso-guide/intrusion-detection/","tags":["ISO 27001 A.12.2.1","ISO 27001 A.12.6.1","ISO 27001 A.16.1.7"],"text":"Intrusion Detection Dashboard \u00b6 Relevant Regulations \u00b6 GDPR \u00b6 GDPR Article 32 : Taking into account the state of the art [...] the controller and the processor shall implement [...] as appropriate [...] encryption of personal data; In assessing the appropriate level of security account shall be taken in particular of the risks that are presented by processing, in particular from accidental or unlawful destruction, loss, alteration, unauthorised disclosure of, or access to personal data transmitted , stored or otherwise processed. [highlights added] Swedish Patient Data Law \u00b6 Note This regulation is only available in Swedish. To avoid confusion, we decided not to produce an unofficial translation. HSLF-FS 2016:40 : Utv\u00e4rdering av skyddet mot olovlig \u00e5tkomst 18 \u00a7 V\u00e5rdgivaren ska \u00e5rligen utv\u00e4rdera skyddet mot s\u00e5v\u00e4l intern som extern olovlig \u00e5tkomst till datorn\u00e4tverk och informationssystem som anv\u00e4nds f\u00f6r behandling av personuppgifter. Mapping to ISO 27001 Controls \u00b6 A.12.2.1 Controls Against Malware A.12.6.1 Management of Technical Vulnerabilities A.16.1.7 Collection of Evidence Compliant Kubernetes Intrusion Detection Dashboard \u00b6 The Compliant Kubernetes Intrusion Detection Dashboard allows to quickly audit any suspicious activity performed by code inside the cluster, such as writing to suspicious files (e.g., in /etc ) or attempting suspicious external network connections (e.g., SSH to a command-and-control server). Such activities may indicate anything from a misconfiguration issue to an ongoing attack. Therefore, this dashboard should be regularly reviewed, perhaps even daily. Handling Non-Compliance \u00b6 Make sure you have a proper incident management policy in place. If an attack is ongoing, it might be better to take the system offline to protect data from getting in the wrong hands. Operators need to be trained on what events justify such an extreme action, otherwise, escalating the issue along the reporting chain may add delays that favor the attacker. In less severe cases, simply contact the developers to investigate their code and fix any potential misconfiguration.","title":"Intrusion Detection"},{"location":"ciso-guide/log-review/","tags":["ISO 27001 A.12.4.1","ISO 27001 A.12.4.3"],"text":"Log Review \u00b6 This document highlights the risks that can be mitigated by regularly reviewing logs and makes concrete recommendations on how to do log review. Relevant Regulations \u00b6 GDPR \u00b6 GDPR Article 32 : Taking into account the state of the art, the costs of implementation and the nature, scope, context and purposes of processing as well as the risk of varying likelihood and severity for the rights and freedoms of natural persons, the controller and the processor shall implement appropriate technical and organisational measures to ensure a level of security appropriate to the risk, including inter alia as appropriate: [...] a process for regularly testing, assessing and evaluating the effectiveness of technical and organisational measures for ensuring the security of the processing. Swedish Patient Data Law \u00b6 Note This regulation is only available in Swedish. To avoid confusion, we decided not to produce an unofficial translation. HSLF-FS 2016:40 : 2 \u00a7 V\u00e5rdgivaren ska genom ledningssystemet s\u00e4kerst\u00e4lla att [...] 4. \u00e5tg\u00e4rder kan h\u00e4rledas till en anv\u00e4ndare (sp\u00e5rbarhet) i informationssystem som \u00e4r helt eller delvis automatiserade. Mapping to ISO 27001 Controls \u00b6 A.12.4.1 \"Event Logging\" A.12.4.3 \"Administrator and Operator Logs\" Purpose \u00b6 Compliant Kubernetes captures application logs and audit logs in a tamper-proof logging environment, which we call the service cluster. By \"tamper-proof\", we mean that even a complete compromise of production infrastructure does not allow an attacker to erase or change existing log entries, as would be required to hide their activity and avoid suspecion. Note Attackers can, however, inject new \"weird\" logs entries. However, that wouldn't remove their tracks and would only trigger more suspecion. However, said logs only help with information security if they are regularly reviewed for suspicious activity. Prefer to use logs for catching \"unknown unknowns\". For known bad failures -- e.g., a fluentd Pod restarting -- prefer alerts. Risks \u00b6 Periodically reviewing logs can mitigate the following information security risks: Information disclosure : Regularly reviewing logs can reveal an attack attempt or an ongoing attack. Downtime : Regularly reviewing logs can reveal misbehaving components (e.g., Pod restarts, various errors) and inform fixes before it leads to downtime. Silent corruption : Regularly reviewing logs can reveal data corruption. How to do log review \u00b6 By review period , we mean the time elapsed since the last review of the logs, e.g., 30 days. Aim for a review which is both wide and deep . By wide we mean that you should vary the time interval, time point, filters, etc., when reviewing log entries. By deep we mean that you should actually read and try to understand a sample of logs. Open up a browser and open the Compliant Kubernetes logs of the cluster you are reviewing. This functionality is currently offered by OpenSearch or Kibana and Elasticsearch depending on version, but the procedure is the same. Search for the following keywords on all indices -- i.e., search over each index pattern -- over the last review period: error , failed , failure , deny , denied , blocked , invalid , expired , unable , unauthorized , bad , 401 , 403 , 500 , unknown . Sample a few keywords you recently encountered during your work, e.g., already installed or not found ; be creative and unpredictable. Vary the time point, the time interval, filters, etc. Go wide : For each query (index pattern, keyword, timepoint, time interval and filter combination), look at the timeline and see if there is an unexpected increase or decrease in the count of log lines. If you find any, focus your attention on those. Go deep : For each query, sample at least 10 log entries, read them and make sure you understand what they mean. Think about the following: What are potential causes? What are potential implications? Time: Do the entries appear periodically or randomly? Space: Does a specific component trigger them? Is the entry generated by the platform or the application? If anything catches your attention vary the time point, time interval and various filters to understand if the log entry is a risk indicator or not. Look for unknown unknowns . Any failures, especially authentication failures, which feature a significant increase are risk indicators. Contact the person owning the component, e.g., the application developer or Compliant Kubernetes architect, to better understand if the entry is suspecious or not. Perhaps it is due to a recent change -- as indicated by an operator log -- and indicates no risk. Possible resolutions \u00b6 If you found a suspecious activity, escalate. If the log entry is due to a bug in Compliant Kubernetes, file an issue.","title":"Log Review"},{"location":"ciso-guide/network-security/","tags":["ISO 27001 A.13"],"text":"Network Security Dashboard \u00b6 Relevant Regulations \u00b6 GDPR \u00b6 GDPR Article 32 : Taking into account the state of the art [...] the controller and the processor shall implement [...] as appropriate [...] encryption of personal data; In assessing the appropriate level of security account shall be taken in particular of the risks that are presented by processing, in particular from accidental or unlawful destruction, loss, alteration, unauthorised disclosure of, or access to personal data transmitted , stored or otherwise processed. [highlights added] Swedish Patient Data Law \u00b6 Note This regulation is only available in Swedish. To avoid confusion, we decided not to produce an unofficial translation. HSLF-FS 2016:40 : Utv\u00e4rdering av skyddet mot olovlig \u00e5tkomst 18 \u00a7 V\u00e5rdgivaren ska \u00e5rligen utv\u00e4rdera skyddet mot s\u00e5v\u00e4l intern som extern olovlig \u00e5tkomst till datorn\u00e4tverk och informationssystem som anv\u00e4nds f\u00f6r behandling av personuppgifter. Mapping to ISO 27001 Controls \u00b6 A.13 Communications Security Compliant Kubernetes Network Security Dashboard \u00b6 The Compliant Kubernetes Network Security Dashboard allows to audit violations of NetworkPolicies (i.e., \"firewall rules\"). In the best case, denied traffic indicates a misconfiguration. In worst case, denied traffic indicates an ongoing security attack. Significant or unexpected increases of allowed traffic should also be closely monitored. In best case, these may indicate inefficient application code which may cause capacity issues later. In worst case, these may indicate an attempt to exfiltrate large amounts of data or to use the cluster as a reflector for an amplification attack . Therefore, this dashboard should be regularly reviewed, perhaps even daily. Handling Non-Compliance \u00b6 Make sure you have a proper incident management policy in place. If an attack is ongoing, it might be better to take the system offline to protect data from getting in the wrong hands. Operators need to be trained on what events justify such an extreme action, otherwise, escalating the issue along the reporting chain may add delays that favor the attacker. In less severe cases, simply contact the developers to investigate their code, fix needless communication attempts or update their NetworkPolicies accordingly to fix any potential misconfiguration. Further Reading \u00b6 Network Policies","title":"Network Security"},{"location":"ciso-guide/policy-as-code/","tags":["ISO 27001 A.18.2.2","ISO 27001 A.18.2.3"],"text":"Policy-as-Code Dashboard \u00b6 Relevant Regulations \u00b6 Although \"policy-as-code\" is not explicit in any regulation, enforcing policies in a consistent technical manner (\"policy-as-code\") is seen as an important strategy to reduce compliance violations, as well as reduce the overhead of complying. Mapping to ISO 27001 Controls \u00b6 A.18.2.2 Compliance with Security Policies & Standards A.18.2.3 Technical Compliance Review Compliant Kubernetes Policy-as-Code Dashboard \u00b6 Some of your policies are best enforced in code, e.g., Ingress resources do not have encryption set up or PersistentVolumeClaims do not have the necessary backup annotations. Setting up such policies as code is highly dependent on your organization, your risk appetite and your operations. Policies that make sense enforcing by code may be required in some organizations, whereas others might see it as unnecessary and prefer simply treat codified policies as aspirational. Whatever your situation, the Compliant Kubernetes Policy-as-Code Dashboard allows to quickly audit what Kubernetes resources are set up in a non-compliant way or how many policy violations were avoided by Compliant Kubernetes. Handling Non-Compliance \u00b6 If an application or user keeps violating a policy, start by reviewing the policy. If the policy seems well codified, contact the developer or the application owner to determine why policy violations occurs or need to be prevented by Compliant Kubernetes. If a policy is missing or too strict, contact the Compliant Kubernetes administrators.","title":"Policy-as-Code"},{"location":"ciso-guide/vulnerability/","tags":["ISO 27001 A.12.6.1"],"text":"Vulnerability Dashboard \u00b6 Relevant Regulations \u00b6 GDPR \u00b6 GDPR Article 32 : Taking into account the state of the art [...] the controller and the processor shall implement [...] as appropriate [...] a process for regularly testing, assessing and evaluating the effectiveness of technical and organisational measures for ensuring the security of the processing. In assessing the appropriate level of security account shall be taken in particular of the risks that are presented by processing, in particular from accidental or unlawful destruction, loss, alteration, unauthorised disclosure of, or access to personal data transmitted , stored or otherwise processed. [highlights added] Swedish Patient Data Law \u00b6 Note This regulation is only available in Swedish. To avoid confusion, we decided not to produce an unofficial translation. HSLF-FS 2016:40 : 10 \u00a7 V\u00e5rdgivaren ska vid utveckling, idrifttagande och \u00e4ndring av informationssystem som anv\u00e4nds f\u00f6r behandling av personuppgifter s\u00e4kerst\u00e4lla att personuppgifternas tillg\u00e4nglighet, riktighet, konfidentialitet och sp\u00e5rbarhet inte riskeras. Mapping to ISO 27001 Controls \u00b6 A.12.6.1 Management of Technical Vulnerabilities Compliant Kubernetes Vulnerability Dashboard \u00b6 The Compliant Kubernetes Vulnerability Dashboard allows to audit what vulnerable container images are running in production. The dashboard allows to asses increase or decrease of exposure over time. It also allows to prioritize vulnerabilities based on CVE score (CVSS). Therefore, this dashboard should be regularly reviewed, perhaps even daily. A vulnerability management process should be in place to decide how to systematically handle vulnerabilities. Handling Non-Compliance \u00b6 Containers should preferably be redeployed with an image that received the necessary security fixes. In case the security fix cannot be deployed in a timely manner -- e.g., due to a slow fix from the vendor -- then the affected containers should be terminated. In all cases, isolating a container using NetworkPolicies, non-root user accounts, no service account token, etc. can make a vulnerability more difficult to exploit. Further Reading \u00b6 Vulnerability management CVE CVSS Starboard","title":"Vulnerability Management"},{"location":"contributor-guide/","tags":["ISO 27001 A.12.6.1"],"text":"Contributor guide \u00b6 Definition of Done \u00b6 When working in regulated industries, it is really important to have the bar high for when something can be called \"done\". In Compliant Kubernetes, we use the following definition of done: Code and documentation is merged on the main branch of upstream projects. This may cause time delays which are outside your control. However, if we cannot convince upstream projects to take our contributions, then we better know about this as soon as possible. A Compliant Kubernetes relying on an abandoned upstream branch is unsustainable. Code is merged in the Compliant Kubernetes project. Documentation is up-to-date. IT systems used in regulated industries need to have documentation. (See ISO 27001 A.12.1.1 \"Documented Operating Procedures\" ). You may either point to upstream documentation -- if Compliant Kubernetes does not add any specifics -- or write a dedicated section/page. Prefer to refer to upstream documentation -- potentially updating that one -- instead of duplicating it in Compliant Kubernetes. You provide evidence for completion. This can be terminal output, screenshot or -- even better, but more time consuming -- a screencast with voice-over explanations. Ideally, these should be attached in the PR to convince the reviewer that the code and documentation are as intended. Submitting PRs \u00b6 To make the review process as smooth as possible for everyone we have some steps that we'd like you to follow Look through our DEVELOPMENT.md The pre-commit hook will run on all PRs to main , so either make sure to have it installed by running: pre-commit install Or manually run it before committing pre-commit run Make sure to follow the PR template, see this for more details. Alternatively start a PR and you'll see it there. Setting up your environment \u00b6 To install all required tools, please follow the instructions here . Tips and tricks \u00b6 To make your life easier we suggest to use language server for the language that you're editing. E.g. terraform: terraform-ls yaml: yaml-language-server To catch pre-commit errors early, direct in your editor, it's also suggested to install plugins for these tools. markdownlint shellcheck When developing and you only working on a single application it will be faster to only deploy that application instead of applying all charts. This can be done by figuring out the app label for the application in question by running: bin/ck8s ops helmfile {wc|sc} list When you figured out the app label (lets say it's dex in this case) you can check the diff of your work by running: bin/ck8s ops helmfile {wc|sc} -l app=dex diff Instead of running helmfile apply , it might be useful to run helmfile sync . This will do a 3-way upgrade and make sure that the helm state matches the objects actually running in kubernetes. This will make sure that you haven't manually edited something for debugging and forgot about it. bin/ck8s ops helmfile {wc|sc} -l app=dex sync Object storage \u00b6 To make creating and deletion of buckets easy, we've a script to help you with that, see here (the quickstart has instructions on how to use it) . DNS \u00b6 These following snippets can be used to setup/remove all DNS records required for ck8s using exoscales cli. Start by setting up some variables: DOMAIN=\"example.com\" IP=\"203.0.113.123\" # IP to LB/ingress endpoint for the service cluster CK8S_ENVIRONMENT_NAME=\"my-cluster-name\" SUBDOMAINS=( \"*.ops.${CK8S_ENVIRONMENT_NAME}\" \"grafana.${CK8S_ENVIRONMENT_NAME}\" \"harbor.${CK8S_ENVIRONMENT_NAME}\" \"kibana.${CK8S_ENVIRONMENT_NAME}\" \"dex.${CK8S_ENVIRONMENT_NAME}\" \"notary.harbor.${CK8S_ENVIRONMENT_NAME}\" ) # Adding the A records for SUBDOMAIN in \"${SUBDOMAINS[@]}\"; do exo dns add A \"${DOMAIN}\" -a \"${IP}\" -n \"${SUBDOMAIN}\" done # Removing the records for SUBDOMAIN in \"${SUBDOMAINS[@]}\"; do exo dns remove \"${DOMAIN}\" \"${SUBDOMAIN}\" done Reusing clusters \u00b6 If you for some reason need to reinstall Compliant Kubernetes from scratch, we have some scripts that removes all objects created by this repo. The scripts can be found here (clean-sc.sh and clean-wc.sh) .","title":"Overview"},{"location":"operator-manual/","text":"Operator Manual Overview \u00b6 This manual is for Compliant Kubernetes administrators. Operators can be described via the following user stories: As an administrator I want to create/destroy/upgrade a Compliant Kubernetes cluster. As an administrator I want to re-configure a Compliant Kubernetes cluster. As an on-call administrator I want to be alerted when abnormal activity is detected, suggesting a pending intrusion. As an on-call administrator I want to be alerted when the Compliant Kubernetes cluster is unhealthy. As an on-call administrator I want \"break glass\" to investigate and recover an unhealthy Compliant Kubernetes cluster.","title":"Overview"},{"location":"operator-manual/access-control/","text":"Access control \u00b6 This guide describes how to set up and make use of group claims for applications. Note This guide assumes your group claim name is groups Kubernetes \u00b6 To set up kubelogin to fetch and use groups make sure that your kubeconfig looks something like this. users : - name : user@my-cluster user : exec : apiVersion : client.authentication.k8s.io/v1beta1 args : - oidc-login - get-token - --oidc-issuer-url=https://dex.my-cluster-domain.com - --oidc-client-id=my-client-id - --oidc-client-secret=my-client-secret - --oidc-extra-scope=email,groups # Make sure groups are here command : kubectl Tips Your token can be found in ~/.kube/cache/oidc-login/ . This is useful if you're trying to debug your claims since you can just paste the token to jwt.io and check it. Example: $ ls ~/.kube/cache/oidc-login/ $ kubectl get pod <log in> $ ls ~/.kube/cache/oidc-login/ 13b165965d8e80749ce3b8d442da3e4e9f5ff5e38900ef104eee99fde85a39d4 $ cat ~/.kube/cache/oidc-login/13b165965d8e80749ce3b8d442da3e4e9f5ff5e38900ef104eee99fde85a39d4 | jq -r .id_token eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2RleC5teS1jbHVzdGVyLWRvbWFpbi5jb20iLCJpYXQiOjE2MjE1MTUxNzcsImV4cCI6MTY1MzEzNzU3NywiYXVkIjoibXktY2xpZW50LWlkIiwic3ViIjoiSGlVUE92S1BKMmVwWUkwR1R1U0JYWGRxYTJTV2ZxRnc1ZjBXNVBQeThTWSIsIm5vdW5jZSI6IkNoVXhNRFk0TVRZNE1qRXpORFUzTURVM01ERXlNREFTQm1kdmIyZHNaUSIsImF0X2hhc2giOiI1aUZjbF9Sc1JvblhHekZaMU0xQ2JnIiwiZW1haWwiOiJ1c2VyQG15LWRvbWFpbi5jb20iLCJlbWFpbF92ZXJpZmllZCI6InRydWUiLCJncm91cHMiOlsibXktZ3JvdXAtb25lIiwibXktZ3JvdXAtdHdvIl19.s65Aowfn6B1PiyQvRGPRu9KgX7G39nkLtx6yCAEElao Copy the token to jwt.io and ensure that the payload includes the expected groups claim. OpenSearch \u00b6 To enable OpenSearch to use the groups for OpenSearch Dashboards access. opensearch : sso : scope : \"... groups\" # Add groups to existing extraRoleMappings : - mapping_name : kibana_user definition : backend_roles : - my-group-name - mapping_name : kubernetes_log_reader definition : backend_roles : - my-group-name - mapping_name : readall_and_monitor definition : backend_roles : - my-group-name Note For Open Distro for Elasticsearch and Kibana used in v0.18 and earlier, the same configuration applies under the root key elasticsearch instead of opensearch . Harbor \u00b6 Set correct group claim name since the default scopes includes groups already. This groups can be assigned to projects or as admin group. harbor : oidc : groupClaimName : groups Note When OIDC (e.g. DeX) is enabled we cannot create static users using the Harbor web interface. But when anyone logs in via DeX they automatically get a user and we can promote that user to admin. Once there is one admin, they can set specific permissions for other users (there should be at least a few users promoted to admins). Grafana \u00b6 Note This section assumes that elastisys/compliantkubernetes-apps/pull/450 is merged OPS Grafana \u00b6 prometheus : grafana : oidc : enabled : true userGroups : grafanaAdmin : my-admin-group grafanaEditor : my-editor-group grafanaViewer : my-viewer-group scopes : \".... groups\" # Add groups to existing allowedDomains : - my-domain.com User Grafana \u00b6 user : grafana : oidc : scopes : \"... groups\" # Add groups to existing allowedDomains : - my-domain.com userGroups : grafanaAdmin : my-admin-group grafanaEditor : my-editor-group grafanaViewer : my-viewer-group","title":"Access Control"},{"location":"operator-manual/aws/","text":"This page is out of date We are currently working on internal documentation to streamline Compliant Kubernetes onboarding for selected cloud providers. Until those documents are ready, and until we have capacity to make parts of that documentation public, this page is out-of-date. Nevertheless, parts of it are useful. Use at your own risk and don't expect things to work smoothly. Compliant Kubernetes Deployment on AWS \u00b6 This document describes how to set up Compliant Kubernetes on AWS. The setup has two major parts: Deploying at least two vanilla Kubernetes clusters Deploying Compliant Kubernetes apps Before starting, make sure you have all necessary tools . Note This guide is written for compliantkubernetes-apps v0.17.0 Setup \u00b6 Choose names for your service cluster and workload clusters, as well as the DNS domain to expose the services inside the service cluster: SERVICE_CLUSTER = \"testsc\" WORKLOAD_CLUSTERS =( \"testwc0\" ) BASE_DOMAIN = \"example.com\" Note If you want to set up multiple workload clusters you can add more names. E.g. WORKLOAD_CLUSTERS=( \"testwc0\" \"testwc1\" \"testwc2\" ) SERVICE_CLUSTER and each entry in WORKLOAD_CLUSTERS must be maximum 17 characters long. Deploying vanilla Kubernetes clusters \u00b6 We suggest to set up Kubernetes clusters using kubespray. If you haven't done so already, clone the Elastisys Compliant Kubernetes Kubespray repo as follows: git clone --recursive https://github.com/elastisys/compliantkubernetes-kubespray cd compliantkubernetes-kubespray Infrastructure Setup using Terraform \u00b6 Note This step will also create the necessary IAM Roles for control plane Nodes to make integration with the cloud provider work. This will ensure that both Service Type LoadBalancer and PersistentVolumes (backed by AWS EBS volumes) will work. The necessary credentials are pulled automatically by control plane Nodes via AWS EC2 instance metadata and require no other configuration. Expose AWS credentials to Terraform \u00b6 We suggest exposing AWS credentials to Terraform via environment variables, so they are not accidentally left on the file-system: export TF_VAR_AWS_ACCESS_KEY_ID = \"xyz\" # Access key for AWS export TF_VAR_AWS_SECRET_ACCESS_KEY = \"zyx\" # Secret key for AWS export TF_VAR_AWS_SSH_KEY_NAME = \"foo\" # Name of the AWS key pair to use for the EC2 instances export TF_VAR_AWS_DEFAULT_REGION = \"bar\" # Region to use for all AWS resources Tip We suggest generating the SSH key locally, then importing it to AWS. Customize your infrastructure \u00b6 Create a configuration for the service cluster and the workload cluster: pushd kubespray for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do cat contrib/terraform/aws/terraform.tfvars \\ | sed \\ -e \"s@^aws_cluster_name =.*@aws_cluster_name = \\\" $CLUSTER \\\"@\" \\ -e \"s@^inventory_file =.*@inventory_file = \\\"../../../inventory/hosts- $CLUSTER \\\"@\" \\ -e \"s@^aws_kube_worker_size =.*@aws_kube_worker_size = \\\"t3.large\\\"@\" \\ > inventory/terraform- $CLUSTER .tfvars done popd Review and, if needed, adjust the files in kubespray/inventory/ . Initialize and Apply Terraform \u00b6 pushd kubespray/contrib/terraform/aws terraform init for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do terraform apply \\ -var-file = ../../../inventory/terraform- $CLUSTER .tfvars \\ -auto-approve \\ -state = ../../../inventory/tfstate- $CLUSTER .tfstate done popd Important The Terraform state is stored in kubespray/inventory/tfstate-* . It is precious. Consider backing it up or using Terraform Cloud . Check that the Ansible inventory was properly generated \u00b6 ls -l kubespray/inventory/hosts-* You may also want to check the AWS Console if the infrastructure was created correctly: Deploying vanilla Kubernetes clusters using Kubespray \u00b6 With the infrastructure provisioned, we can now deploy both the sc and wc Kubernetes clusters using kubespray. Before trying any of the steps, make sure you are in the repo's root folder. Init the Kubespray config in your config path \u00b6 export CK8S_CONFIG_PATH = ~/.ck8s/aws export CK8S_PGP_FP = <your GPG key fingerprint> # retrieve with gpg --list-secret-keys for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ./bin/ck8s-kubespray init $CLUSTER aws $CK8S_PGP_FP done Copy the inventories generated by Terraform above in the right place \u00b6 for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do cp kubespray/inventory/hosts- $CLUSTER $CK8S_CONFIG_PATH / $CLUSTER -config/inventory.ini done Run kubespray to deploy the Kubernetes clusters \u00b6 for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ./bin/ck8s-kubespray apply $CLUSTER --flush-cache -e ansible_user = ubuntu done This may take up to 20 minutes per cluster. Correct the Kubernetes API IP addresses \u00b6 Find the DNS names of the load balancers fronting the API servers: grep apiserver_loadbalancer $CK8S_CONFIG_PATH /*-config/inventory.ini Locate the encrypted kubeconfigs kube_config_*.yaml and edit them using sops. Copy the URL of the load balancer from inventory files shown above into kube_config_*.yaml . Do not overwrite the port. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops $CK8S_CONFIG_PATH /.state/kube_config_ $CLUSTER .yaml done Test access to the clusters as follows \u00b6 for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file $CK8S_CONFIG_PATH /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get nodes' done Deploying Compliant Kubernetes Apps \u00b6 Now that the Kubernetes clusters are up and running, we are ready to install the Compliant Kubernetes apps. Clone compliantkubernetes-apps and Install Pre-requisites \u00b6 If you haven't done so already, clone the compliantkubernetes-apps repo and install pre-requisites. git clone https://github.com/elastisys/compliantkubernetes-apps.git cd compliantkubernetes-apps ansible-playbook -e 'ansible_python_interpreter=/usr/bin/python3' --ask-become-pass --connection local --inventory 127 .0.0.1, get-requirements.yaml Initialize the apps configuration \u00b6 export CK8S_ENVIRONMENT_NAME = my-environment-name #export CK8S_FLAVOR=[dev|prod] # defaults to dev export CK8S_CONFIG_PATH = ~/.ck8s/my-cluster-path export CK8S_CLOUD_PROVIDER = # [exoscale|safespring|citycloud|aws|baremetal] export CK8S_PGP_FP = <your GPG key fingerprint> # retrieve with gpg --list-secret-keys ./bin/ck8s init This will initialise the configuration in the ${CK8S_CONFIG_PATH} directory. Generating configuration files sc-config.yaml and wc-config.yaml , as well as secrets with randomly generated passwords in secrets.yaml . This will also generate read-only default configuration under the directory defaults/ which can be used as a guide for available and suggested options. ls -l $CK8S_CONFIG_PATH Configure the apps \u00b6 Edit the configuration files ${CK8S_CONFIG_PATH}/sc-config.yaml , ${CK8S_CONFIG_PATH}/wc-config.yaml and ${CK8S_CONFIG_PATH}/secrets.yaml and set the appropriate values for some of the configuration fields. Note that, the latter is encrypted. vim ${ CK8S_CONFIG_PATH } /sc-config.yaml vim ${ CK8S_CONFIG_PATH } /wc-config.yaml sops ${ CK8S_CONFIG_PATH } /secrets.yaml Tip The default configuration for the service cluster and workload cluster are available in the directory ${CK8S_CONFIG_PATH}/defaults/ and can be used as a reference for available options. Warning Do not modify the read-only default configurations files found in the directory ${CK8S_CONFIG_PATH}/defaults/ . Instead configure the cluster by modifying the regular files ${CK8S_CONFIG_PATH}/sc-config.yaml and ${CK8S_CONFIG_PATH}/wc-config.yaml as they will override the default options. The following are the minimum change you should perform: # sc-config.yaml and wc-config.yaml global : baseDomain : \"set-me\" # set to $BASE_DOMAIN opsDomain : \"set-me\" # set to ops.$BASE_DOMAIN issuer : letsencrypt-prod objectStorage : type : \"s3\" s3 : region : \"set-me\" # Region for S3 buckets, e.g, eu-central-1 regionEndpoint : \"set-me\" # e.g., https://s3.us-west-1.amazonaws.com # sc-config.yaml harbor : oidc : groupClaimName : \"set-me\" # set to group claim name used by OIDC provider adminGroupName : \"set-me\" # name of the group that automatically will get admin issuers : letsencrypt : prod : email : \"set-me\" # set this to an email to receive LetsEncrypt notifications staging : email : \"set-me\" # set this to an email to receive LetsEncrypt notifications # secrets.yaml objectStorage : s3 : accessKey : \"set-me\" #put your s3 accesskey secretKey : \"set-me\" #put your s3 secretKey Create placeholder DNS entries \u00b6 To avoid negative caching and other surprises. Create two placeholders as follows (feel free to use the \"Import zone\" feature of AWS Route53): echo \"\"\" *. $BASE_DOMAIN 60s A 203.0.113.123 *.ops. $BASE_DOMAIN 60s A 203.0.113.123 \"\"\" NOTE: 203.0.113.123 is in TEST-NET-3 and okey to use as placeholder. Install Compliant Kubernetes apps \u00b6 Start with the service cluster: ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ SERVICE_CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_sc.yaml ./bin/ck8s apply sc # Respond \"n\" if you get a WARN Then the workload clusters: for CLUSTER in \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_wc.yaml ./bin/ck8s apply wc # Respond \"n\" if you get a WARN done Settling \u00b6 Important Leave sufficient time for the system to settle, e.g., request TLS certificates from LetsEncrypt, perhaps as much as 20 minutes. You can check if the system settled as follows: for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get --all-namespaces pods' done Check the output of the command above. All Pods needs to be Running or Completed. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get --all-namespaces issuers,clusterissuers,certificates' done Check the output of the command above. All resources need to have the Ready column True. Setup required DNS entries \u00b6 You will need to set up the following DNS entries. First, determine the public IP of the load-balancer fronting the Ingress controller of the service cluster : SC_INGRESS_LB_HOSTNAME = $( sops exec-file $CK8S_CONFIG_PATH /.state/kube_config_sc.yaml 'kubectl --kubeconfig {} get -n ingress-nginx svc ingress-nginx-controller -o jsonpath={.status.loadBalancer.ingress[0].hostname}' ) SC_INGRESS_LB_IP = $( dig +short $SC_INGRESS_LB_HOSTNAME | head -1 ) echo $SC_INGRESS_LB_IP Then, import the following zone in AWS Route53: echo \"\"\" *.ops. $BASE_DOMAIN 60s A $SC_INGRESS_LB_IP dex. $BASE_DOMAIN 60s A $SC_INGRESS_LB_IP grafana. $BASE_DOMAIN 60s A $SC_INGRESS_LB_IP harbor. $BASE_DOMAIN 60s A $SC_INGRESS_LB_IP kibana. $BASE_DOMAIN 60s A $SC_INGRESS_LB_IP \"\"\" Testing \u00b6 After completing the installation step you can test if the apps are properly installed and ready using the commands below. Start with the service cluster: ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ SERVICE_CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_sc.yaml ./bin/ck8s test sc # Respond \"n\" if you get a WARN Then the workload clusters: for CLUSTER in \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_wc.yaml ./bin/ck8s test wc # Respond \"n\" if you get a WARN done Done. Navigate to the endpoints, for example grafana.$BASE_DOMAIN , kibana.$BASE_DOMAIN , harbor.$BASE_DOMAIN , etc. to discover Compliant Kubernetes's features. Teardown \u00b6 Removing Compliant Kubernetes Apps from your cluster \u00b6 To remove the applications added by compliant kubernetes you can use the two scripts clean-sc.sh and clean-wc.sh , they are located here in the scripts folder . They perform the following actions: Delete the added helm charts Delete the added namespaces Delete any remaining PersistentVolumes Delete the added CustomResourceDefinitions Note: if user namespaces are managed by Compliant Kubernetes apps then they will also be deleted if you clean up the workload cluster. Remove infrastructure \u00b6 Note Even if you want to completely destroy the cluster with all its infrastructure, it is recommended to first execute the clean scripts described above, otherwise resources created by the cloud controller (e.g. volumes and loadbalancers) are not removed and terraform destroy might fail. pushd kubespray/contrib/terraform/aws for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do terraform destroy \\ -auto-approve \\ -state = ../../../inventory/tfstate- $CLUSTER .tfstate done popd Further Reading \u00b6 Compliant Kubernetes apps repo Configurations option","title":"On AWS"},{"location":"operator-manual/azure/","text":"This page is out of date We are currently working on internal documentation to streamline Compliant Kubernetes onboarding for selected cloud providers. Until those documents are ready, and until we have capacity to make parts of that documentation public, this page is out-of-date. Nevertheless, parts of it are useful. Use at your own risk and don't expect things to work smoothly. Compliant Kubernetes Deployment on Azure \u00b6 This document contains instructions on how to setup a service cluster and a workload cluster in Azure. The following are the main tasks addressed in this document: Infrastructure setup for two clusters: one service and one workload cluster Deploying Compliant Kubernetes on top of the two clusters. Creating DNS Records Deploying Rook Storage Orchestration Service Deploying Compliant Kubernetes apps Before starting, make sure you have all necessary tools . Note This guide is written for compliantkubernetes-apps v0.13.0 Setup \u00b6 Choose names for your service cluster and workload clusters, as well as the DNS domain to expose the services inside the service cluster: SERVICE_CLUSTER = \"sc-test\" WORKLOAD_CLUSTERS =( \"wc-test0\" ) BASE_DOMAIN = \"example.com\" Infrastructure Setup using AzureRM \u00b6 We suggest to set up Kubernetes clusters using kubespray. If you haven't done so already, clone the Elastisys Compliant Kubernetes Kubespray repo as follows: git clone --recursive https://github.com/elastisys/compliantkubernetes-kubespray cd compliantkubernetes-kubespray Install azure-cli \u00b6 If you haven't done so already, please install and configure azure-cli . Login with azure-cli \u00b6 az login Customize your infrastructure \u00b6 Create a configuration for the service and the workload clusters: pushd kubespray/contrib/azurerm/ for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do az group create -g $CLUSTER -l northeurope mkdir -p $CLUSTER /inventory done popd Note Please specify the value for the ssh_public_keys variable in kubespray/contrib/azurerm/group_vars/all . It must be your SSH public key to access your Azure virtual machines. Besides, the value for the cluster_name variable must be globally unique due to some restrictions in Azure. Make sure that $SERVICE_CLUSTER and $WORKLOAD_CLUSTERS are unique. Review and, if needed, adjust the files in kubespray/contrib/azurerm/group_vars/all accordingly. Generate and apply the templates \u00b6 pushd kubespray/contrib/azurerm/ tmp = \"\" for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do cat group_vars/all \\ | sed \\ -e \"s@^cluster_name:.*@cluster_name: \\\" $CLUSTER \\\"@\" \\ > group_vars/all1 cat group_vars/all1 > group_vars/all rm group_vars/all1 if [ -z $tmp ] then sed -i \"s/{{ playbook_dir }}/{{ playbook_dir }}\\/ $CLUSTER /g\" roles/generate-templates/tasks/main.yml ansible-playbook generate-templates.yml az deployment group create --template-file ./ $CLUSTER /.generated/network.json -g $CLUSTER az deployment group create --template-file ./ $CLUSTER /.generated/storage.json -g $CLUSTER az deployment group create --template-file ./ $CLUSTER /.generated/availability-sets.json -g $CLUSTER az deployment group create --template-file ./ $CLUSTER /.generated/bastion.json -g $CLUSTER az deployment group create --template-file ./ $CLUSTER /.generated/masters.json -g $CLUSTER az deployment group create --template-file ./ $CLUSTER /.generated/minions.json -g $CLUSTER else sed -i \"s/{{ playbook_dir }}\\/ $tmp /{{ playbook_dir }}\\/ $CLUSTER /g\" roles/generate-templates/tasks/main.yml ansible-playbook generate-templates.yml az deployment group create --template-file ./ $CLUSTER /.generated/network.json -g $CLUSTER az deployment group create --template-file ./ $CLUSTER /.generated/storage.json -g $CLUSTER az deployment group create --template-file ./ $CLUSTER /.generated/availability-sets.json -g $CLUSTER az deployment group create --template-file ./ $CLUSTER /.generated/bastion.json -g $CLUSTER az deployment group create --template-file ./ $CLUSTER /.generated/masters.json -g $CLUSTER az deployment group create --template-file ./ $CLUSTER /.generated/minions.json -g $CLUSTER fi tmp = $CLUSTER done sed -i \"s/{{ playbook_dir }}\\/ $tmp /{{ playbook_dir }}/g\" roles/generate-templates/tasks/main.yml sed -i \"s/{{ playbook_dir }}\\/ $tmp /{{ playbook_dir }}/g\" roles/generate-inventory_2/tasks/main.yml sed -i \"s/{{ playbook_dir }}\\/ $tmp /{{ playbook_dir }}/g\" roles/generate-inventory/tasks/main.yml popd Generating an inventory for kubespray \u00b6 pushd kubespray/contrib/azurerm/ tmp = \"\" for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do if [ -z $tmp ] then sed -i \"s/{{ playbook_dir }}/{{ playbook_dir }}\\/ $CLUSTER /g\" roles/generate-inventory_2/tasks/main.yml sed -i \"s/{{ playbook_dir }}/{{ playbook_dir }}\\/ $CLUSTER /g\" roles/generate-inventory/tasks/main.yml ./generate-inventory.sh $CLUSTER else sed -i \"s/{{ playbook_dir }}\\/ $tmp /{{ playbook_dir }}\\/ $CLUSTER /g\" roles/generate-inventory_2/tasks/main.yml sed -i \"s/{{ playbook_dir }}\\/ $tmp /{{ playbook_dir }}\\/ $CLUSTER /g\" roles/generate-inventory/tasks/main.yml ./generate-inventory.sh $CLUSTER fi tmp = $CLUSTER done sed -i \"s/{{ playbook_dir }}\\/ $tmp /{{ playbook_dir }}/g\" roles/generate-inventory_2/tasks/main.yml sed -i \"s/{{ playbook_dir }}\\/ $tmp /{{ playbook_dir }}/g\" roles/generate-inventory/tasks/main.yml popd The inventory files for for cluster will be created under */inventory/ . Besides, two loadBalancer_vars.yaml files will be created, one for each cluster. You may also want to check the Azure portal if the infrastructure was created correctly. The figure below shows for wc-test0 . Deploying vanilla Kubernetes clusters using Kubespray \u00b6 With the infrastructure provisioned, we can now deploy Kubernetes using kubespray. First, change to the compliantkubernetes-kubespray root directory. cd .. Init the Kubespray config in your config path \u00b6 export CK8S_CONFIG_PATH = ~/.ck8s/azure export CK8S_PGP_FP = <your GPG key fingerprint> # retrieve with gpg --list-secret-keys for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ./bin/ck8s-kubespray init $CLUSTER default $CK8S_PGP_FP done Copy the generated inventory files in the right location \u00b6 for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do #add calico to the inventory file cat kubespray/contrib/azurerm/ $CLUSTER /inventory/inventory.j2 \\ | sed '/\\[k8s_cluster:children\\]/i \\[calico-rr\\]' \\ > $CK8S_CONFIG_PATH / $CLUSTER -config/inventory.ini echo \"calico-rr\" >> $CK8S_CONFIG_PATH / $CLUSTER -config/inventory.ini $CK8S_CONFIG_PATH / $CLUSTER -config/inventory.ini # Add ansible_user ubuntu (note that this assumes you have set admin_username in azurerm/group_vars/all to ubuntu) echo -e 'ansible_user: ubuntu' >> $CK8S_CONFIG_PATH / $CLUSTER -config/group_vars/k8s_cluster/ck8s-k8s_cluster.yaml # Get the IP address of the loadbalancer (to be added in kubadmin certSANs list which will be used for kubectl) ip = $( grep -o '[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}' kubespray/contrib/azurerm/ $CLUSTER /loadbalancer_vars.yml ) echo 'supplementary_addresses_in_ssl_keys: [\"' $ip '\"]' >> $CK8S_CONFIG_PATH / $CLUSTER -config/group_vars/k8s_cluster/ck8s-k8s_cluster.yaml echo -e 'nameservers:\\n - 1.1.1.1' >> $CK8S_CONFIG_PATH / $CLUSTER -config/group_vars/k8s_cluster/ck8s-k8s_cluster.yaml echo 'resolvconf_mode: host_resolvconf' >> $CK8S_CONFIG_PATH / $CLUSTER -config/group_vars/k8s_cluster/ck8s-k8s_cluster.yaml done Run kubespray to deploy the Kubernetes clusters \u00b6 for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ./bin/ck8s-kubespray apply $CLUSTER --flush-cache done This may take up to 30 minutes per cluster. Please increase the value for timeout, e.g timeout=30 , in kubespray/ansible.cfg if you face the following issue while running step-3. TASK [bootstrap-os : Fetch /etc/os-release] **************************************************** fatal: [minion-0]: FAILED! => {\"msg\": \"Timeout (12s) waiting for privilege escalation prompt: \"} fatal: [minion-1]: FAILED! => {\"msg\": \"Timeout (12s) waiting for privilege escalation prompt: \"} fatal: [minion-2]: FAILED! => {\"msg\": \"Timeout (12s) waiting for privilege escalation prompt: \"} fatal: [master-0]: FAILED! => {\"msg\": \"Timeout (12s) waiting for privilege escalation prompt: \"} Correct the Kubernetes API IP addresses \u00b6 Get the public IP address of the loadbalancer: grep -o '[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}' kubespray/contrib/azurerm/ $CLUSTER /loadbalancer_vars.yml Locate the encrypted kubeconfigs kube_config_*.yaml and edit them using sops. Copy the IP shown above into kube_config_*.yaml . Do not overwrite the port. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml done Test access to the clusters as follows \u00b6 for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get nodes' done Deploy Rook \u00b6 To deploy Rook, please go to the compliantkubernetes-kubespray repo root directory and run the following. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops --decrypt ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml > $CLUSTER .yaml export KUBECONFIG = $CLUSTER .yaml ./rook/deploy-rook.sh shred -zu $CLUSTER .yaml done Please restart the operator pod, rook-ceph-operator* , if some pods stalls in initialization state as shown below: rook-ceph rook-ceph-crashcollector-minion-0-b75b9fc64-tv2vg 0/1 Init:0/2 0 24m rook-ceph rook-ceph-crashcollector-minion-1-5cfb88b66f-mggrh 0/1 Init:0/2 0 36m rook-ceph rook-ceph-crashcollector-minion-2-5c74ffffb6-jwk55 0/1 Init:0/2 0 14m Important Pods in pending state usually indicate resource shortage. In such cases you need to use bigger instances. Test Rook \u00b6 To test Rook, proceed as follows: for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml 'kubectl --kubeconfig {} apply -f https://raw.githubusercontent.com/rook/rook/release-1.5/cluster/examples/kubernetes/ceph/csi/rbd/pvc.yaml' ; done for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml 'kubectl --kubeconfig {} get pvc' ; done You should see PVCs in Bound state. If you want to clean the previously created PVCs: for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml 'kubectl --kubeconfig {} delete pvc rbd-pvc' ; done Deploying Compliant Kubernetes Apps \u00b6 Now that the Kubernetes clusters are up and running, we are ready to install the Compliant Kubernetes apps. Clone compliantkubernetes-apps and Install Pre-requisites \u00b6 If you haven't done so already, clone the compliantkubernetes-apps repo and install pre-requisites. git clone https://github.com/elastisys/compliantkubernetes-apps.git cd compliantkubernetes-apps ansible-playbook -e 'ansible_python_interpreter=/usr/bin/python3' --ask-become-pass --connection local --inventory 127 .0.0.1, get-requirements.yaml Initialize the apps configuration \u00b6 export CK8S_ENVIRONMENT_NAME = my-environment-name #export CK8S_FLAVOR=[dev|prod] # defaults to dev export CK8S_CONFIG_PATH = ~/.ck8s/my-cluster-path export CK8S_CLOUD_PROVIDER = # [exoscale|safespring|citycloud|aws|baremetal] export CK8S_PGP_FP = <your GPG key fingerprint> # retrieve with gpg --list-secret-keys ./bin/ck8s init This will initialise the configuration in the ${CK8S_CONFIG_PATH} directory. Generating configuration files sc-config.yaml and wc-config.yaml , as well as secrets with randomly generated passwords in secrets.yaml . This will also generate read-only default configuration under the directory defaults/ which can be used as a guide for available and suggested options. ls -l $CK8S_CONFIG_PATH Configure the apps \u00b6 Edit the configuration files ${CK8S_CONFIG_PATH}/sc-config.yaml , ${CK8S_CONFIG_PATH}/wc-config.yaml and ${CK8S_CONFIG_PATH}/secrets.yaml and set the appropriate values for some of the configuration fields. Note that, the latter is encrypted. vim ${ CK8S_CONFIG_PATH } /sc-config.yaml vim ${ CK8S_CONFIG_PATH } /wc-config.yaml sops ${ CK8S_CONFIG_PATH } /secrets.yaml Tip The default configuration for the service cluster and workload cluster are available in the directory ${CK8S_CONFIG_PATH}/defaults/ and can be used as a reference for available options. Warning Do not modify the read-only default configurations files found in the directory ${CK8S_CONFIG_PATH}/defaults/ . Instead configure the cluster by modifying the regular files ${CK8S_CONFIG_PATH}/sc-config.yaml and ${CK8S_CONFIG_PATH}/wc-config.yaml as they will override the default options. The following are the minimum change you should perform: # ${CK8S_CONFIG_PATH}/sc-config.yaml and ${CK8S_CONFIG_PATH}/wc-config.yaml global : baseDomain : \"set-me\" # set to <enovironment_name>.$DOMAIN opsDomain : \"set-me\" # set to ops.<environment_name>.$DOMAIN issuer : letsencrypt-prod objectStorage : type : \"s3\" s3 : region : \"set-me\" # Region for S3 buckets, e.g, west-1 regionEndpoint : \"set-me\" # e.g., https://s3.us-west-1.amazonaws.com storageClasses : default : rook-ceph-block nfs : enabled : false cinder : enabled : false local : enabled : false ebs : enabled : false # ${CK8S_CONFIG_PATH}/sc-config.yaml ingressNginx : controller : service : type : \"this-is-not-used\" annotations : \"this-is-not-used\" harbor : oidc : groupClaimName : \"set-me\" # set to group claim name used by OIDC provider issuers : letsencrypt : prod : email : \"set-me\" # set this to an email to receive LetsEncrypt notifications staging : email : \"set-me\" # set this to an email to receive LetsEncrypt notifications # ${CK8S_CONFIG_PATH}/secrets.yaml objectStorage : s3 : accessKey : \"set-me\" #set to your s3 accesskey secretKey : \"set-me\" #set to your s3 secretKey Install Compliant Kubernetes apps \u00b6 Start with the service cluster: ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ SERVICE_CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_sc.yaml ./bin/ck8s apply sc # Respond \"n\" if you get a WARN Then the workload clusters: for CLUSTER in \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_wc.yaml ./bin/ck8s apply wc # Respond \"n\" if you get a WARN done Settling \u00b6 Important Leave sufficient time for the system to settle, e.g., request TLS certificates from LetsEncrypt, perhaps as much as 20 minutes. You can check if the system settled as follows: for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get --all-namespaces pods' done Check the output of the command above. All Pods needs to be Running or Completed. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get --all-namespaces issuers,clusterissuers,certificates' done Check the output of the command above. All resources need to have the Ready column True. Testing \u00b6 After completing the installation step you can test if the apps are properly installed and ready using the commands below. Start with the service cluster: ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ SERVICE_CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_sc.yaml ./bin/ck8s test sc # Respond \"n\" if you get a WARN Then the workload clusters: for CLUSTER in \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_wc.yaml ./bin/ck8s test wc # Respond \"n\" if you get a WARN done Done. Navigate to the endpoints, for example grafana.$BASE_DOMAIN , kibana.$BASE_DOMAIN , harbor.$BASE_DOMAIN , etc. to discover Compliant Kubernetes's features. Teardown \u00b6 Removing Compliant Kubernetes Apps from your cluster \u00b6 To remove the applications added by compliant kubernetes you can use the two scripts clean-sc.sh and clean-wc.sh , they are located here in the scripts folder . They perform the following actions: Delete the added helm charts Delete the added namespaces Delete any remaining PersistentVolumes Delete the added CustomResourceDefinitions Note: if user namespaces are managed by Compliant Kubernetes apps then they will also be deleted if you clean up the workload cluster. Remove infrastructure \u00b6 To teardown the cluster, please go to the compliantkubernetes-kubespray repo root directory and run the following. pushd kubespray/contrib/azurerm for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ansible-playbook generate-templates.yml az group deployment create -g \" $CLUSTER \" --template-file ./ $CLUSTER /.generated/clear-rg.json --mode Complete done popd Further Reading \u00b6 Elastisys Compliant Kubernetes Kubespray Compliant Kubernetes apps repo Configurations option","title":"On Azure"},{"location":"operator-manual/break-glass/","text":"Work in progress \u00b6 Please check back soon!","title":"Breaking the Glass"},{"location":"operator-manual/capacity-management/","text":"Capacity Management \u00b6 Our users trust us -- the Compliant Kubernetes administrators -- to keep applications up and secure. Keeping the application up means that there is sufficient capacity in the environment, both for headroom -- in case the application suddenly gets popular -- and resilience to Node or Zone failure. Keeping the application secure means having sufficient capacity in the environment to allow rolling Node restarts -- as required for keeping the base OS up-to-date and secure -- without causing downtime. Types of Failure \u00b6 Compliant Kubernetes environments are set up to withstand either: a single Node failure (Node resilient); or a single Zone failure (Zone resilient). Zone resilient environments are set up over three Zones, two active and one arbiter. The arbiter only runs some control-plane components (e.g., Kubernetes Data Plane, Ceph Mon), whereas the active Zones run both control-plane and data-plane components (e.g., Ceph OSD, Kubernetes data plane Nodes). Upscaling \u00b6 When? \u00b6 Compliant Kubernetes triggers a P2 alert when any capacity dimension is predicted to exceed 66% (for Node resilient) or 50% (for Zone resilient) within 3 days. That was very information dense, so let's break it down. Why a P2 alert? P2 alerts are events that need to be dealt with within a business day. Capacity can be easily predicted and added in advance. Excess capacity is cheaper than frustrated administrators. There is no need to disturb anyone's sleep. What capacity dimensions? CPU: sum of (Kubernetes) CPU requests to CPU allocatable; sum of CPU used to CPU allocatable; load average: since this is not a percentage, scale up when above 3; Memory: sum of (Kubernetes) memory request to memory allocatable; sum of memory non-available to memory total; Storage: host disk used to size; PersistentVolumeClaim used to size; Rook/Ceph OSD used to size; Why 66% or 50%? Most Node-resilient Kubernetes clusters will feature 3 Nodes (see discussion below about too many Nodes). Hence, 1 extra Node means 66% capacity. Zone-resilient environments need 50% extra capacity, so that each active Zone can take over the load of the other active Zone. Why within 3 days? This should ensure sufficient time to act on the capacity shortage, without ruining anyone's weekend. Note Compliant Kubernetes can be configured to require resource requests for all Pods. Important Nodes dedicated for data services, such as PostgreSQL, are excluded from Kubernetes requests to allocatable calculation. How? \u00b6 Add a new Node of the same type as the other Nodes in the cluster. If the cluster has 6 Nodes, consider consolidating to 3 Nodes of twice-the-size -- in number of CPUs or memory or both -- if the infrastructure cost is reduced. Before doing this, get in touch with application developers to ensure they don't have Kubernetes scheduling constraints that would cause issues on the consolidated environment. If you are about to double the number of Nodes, get in touch with application developers to ensure their application is not misbehaving, before upscaling. Optimization \u00b6 If the cluster has at least 5 Nodes, consider reducing the watermark to 80% to reduce extra capacity. Downscaling \u00b6 We hope that the applications we host will only grow in popularity and that downscaling is never needed. Nevertheless, application developer trust us to keep infrastructure costs down, if their application hasn't gone viral -- yet! When? \u00b6 The capacity of the environment should be regularly reviewed, for example, after a maintenance window. Important Downscaling may put application uptime at risk. Therefore, be conservative when downscaling. Before downscaling you should: Evaluate the capacity trends in last 3 to 6 months and take decision based on that. Notice that capacity usage may be smaller during weekends, at the beginning or end of the month, during vacation periods, etc. Ask the user if the reduction in capacity usage is a real trend, and not just sporadic due to quiet periods or vacation periods. E.g., an EdTech app won't be used as intensively during school holidays. Ask the user if they foresee any increase in capacity due to new app releases or new apps additions or something that will require more resources. How? \u00b6 If any capacity dimension -- as defined above -- was below 33% for at least 3 days, then remove one Node at a time, until capacity is above 33%. Make sure to drain and cordon the Node before decommissioning it. If you are about to go below 3 Nodes, consider replacing the Nodes with 6 Nodes of half-the-size before downscaling. Before doing this, get in touch with application developers to ensure they don't have Kubernetes scheduling constraints that would cause issues on the consolidated environment. If you are about to half the number of Nodes, get in touch with application developers to ensure their application is not misbehaving, before downscaling. Optimization \u00b6 Removing capacity is more dangerous than having extra capacity, when it comes to application uptime. Furthermore, we need to avoid oscillations: Removing capacity to only add it back a few days later is no fun for the administrator. Therefore, downscaling should only be performed periodically, whereas upscaling should be performed as soon as predictions show it is needed.","title":"Capacity Management"},{"location":"operator-manual/clean-up/","text":"Removing Compliant Kubernetes Apps from your cluster \u00b6 To remove the applications added by Compliant Kubernetes you can use the two scripts clean-sc.sh and clean-wc.sh , they are located here in the scripts folder . They perform the following actions: Delete the added helm charts Delete the added namespaces Delete any remaining PersistentVolumes Delete the added CustomResourceDefinitions Note If user namespaces are managed by Compliant Kubernetes apps then they will also be deleted if you clean up the workload cluster.","title":"Remove Compliant Kubernetes Apps"},{"location":"operator-manual/cluster-sizing/","text":"Cluster Sizing \u00b6 A full Compliant Kubernetes deployment requires a cluster with at least 40 CPUs and 82 GB of memory in total. Monitoring \u00b6 Monitoring stack (InfluxDB) can handle 2500 metrics per second while provisioned with 4 CPUs and 16 GB of memory. Logging \u00b6 Logging stack (OpenSearch) can take 100 records per second while provisioned with 12 CPUs and 24 GB of memory.","title":"Sizing"},{"location":"operator-manual/common/","text":"Deploy Rook \u00b6 To deploy Rook, please go to the compliantkubernetes-kubespray repo root directory and run the following. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops --decrypt ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml > $CLUSTER .yaml export KUBECONFIG = $CLUSTER .yaml ./rook/deploy-rook.sh shred -zu $CLUSTER .yaml done Please restart the operator pod, rook-ceph-operator* , if some pods stalls in initialization state as shown below: rook-ceph rook-ceph-crashcollector-minion-0-b75b9fc64-tv2vg 0/1 Init:0/2 0 24m rook-ceph rook-ceph-crashcollector-minion-1-5cfb88b66f-mggrh 0/1 Init:0/2 0 36m rook-ceph rook-ceph-crashcollector-minion-2-5c74ffffb6-jwk55 0/1 Init:0/2 0 14m Important Pods in pending state usually indicate resource shortage. In such cases you need to use bigger instances. Test Rook \u00b6 To test Rook, proceed as follows: for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml 'kubectl --kubeconfig {} apply -f https://raw.githubusercontent.com/rook/rook/release-1.5/cluster/examples/kubernetes/ceph/csi/rbd/pvc.yaml' ; done for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml 'kubectl --kubeconfig {} get pvc' ; done You should see PVCs in Bound state. If you want to clean the previously created PVCs: for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml 'kubectl --kubeconfig {} delete pvc rbd-pvc' ; done Now that the Kubernetes clusters are up and running, we are ready to install the Compliant Kubernetes apps. Clone compliantkubernetes-apps and Install Pre-requisites \u00b6 If you haven't done so already, clone the compliantkubernetes-apps repo and install pre-requisites. git clone https://github.com/elastisys/compliantkubernetes-apps.git cd compliantkubernetes-apps ansible-playbook -e 'ansible_python_interpreter=/usr/bin/python3' --ask-become-pass --connection local --inventory 127 .0.0.1, get-requirements.yaml Initialize the apps configuration \u00b6 export CK8S_ENVIRONMENT_NAME = my-environment-name #export CK8S_FLAVOR=[dev|prod] # defaults to dev export CK8S_CONFIG_PATH = ~/.ck8s/my-cluster-path export CK8S_CLOUD_PROVIDER = # [exoscale|safespring|citycloud|aws|baremetal] export CK8S_PGP_FP = <your GPG key fingerprint> # retrieve with gpg --list-secret-keys ./bin/ck8s init This will initialise the configuration in the ${CK8S_CONFIG_PATH} directory. Generating configuration files sc-config.yaml and wc-config.yaml , as well as secrets with randomly generated passwords in secrets.yaml . This will also generate read-only default configuration under the directory defaults/ which can be used as a guide for available and suggested options. ls -l $CK8S_CONFIG_PATH Configure the apps \u00b6 Edit the configuration files ${CK8S_CONFIG_PATH}/sc-config.yaml , ${CK8S_CONFIG_PATH}/wc-config.yaml and ${CK8S_CONFIG_PATH}/secrets.yaml and set the appropriate values for some of the configuration fields. Note that, the latter is encrypted. vim ${ CK8S_CONFIG_PATH } /sc-config.yaml vim ${ CK8S_CONFIG_PATH } /wc-config.yaml sops ${ CK8S_CONFIG_PATH } /secrets.yaml Tip The default configuration for the service cluster and workload cluster are available in the directory ${CK8S_CONFIG_PATH}/defaults/ and can be used as a reference for available options. Warning Do not modify the read-only default configurations files found in the directory ${CK8S_CONFIG_PATH}/defaults/ . Instead configure the cluster by modifying the regular files ${CK8S_CONFIG_PATH}/sc-config.yaml and ${CK8S_CONFIG_PATH}/wc-config.yaml as they will override the default options. Install Compliant Kubernetes apps \u00b6 Start with the service cluster: ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ SERVICE_CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_sc.yaml ./bin/ck8s apply sc # Respond \"n\" if you get a WARN Then the workload clusters: for CLUSTER in \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_wc.yaml ./bin/ck8s apply wc # Respond \"n\" if you get a WARN done Settling \u00b6 Important Leave sufficient time for the system to settle, e.g., request TLS certificates from LetsEncrypt, perhaps as much as 20 minutes. You can check if the system settled as follows: for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get --all-namespaces pods' done Check the output of the command above. All Pods needs to be Running or Completed. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get --all-namespaces issuers,clusterissuers,certificates' done Check the output of the command above. All resources need to have the Ready column True. Testing \u00b6 After completing the installation step you can test if the apps are properly installed and ready using the commands below. Start with the service cluster: ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ SERVICE_CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_sc.yaml ./bin/ck8s test sc # Respond \"n\" if you get a WARN Then the workload clusters: for CLUSTER in \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_wc.yaml ./bin/ck8s test wc # Respond \"n\" if you get a WARN done Done. Navigate to the endpoints, for example grafana.$BASE_DOMAIN , kibana.$BASE_DOMAIN , harbor.$BASE_DOMAIN , etc. to discover Compliant Kubernetes's features. Removing Compliant Kubernetes Apps from your cluster \u00b6 To remove the applications added by compliant kubernetes you can use the two scripts clean-sc.sh and clean-wc.sh , they are located here in the scripts folder . They perform the following actions: Delete the added helm charts Delete the added namespaces Delete any remaining PersistentVolumes Delete the added CustomResourceDefinitions Note: if user namespaces are managed by Compliant Kubernetes apps then they will also be deleted if you clean up the workload cluster. Create S3 buckets \u00b6 You can use the following script to create required S3 buckets. The script uses s3cmd in the background and gets configuration and credentials for your S3 provider from ${HOME}/.s3cfg file. # Use your default s3cmd config file: ${HOME}/.s3cfg scripts/S3/entry.sh create Important You should not use your own credentials for S3. Rather create a new set of credentials with write-only access, when supported by the object storage provider ( check a feature matrix ). Test S3 \u00b6 To ensure that you have configured S3 correctly, run the following snippet: ( access_key = $( sops exec-file ${ CK8S_CONFIG_PATH } /secrets.yaml 'yq r {} \"objectStorage.s3.accessKey\"' ) secret_key = $( sops exec-file ${ CK8S_CONFIG_PATH } /secrets.yaml 'yq r {} \"objectStorage.s3.secretKey\"' ) sc_config = $( yq m ${ CK8S_CONFIG_PATH } /defaults/sc-config.yaml ${ CK8S_CONFIG_PATH } /sc-config.yaml -a overwrite -x ) region = $( echo ${ sc_config } | yq r - 'objectStorage.s3.region' ) host = $( echo ${ sc_config } | yq r - 'objectStorage.s3.regionEndpoint' ) for bucket in $( echo ${ sc_config } | yq r - 'objectStorage.buckets.*' ) ; do s3cmd --access_key = ${ access_key } --secret_key = ${ secret_key } \\ --region = ${ region } --host = ${ host } \\ ls s3:// ${ bucket } > /dev/null [ ${ ? } = 0 ] && echo \"Bucket ${ bucket } exists!\" done ) This page is out of date We are currently working on internal documentation to streamline Compliant Kubernetes onboarding for selected cloud providers. Until those documents are ready, and until we have capacity to make parts of that documentation public, this page is out-of-date. Nevertheless, parts of it are useful. Use at your own risk and don't expect things to work smoothly.","title":"Common"},{"location":"operator-manual/configure/","text":"Advanced Configuration \u00b6 You have already been exposed to some of Compliant Kubernetes's configuration options while creating a cluster. If not, read that section first (e.g., On Exoscale ). This section will outline some advanced configuration topics. Overview \u00b6 Compliant Kubernetes is composed of two layers, the Kubernetes layer and the apps layer. Each is configured slightly differently. Kubernetes Layer \u00b6 To find all configuration option of the Kubernetes layer, please read the upstream Kubespray documentation. Compliant Kubernetes overrides some of Kubespray's defaults, as shown here . Apps Layer \u00b6 The configuration of the apps layer is documented as comments in the sc-config.yaml , wc-config.yaml and secrets.yaml files generated by ./bin/ck8s init . You can find the default here . If you find a configuration key is insufficiently documented, please open a PR .","title":"Overview"},{"location":"operator-manual/credentials/","text":"Use of Credentials \u00b6 Compliant Kubernetes interacts with a lot of credentials. This document captures all of them in an orderly fashion, layer-by-layer. Terminology \u00b6 Purpose: Why are these credentials necessary, what can be done with them. Owner: The person (e.g., John Smith) or computing system (e.g., control plane Node, Pod) who controls the credentials, and is responsible for their safe storage and usage. Type: Individual credentials identify a person, while service accounts identify a computing system. Use for: What should these credentials be used for. Do not use for: When should these credentials NOT be used, although they technically could. Single Sign-On (SSO) Credentials \u00b6 Example: Company Google Accounts Purpose: authenticate a person with various system, in particular Kubernetes API via Dex Grafana via Dex OpenSearch Dashboards via Dex Harbor via Dex Owner: individual person (user or administrator) Type: individual credentials Use for: identifying yourself Do not use for: These credentials are super valuable and should not be shared with anyone, not even family, friends, workmates, etc., even if requested. Report such sharing requests. Misc: Protect using 2FA Cloud Provider (Infrastructure) Credentials \u00b6 Purpose: create infrastructure, e.g., VMs, load balancers, networks, buckets. Owner: administrator Type: individual credentials Use for: Terraform layer in Kubespray Creating and destroying buckets via helper scripts Do not use for: Kubernetes cloud-controller integration , use Cloud Controller Credentials instead. Access to object storage / S3 bucket, use backup credentials instead. SSH Keys \u00b6 Purpose: access Nodes for setup, break glass or disaster recovery Owner: administrator Type: individual credentials Use for: Accessing Nodes via SSH Do not use for: Giving a system access to a Git repository. Create a separate SSH key only for that purpose instead. PGP Keys \u00b6 Purpose: encrypt/decrypt sensitive information, e.g., service account credentials, customer names, incident reports, financial information, etc. Owner: administrator Type: individual credentials Use for: Encrypting/decrypting sensitive information Do not use for: Encrypting/decrypting individual credentials. These are meant to be individual and never shared. Encrypting/decrypting SSH key. These are meant to be individual and never shared. Prefer protecting your SSH key with a passphrase or storing it on a YubiKey . Encrypting non-sensitive information. This leads to a culture of \"security by obscurity\" in which people over-rely on encryption. Prefer being mindful about what data you store and why. If unsure, prefer not storing credentials, as Cloud Provider Credentials and SSH keys should be enough to restore any access. Cloud Controller (Integration) Credentials \u00b6 Purpose: allow Kubernetes control Nodes, specifically the cloud-controller-manager , to create LoadBalancers and PersistentVolumes Owner: each Kubernetes cluster should have their own Type: service account Use for: Configuring Kubespray to set up a Kubernetes cluster with cloud integration Do not use for: AWS. Use AWS IAM Node Roles instead. Exoscale. We currently don't integrate with Exoscale for LoadBalancer or PersistentVolumes. Terraform layer in Kubespray Backup and Long-Term Logging Credentials \u00b6 Purpose: Allow backup of various components, e.g., PVCs via Velero, InfluxDB metrics, OpenSearch Indexes, PostgreSQL databases. Allow long-term logging, e.g., Service Cluster logs Owner: each Compliant Kubernetes cluster should have their own Type: service account Use for: Backup Logging Do not use for: Other object storage, e.g., Harbor container images Disaster recovery, investigations. Use Cloud Provider credentials instead. Misc: Ensure these credentials are write-only , if supported by the underlying cloud provider, to comply with ISO 27001 A.12.3.1 Information Backup and ISO 27001 A.12.4.2 Protection of Log Information . As of 2021-05-20, this is supported by AWS S3, Exoscale S3, GCP and SafeSpring S3. OpsGenie Credentials \u00b6 Purpose: Allow the Cluster to issue alerts to OpsGenie. Owner: each Compliant Kubernetes cluster should have their own Type: service account Use for: alerting Do not use for: Operator access to OpsGenie. Prefer Single Sign-On (SSO) . Dex OpenID Client Secret \u00b6 Purpose: Complete the \"OAuth dance\" between Grafana, OpenSearch Dashboard, Harbor and kubectl, on one side, and Dex, on the other side. Used both by administrators and users. Owner: each Compliant Kubernetes cluster should have their own Type: not secret Misc: We have determined that the OpenID client secret should not be treated as a secret. See risk analysis here and here . Kubeconfig with OpenID Authentication \u00b6 Purpose: access the Kubernetes API in normal situations Owner: shared between administrators and users Type: not secret Use for: Routine checks Routine maintenance Investigations \"Simple\" recovery Misc: If these credentials become unusable, you are in a \"break glass\" situation. Use cloud provider credentials or SSH keys to initiate disaster recovery. Kubeconfig with Client Certificate Key \u00b6 Purpose: access the Kubernetes API for disaster recovery, break glass or initial setup Owner: shared between administrators Type: special Use for: Initial setup Break glass Disaster recovery Do not use for: Routine maintenance or investigation. Use Kubeconfig with OpenID Authentication Misc: Such a Kubeconfig is available on all control plane Nodes at /etc/kubernetes/admin.conf . SSH into a control plane Node then type sudo su and you can readily use kubectl commands. Unless absolutely necessary, avoid storing this file outside the control plane Nodes. If, for some good reason, you downloaded this file, shred it after usage.","title":"Use of Credentials"},{"location":"operator-manual/disaster-recovery/","tags":["ISO 27001 A.12.3.1","ISO 27001 A.17.1.1"],"text":"Disaster Recovery \u00b6 This document details disaster recovery procedures for Compliant Kubernetes. These procedures must be executed by the administrator. Compliant Need \u00b6 Disaster recovery is mandated by several regulations and information security standards. For example, in ISO 27001:2013, the annexes that mostly concerns disaster recovery are: A.12.3.1 Information Backup A.17.1.1 Planning Information Security Continuity Object storage providers \u00b6 Feature matrix \u00b6 Provider Write-only credentials AWS S3 Yes Citycloud S3 No Exoscale S3 Yes GCP Yes Safespring S3 Yes Off-site backups \u00b6 Backups can be set up to be replicated off-site using CronJobs. In version v0.23 these can be encrypted before they are sent off-site, which means they must first be restored to be usable. It is possible to restore serviecs directly from unencrypted off-site backups with some additional steps. See the instructions in compliantkubernetes-apps for how to restore off-site backups. OpenSearch \u00b6 Note For Open Distro for Elasticsearch used in v0.18 and earlier, the same approach for backup and restore can be used with different naming, using elasticsearch for the CronJob and elastic for the namespace instead of opensearch . Backup \u00b6 OpenSearch is set up to store backups in an S3 bucket. There is a CronJob called opensearch-backup in the cluster that is invoking the snapshot process in OpenSearch. To take a snapshot on-demand, execute ./bin/ck8s ops kubectl sc -n opensearch-system create job --from=cronjob/opensearch-backup <name-of-job> Restore \u00b6 Set the following variables user - OpenSearch user with permissions to manage snapshots, usually snapshotter password - password for the above user os_url - URL to OpenSearch Restoring from off-site backup To restore from an encrypted off-site backup: First import the backup into the main S3 service and register the restored bucket as a new snapshot repository: curl -kL -u \" ${ user } : ${ password } \" -X PUT \" ${ os_url } /_snapshot/backup-repository?pretty\" -H 'Content-Type: application/json' -d ' { \"type\": \"s3\", \"settings\": { \"bucket\": \"<restored-bucket>\", \"readonly\": true } } ' Then restore from this snapshot repository ( backup-repositroy ) in OpenSearch. To restore from an unencrypted off-site backup: Configure the remote and bucket as the main S3 service and bucket for apps and OpenSearch respectively, then update the OpenSearch Helm releases and perform the restore. It is recommended to either suspend or remove the OpenSearch backup CronJob to prevent it from running while restoring. Remember to revert to the regular S3 service afterwards and reactivate the backup CronJob! Replace the previous snapshot repository if it is unusable. List snapshot repositories # Simple \u276f curl -kL -u \" ${ user } : ${ password } \" \" ${ os_url } /_cat/repositories?v\" id type opensearch-snapshots s3 # Detailed \u276f curl -kL -u \" ${ user } : ${ password } \" \" ${ os_url } /_snapshot/?pretty\" { \"opensearch-snapshots\" : { \"type\" : \"s3\" , \"settings\" : { \"bucket\" : \"opensearch-backup\" , \"client\" : \"default\" } } } List available snapshots snapshot_repo = <name/id from previous step> # Simple \u276f curl -kL -u \" ${ user } : ${ password } \" \" ${ os_url } /_cat/snapshots/ ${ snapshot_repo } ?v&s=id\" id status start_epoch start_time end_epoch end_time duration indices successful_shards failed_shards total_shards snapshot-20211231_120002z SUCCESS 1640952003 12 :00:03 1640952082 12 :01:22 1 .3m 54 54 0 54 snapshot-20220101_000003z SUCCESS 1640995203 00 :00:03 1640995367 00 :02:47 2 .7m 59 59 0 59 snapshot-20220101_120002z SUCCESS 1641038403 12 :00:03 1641038533 12 :02:13 2 .1m 57 57 0 57 ... # Detailed list of all snapshots curl -kL -u \" ${ user } : ${ password } \" \" ${ os_url } /_snapshot/ ${ snapshot_repo } /_all?pretty\" # Detailed list of specific snapshot \u276f curl -kL -u \" ${ user } : ${ password } \" \" ${ os_url } /_snapshot/ ${ snapshot_repo } /snapshot-20220104_120002z?pretty\" {{ \"snapshots\" : [ { \"snapshot\" : \"snapshot-20220104_120002z\" , \"uuid\" : \"oClQdNAyTeiEmZb5dVh0SQ\" , \"version_id\" : 135238127 , \"version\" : \"1.2.3\" , \"indices\" : [ \"authlog-default-2021.12.20-000001\" , \"authlog-default-2021.12.30-000011\" , \"authlog-default-2022.01.03-000015\" , \"other-default-2021.12.30-000011\" , ... ] , \"data_streams\" : [ ] , \"include_global_state\" : false, \"state\" : \"SUCCESS\" , \"start_time\" : \"2022-01-04T12:00:02.596Z\" , \"start_time_in_millis\" : 1641297602596 , \"end_time\" : \"2022-01-04T12:01:07.833Z\" , \"end_time_in_millis\" : 1641297667833 , \"duration_in_millis\" : 65237 , \"failures\" : [ ] , \"shards\" : { \"total\" : 66 , \"failed\" : 0 , \"successful\" : 66 } } ] } You usually select the latest snapshot containing the indices you want to restore. Restore one or multiple indices from a snapshot Note You cannot restore a write index (the latest index) if you already have a write index connected to the same index alias (which will happen if you have started to receive logs). snapshot_name = <Snapshot name from previous step> indices = \"<list of comma separated indices/index patterns>\" curl -kL -u \" ${ user } : ${ password } \" -X POST \" ${ os_url } /_snapshot/ ${ snapshot_repo } / ${ snapshot_name } /_restore?pretty\" -H 'Content-Type: application/json' -d ' { \"indices\": \"' ${ indices } '\" } ' Read the documentation to see the API, all parameters and their explanations. Restoring OpenSearch Dashboards data \u00b6 Data in OpenSearch Dashboards (saved searches, visualizations, dashboards, etc) is stored in the index .opensearch_dashboards_1 . To restore that data you first need to delete the index and then do a restore. This will overwrite anything in the current .opensearch_dashboards_1 index. If there is something new that should be saved, then export the saved objects and import them after the restore. snapshot_name = <Snapshot name from previous step> curl -kL -u \" ${ user } : ${ password } \" -X DELETE \" ${ os_url } /.opensearch_dashboards_1?pretty\" curl -kL -u \" ${ user } : ${ password } \" -X POST \" ${ os_url } /_snapshot/ ${ snapshot_repo } / ${ snapshot_name } /_restore?pretty\" -H 'Content-Type: application/json' -d ' { \"indices\": \"' .opensearch_dashboards_1 '\" } ' Note For Open Distro for Elasticsearch and Kibana used in v0.18 and earlier, the same approach can be used with different naming, using .kibana_1 for the index name instead of .opensearch_dashboards_1 . Start new cluster from snapshot \u00b6 This process is very similar to the one described above, but there are a few extra steps to carry out. Before you install OpenSearch you can preferably disable the initial index creation to make the restore process leaner by setting the following configuration option: opensearch.createIndices: false Install the OpenSearch suite: ./bin/ck8s ops helmfile sc -l group = opensearch apply Wait for the the installation to complete. After the installation, go back up to the Restore section to proceed with the restore. If you want to restore all indices, use the following indices variable indices = \"kubernetes-*,kubeaudit-*,other-*\" Note This process assumes that you are using the same S3 bucket as your previous cluster. If you aren't: Register a new S3 snapshot repository to the old bucket as described here Use the newly registered snapshot repository in the restore process Harbor \u00b6 Backup \u00b6 Harbor is set up to store backups of the database in an S3 bucket (note that this does not include the actual images, since those are already stored in S3 by default). There is a CronJob called harbor-backup-cronjob in the cluster that is taking a database dump and uploading it to a S3 bucket. To take a backup on-demand, execute ./bin/ck8s ops kubectl sc -n harbor create job --from = cronjob/harbor-backup-cronjob <name-of-job> Restore \u00b6 Restoring from off-site backup Since Harbor stores both database backups and images in the same bucket it is recommended to restore the off-site backup into the main S3 service first, reconfigure Harbor to use it, then restore the database from it. Instructions for how to restore Harbor can be found in compliantkubernetes-apps : https://github.com/elastisys/compliantkubernetes-apps/tree/main/scripts/restore#restore-harbor Velero \u00b6 These instructions make use of the Velero CLI, you can download it here: https://github.com/vmware-tanzu/velero/releases/tag/v1.7.1 (version 1.7.1). The CLI needs the env variable KUBECONFIG set to the path of a decrypted kubeconfig. Read more about Velero here: https://compliantkubernetes.io/user-guide/backup/ Note This documentation uses the Velero CLI, as opposed to Velero CRDs, since that is what is encouraged by upstream documentation. Backup \u00b6 Velero is set up to take daily backups and store them in an S3 bucket. The daily backup will not take backups of everything in a kubernetes cluster, it will instead look for certain labels and annotations. Read more about those labels and annotations here: https://compliantkubernetes.io/user-guide/backup/#backing-up It is also possible to take on-demand backups. Then you can freely chose what to backup and do not have to base it on the same labels. A basic example with the Velero CLI would be velero backup create manual-backup , which would take a backup of all kubernetes resources (though not the data in the volumes by default). Check which arguments you can use by running velero backup create --help . Restore \u00b6 Restoring from a backup with Velero is meant to be a type of disaster recovery. Velero will not overwrite existing Resources when restoring. As such, if you want to restore the state of a Resource that is still running, the Resource must be deleted first. To restore the state from the latest daily backup, run: velero restore create --from-schedule velero-daily-backup --wait This command will wait until the restore has finished. You can also do partial restorations, e.g. just restoring one namespace, by using different arguments. You can also restore from manual backups by using the flag --from-backup <backup-name> Persistent Volumes are only restored if a Pod with the backup annotation is restored. Multiple Pods can have an annotation for the same Persistent Volume. When restoring the Persistent Volume it will overwrite any existing files with the same names as the files to be restored. Any other files will be left as they were before the restoration started. So a restore will not wipe the volume clean and then restore. If a clean wipe is the desired behavior, then the volume must be wiped manually before restoring. Restore from off-site backup \u00b6 Restoring from encrypted off-site backup: Recover the encrypted bucket into the main S3 service and reconfigure Velero to use this bucket, then follow the regular instructions. The references in Kubernetes might need to be deleted so Velero can resync from the bucket: # Note that this is only backup metadata kubectl -n velero delete backups.velero.io --all Restoring from unencrypted off-site backup: To recover directly from off-site backup the backup-location must be reconfigured: export S3_BUCKET = \"<off-site-s3-bucket>\" export S3_PREFIX = \"<service-cluster|workload-cluster>\" export S3_ACCESS_KEY = $( sops -d --extract '[\"objectStorage\"][\"sync\"][\"s3\"][\"accessKey\"]' ) \" $CK8S_CONFIG_PATH /secrets.yaml\" ) export S3_SECRET_KEY = $( sops -d --extract '[\"objectStorage\"][\"sync\"][\"s3\"][\"accessKey\"]' ) \" $CK8S_CONFIG_PATH /secrets.yaml\" ) export S3_REGION = $( yq r \" $CK8S_CONFIG_PATH /sc-config.yaml\" \"objectStorage.sync.s3.region\" ) export S3_ENDPOINT = $( yq r \" $CK8S_CONFIG_PATH /sc-config.yaml\" \"objectStorage.sync.s3.regionEndpoint\" ) export S3_PATH_STYLE = $( yq r \" $CK8S_CONFIG_PATH /sc-config.yaml\" \"objectStorage.sync.s3.forcePathStyle\" ) # Delete default backup location velero backup-location delete default # Delete backups from default backup location, note that this is only the backup metadata kubectl -n velero delete backups.velero.io --all # Create off-site credentials kubectl -n velero create secret generic velero-backup \\ --from-literal = cloud = \" $( echo \"[default]\\naws_access_key_id: ${ S3_ACCESS_KEY } \\naws_secret_access_key: ${ S3_SECRET_KEY } \\n\" ) \" # Create off-site backup location velero backup-location create backup \\ --access-mode ReadOnly \\ --provider aws \\ --bucket \" ${ S3_BUCKET } \" \\ --prefix \" ${ S3_PREFIX } \" \\ --config = \"region= ${ S3_REGION } ,s3Url= ${ S3_ENDPOINT } ,s3ForcePathStyle= ${ S3_PATH_STYLE } \" \\ --credential = velero-backup = cloud Check that the backup-location becomes available: $ velero backup-location get NAME PROVIDER BUCKET/PREFIX PHASE LAST VALIDATED ACCESS MODE DEFAULT backup aws <bucket>/<prefix> Available <timestamp> ReadOnly Then check that the backups becomes available using velero backup get . When they are available restore one of them using velero restore create <name-of-restore> --from-backup <name-of-backup> . After the restore is complete Velero should be reconfigured to use the main S3 service again, with a new bucket if the previous one is unusable. Updating or syncing the Helm chart will reset the backup location. The secret and the backup metadata from the off-site backups can be deleted: kubectl -n velero delete secret velero-backup kubectl -n velero delete backups.velero.io --all Grafana \u00b6 This refers to the user Grafana, not the ops Grafana. Backup \u00b6 Grafana is set up to be included in the daily Velero backup. We then include the Grafana deployment, pod, and PVC (including the data). Manual backups can be taken using velero (include the same resources). Restore \u00b6 To restore the Grafana backup you must: Have Grafana installed Delete the grafana deployment, PVC and PV kubectl delete deploy -n monitoring user-grafana kubectl delete pvc -n monitoring user-grafana Restore the velero backup velero restore create --from-schedule velero-daily-backup --wait You can also restore Grafana by setting restore.velero in your {CK8S_CONFIG_PATH}/sc-config.yaml to true , and then reapply the service cluster apps: .bin/ck8s apply sc This will go through the same steps as above. By default, the latest daily backup is chosen; to restore from a different backup, set restore.veleroBackupName to the desired backup name.","title":"Disaster Recovery"},{"location":"operator-manual/eksd/","text":"This page is out of date We are currently working on internal documentation to streamline Compliant Kubernetes onboarding for selected cloud providers. Until those documents are ready, and until we have capacity to make parts of that documentation public, this page is out-of-date. Nevertheless, parts of it are useful. Use at your own risk and don't expect things to work smoothly. Compliant Kubernetes on EKS-D based clusters \u00b6 This document contains instructions on how to install Compliant Kubernetes on AWS using EKS-D . Note This guide is written for compliantkubernetes-apps v0.13.0 Requirements \u00b6 An AWS account with billing enabled. A hosted zone in Route53. yq v3.4.1 installed on you machine. gpg2 installed on your machine with at least one key available. kubectl installed on your machine. Infrastructure and Kubernetes \u00b6 Get EKS-D \u00b6 git clone https://github.com/aws/eks-distro.git cd eks-distro/development/kops git checkout v1-19-eks-1 Configure your AWS environment \u00b6 Follow the instructions in Getting Started with kOps on AWS up until you reach Creating your first cluster . Unless you have very specific requirements you shouldn't need to take any action when it comes to the DNS configuration . If you followed the instructions you should have: An IAM user for kOps with the correct permissions. Set AWS credentials and any other AWS environment variables you require in your shell. An S3 bucket for storing the kOps cluster state. Create initial kOps cluster configurations \u00b6 export AWS_REGION=<region where you want the infrastructure to be created> export KOPS_STATE_STORE=s3://<name of the bucket you created in previous step> SERVICE_CLUSTER=\"<xyz, e.g. test-sc>.<your hosted zone in Route53, e.g. example.com>\" WORKLOAD_CLUSTER=\"<xyz, e.g. test-wc>.<your hosted zone in Route53, e.g. example.com>\" for CLUSTER in ${SERVICE_CLUSTER} ${WORKLOAD_CLUSTER}; do export KOPS_CLUSTER_NAME=${CLUSTER} ./create_values_yaml.sh ./create_configuration.sh done Modify kOps cluster configurations \u00b6 for CLUSTER in ${SERVICE_CLUSTER} ${WORKLOAD_CLUSTER}; do echo ' --- - command: update path: spec.etcdClusters[0].manager value: env: - name: ETCD_LISTEN_METRICS_URLS value: http://0.0.0.0:8081 - name: ETCD_METRICS value: basic - command: update path: spec.networking value: calico: encapsulationMode: ipip - command: update path: spec.metricsServer.enabled value: false - command: update path: spec.kubeAPIServer value: image: public.ecr.aws/eks-distro/kubernetes/kube-apiserver:v1.19.6-eks-1-19-1 auditLogMaxAge: 7 auditLogMaxBackups: 1 auditLogMaxSize: 100 auditLogPath: /var/log/kubernetes/audit/kube-apiserver-audit.log auditPolicyFile: /srv/kubernetes/audit/policy-config.yaml enableAdmissionPlugins: - \"PodSecurityPolicy\" - \"NamespaceLifecycle\" - \"LimitRanger\" - \"ServiceAccount\" - \"DefaultStorageClass\" - \"DefaultTolerationSeconds\" - \"MutatingAdmissionWebhook\" - \"ValidatingAdmissionWebhook\" - \"ResourceQuota\" - \"NodeRestriction\" - command: update path: spec.fileAssets value: - name: audit-policy-config path: /srv/kubernetes/audit/policy-config.yaml roles: - Master content: | apiVersion: audit.k8s.io/v1 kind: Policy rules: - level: RequestResponse resources: - group: \"\" resources: [\"pods\"] - level: Metadata resources: - group: \"\" resources: [\"pods/log\", \"pods/status\"] - level: None resources: - group: \"\" resources: [\"configmaps\"] resourceNames: [\"controller-leader\"] - level: None users: [\"system:kube-proxy\"] verbs: [\"watch\"] resources: - group: \"\" # core API group resources: [\"endpoints\", \"services\"] - level: None userGroups: [\"system:authenticated\"] nonResourceURLs: - \"/api*\" # Wildcard matching. - \"/version\" - level: Request resources: - group: \"\" # core API group resources: [\"configmaps\"] namespaces: [\"kube-system\"] - level: Metadata resources: - group: \"\" # core API group resources: [\"secrets\", \"configmaps\"] - level: Request resources: - group: \"\" # core API group - group: \"extensions\" # Version of group should NOT be included. - level: Metadata omitStages: - \"RequestReceived\" ' | yq w -i -s - ${CLUSTER}/${CLUSTER}.yaml done # Configure OIDC flags for kube-apiserver. for CLUSTER in ${WORKLOAD_CLUSTERS}; do yq w -i ${CLUSTER}/${CLUSTER}.yaml 'spec.kubeAPIServer.oidcIssuerURL' https://dex.${SERVICE_CLUSTER} yq w -i ${CLUSTER}/${CLUSTER}.yaml 'spec.kubeAPIServer.oidcUsernameClaim' email yq w -i ${CLUSTER}/${CLUSTER}.yaml 'spec.kubeAPIServer.oidcClientID' kubelogin done # Use bigger machines for service cluster worker nodes. yq w -i -d2 ${SERVICE_CLUSTER}/${SERVICE_CLUSTER}.yaml 'spec.machineType' t3.large # Update kOps cluster configurations in state bucket. for CLUSTER in ${SERVICE_CLUSTER} ${WORKLOAD_CLUSTER}; do ./bin/kops-1-19 replace -f \"./${CLUSTER}/${CLUSTER}.yaml\" done Create clusters \u00b6 for CLUSTER in ${SERVICE_CLUSTER} ${WORKLOAD_CLUSTER}; do export KOPS_CLUSTER_NAME=${CLUSTER} ./create_cluster.sh done The creation of the clusters might take anywhere from 5 minutes to 20 minutes. You should run the ./cluster_wait.sh script against all of your clusters as it creates a configmap needed by the aws-iam-authenticator pod, e.g. for CLUSTER in ${SERVICE_CLUSTER} ${WORKLOAD_CLUSTER}; do export KOPS_CLUSTER_NAME=${CLUSTER} kubectl config use-context ${CLUSTER} timeout 600 ./cluster_wait.sh done Compliant Kubernetes Apps \u00b6 Get Compliant Kubernetes Apps \u00b6 git clone git@github.com:elastisys/compliantkubernetes-apps cd compliantkubernetes-apps git checkout v0.13.0 Install requirements \u00b6 ansible-playbook -e 'ansible_python_interpreter=/usr/bin/python3' --ask-become-pass --connection local --inventory 127.0.0.1, get-requirements.yaml Initialize configuration \u00b6 export CK8S_ENVIRONMENT_NAME=aws-eks-d #export CK8S_FLAVOR=[dev|prod] # defaults to dev export CK8S_CONFIG_PATH=~/.ck8s/aws-eks-d export CK8S_CLOUD_PROVIDER=aws export CK8S_PGP_FP=<your GPG key ID> # retrieve with gpg --list-secret-keys ./bin/ck8s init Three files, sc-config.yaml and wc-config.yaml , and secrets.yaml , were generated in the ${CK8S_CONFIG_PATH} directory. ls -l ${CK8S_CONFIG_PATH} Edit configuration files \u00b6 Edit the configuration files sc-config.yaml , wc-config.yaml and secrets.yaml and set the appropriate values for some of the configuration fields. Note that, the latter is encrypted. vim ${CK8S_CONFIG_PATH}/sc-config.yaml vim ${CK8S_CONFIG_PATH}/wc-config.yaml sops ${CK8S_CONFIG_PATH}/secrets.yaml You should perform the following changes: # sc-config.yaml global: baseDomain: \"set-me\" # Set to ${SERVICE_CLUSTER} opsDomain: \"set-me\" # Set to ops.${SERVICE_CLUSTER} issuer: letsencrypt-prod verifyTls: true clusterDNS: 100.64.0.10 storageClasses: default: kops-ssd-1-17 nfs: enabled: false cinder: enabled: false local: enabled: false ebs: enabled: false objectStorage: type: \"s3\" s3: region: \"set-me\" # e.g. eu-north-1 regionEndpoint: \"set-me\" # e.g. https://s3.eu-north-1.amazonaws.com issuers: letsencrypt: prod: email: \"set-me\" # Set to a valid email address staging: email: \"set-me\" # Set to a valid email address # wc-config.yaml global: baseDomain: \"set-me\" # Set to ${WORKLOAD_CLUSTER} opsDomain: \"set-me\" # Set to ops.${SERVICE_CLUSTER} issuer: letsencrypt-prod verifyTls: true clusterDNS: 100.64.0.10 storageClasses: default: kops-ssd-1-17 nfs: enabled: false cinder: enabled: false local: enabled: false ebs: enabled: false objectStorage: type: \"s3\" s3: region: \"set-me\" # e.g. eu-north-1 regionEndpoint: \"set-me\" # e.g. https://s3.eu-north-1.amazonaws.com opa: enabled: false # Does not work with k8s 1.19+ # secrets.yaml objectStorage: s3: accessKey: \"set-me\" # Set to your AWS S3 accesskey secretKey: \"set-me\" # Set to your AWS S3 secretKey PSP and RBAC \u00b6 Since we've enabled the PodSecurityPolicy admission plugin in the kube-apiserver we'll need to create some basic PSPs and RBAC rules that both you and Compliant Kubernetes Apps will need to run workloads. for CLUSTER in ${SERVICE_CLUSTER} ${WORKLOAD_CLUSTER}; do kubectl config use-context ${CLUSTER} # Install 'restricted' and 'privileged' podSecurityPolicies. kubectl apply -f https://raw.githubusercontent.com/kubernetes/website/master/content/en/examples/policy/privileged-psp.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes/website/master/content/en/examples/policy/restricted-psp.yaml # Install RBAC so authenticated users are be able to use the 'restricted' psp. echo ' --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: addonmanager.kubernetes.io/mode: Reconcile name: psp:restricted rules: - apiGroups: - policy resourceNames: - restricted resources: - podsecuritypolicies verbs: - use --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: psp:any:restricted roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: psp:restricted subjects: - apiGroup: rbac.authorization.k8s.io kind: Group name: system:authenticated ' | kubectl apply -f - done Create placeholder DNS records \u00b6 To avoid negative caching and other surprises. Create the following records using your favorite tool or you can use the Import zone file feature in Route53: echo \" *.${SERVICE_CLUSTER} 60s A 203.0.113.123 *.${WORKLOAD_CLUSTER} 60s A 203.0.113.123 *.ops.${SERVICE_CLUSTER} 60s A 203.0.113.123 \" Create S3 buckets \u00b6 Depending on you configuration you may want to create S3 buckets. Create the following buckets using your favorite tool or via the AWS console: # List bucket names. { yq r ${CK8S_CONFIG_PATH}/wc-config.yaml 'objectStorage.buckets.*';\\ yq r ${CK8S_CONFIG_PATH}/sc-config.yaml 'objectStorage.buckets.*'; } | sort | uniq # Create buckets using the AWS CLI. # Assumes that the same bucket is used for velero in both service and workload cluster. for BUCKET in $(yq r ${CK8S_CONFIG_PATH}/sc-config.yaml 'objectStorage.buckets.*'); do aws s3api create-bucket\\ --bucket ${BUCKET} \\ --create-bucket-configuration LocationConstraint=${AWS_REGION} done Prepare kubeconfigs \u00b6 Compliant Kubernetes Apps demands that the kube contexts for the workload and service cluster are found in separate files encrypted with sops. kubectl config view --minify --flatten --context=${SERVICE_CLUSTER} > ${CK8S_CONFIG_PATH}/.state/kube_config_sc.yaml sops -e -i --config ${CK8S_CONFIG_PATH}/.sops.yaml ${CK8S_CONFIG_PATH}/.state/kube_config_sc.yaml kubectl config view --minify --flatten --context=${WORKLOAD_CLUSTER} > ${CK8S_CONFIG_PATH}/.state/kube_config_wc.yaml sops -e -i --config ${CK8S_CONFIG_PATH}/.sops.yaml ${CK8S_CONFIG_PATH}/.state/kube_config_wc.yaml Install apps \u00b6 You can install apps in parallel, although it is recommended to install the service cluster before the workload cluster. # Service cluster ./bin/ck8s apply sc # Respond \"n\" if you get a WARN # Workload cluster ./bin/ck8s apply wc # Respond \"n\" if you get a WARN Run the following to get metrics from etcd-manager # Service cluster ./bin/ck8s ops helmfile sc -l app=kube-prometheus-stack apply --skip-deps --set kubeEtcd.service.selector.k8s-app=etcd-manager-main --set kubeEtcd.service.targetPort=8081 # Workload cluster ./bin/ck8s ops helmfile wc -l app=kube-prometheus-stack apply --skip-deps --set kubeEtcd.service.selector.k8s-app=etcd-manager-main --set kubeEtcd.service.targetPort=8081 Update DNS records \u00b6 Now that we've installed all applications, the loadbalancer fronting the ingress controller should be ready. Run the following commands and update the A records in Route53. sc_lb=$(./bin/ck8s ops kubectl sc -n ingress-nginx get svc ingress-nginx-controller -ojsonpath={.status.loadBalancer.ingress[0].hostname}) wc_lb=$(./bin/ck8s ops kubectl wc -n ingress-nginx get svc ingress-nginx-controller -ojsonpath={.status.loadBalancer.ingress[0].hostname}) sc_lb_ip=$(dig +short ${sc_lb} | head -1) wc_lb_ip=$(dig +short ${wc_lb} | head -1) echo \" *.${SERVICE_CLUSTER} 60s A ${sc_lb_ip} *.${WORKLOAD_CLUSTER} 60s A ${wc_lb_ip} *.ops.${SERVICE_CLUSTER} 60s A ${sc_lb_ip} \" Teardown \u00b6 Compliant Kubernetes Apps \u00b6 This step is optional. If this is not run you'll have to check and manually remove any leftover cloud resources like S3 buckets, ELBs, and EBS volumes. git checkout 6f2e386 timeout 180 ./scripts/clean-wc.sh timeout 180 ./scripts/clean-sc.sh # Delete buckets for BUCKET in $(yq r ${CK8S_CONFIG_PATH}/sc-config.yaml 'objectStorage.buckets.*'); do aws s3 rb --force s3://${BUCKET} done # Delete config repo rm -rf ${CK8S_CONFIG_PATH} Remember to also remove the A records from Route53. Infrastructure and Kubernetes \u00b6 Enter eks-distro/development/kops and run: # Destroy clusters and local cluster configurations. for CLUSTER in ${SERVICE_CLUSTER} ${WORKLOAD_CLUSTER}; do export KOPS_CLUSTER_NAME=${CLUSTER} ./delete_cluster.sh rm -rf ${CLUSTER} done You'll have to manually remove the leftover kOps A records from Route53. # Get names of the A records to be removed. for CLUSTER in ${SERVICE_CLUSTER} ${WORKLOAD_CLUSTER}; do echo kops-controller.internal.${CLUSTER} done Finally, you'll also need to remove the ${KOPS_STATE_STORE} from S3 and the IAM user that you used for this guide.","title":"On EKS-D"},{"location":"operator-manual/exoscale/","text":"This page is out of date We are currently working on internal documentation to streamline Compliant Kubernetes onboarding for selected cloud providers. Until those documents are ready, and until we have capacity to make parts of that documentation public, this page is out-of-date. Nevertheless, parts of it are useful. Use at your own risk and don't expect things to work smoothly. Compliant Kubernetes Deployment on Exoscale \u00b6 This document contains instructions on how to setup a service cluster and a workload cluster in Exoscale. The following are the main tasks addressed in this document: Infrastructure setup for two clusters: one service and one workload cluster Deploying Compliant Kubernetes on top of the two clusters. Creating DNS Records Deploying Rook Storage Orchestration Service Deploying Compliant Kubernetes apps The instructions below are just samples, you need to update them according to your requirements. Besides, the exoscale cli is used to manage DNS. If you are using any other DNS service provider for managing your DNS you can skip it. Before starting, make sure you have all necessary tools . Note This guide is written for compliantkubernetes-apps v0.17.0 Setup \u00b6 Choose names for your service cluster and workload cluster, as well as a name for your environment: SERVICE_CLUSTER = \"testsc\" WORKLOAD_CLUSTERS =( \"testwc0\" ) CK8S_ENVIRONMENT_NAME = my-environment-name Infrastructure Setup using Terraform \u00b6 Before trying any of the steps, clone the Elastisys Compliant Kubernetes Kubespray repo as follows: git clone --recursive https://github.com/elastisys/compliantkubernetes-kubespray cd compliantkubernetes-kubespray/kubespray Expose Exoscale credentials to Terraform \u00b6 For authentication create the file ~/.cloudstack.ini and put your Exoscale credentials in it. The file should look like something like this: [cloudstack] key = <API key> secret = <API secret> Customize your infrastructure \u00b6 Create a configuration for the service and the workload clusters: for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do cp -r inventory/sample inventory/ $CLUSTER cp contrib/terraform/exoscale/default.tfvars inventory/ $CLUSTER / done Review and, if needed, adjust the files in inventory/$CLUSTER/default.tfvars , where $CLUSTER is the cluster name: Use different value for the prefix field in /default.tfvars for the two clusters. Failing to do so will result in a name conflict. Set a non-zero value for ceph_partition_size field, e.g., \"ceph_partition_size\": 50 , as it will be used by Rook storage service to provide local disk storage. To security harden your cluster, set ssh_whitelist and api_server_whitelist to the IP addresses from which you expect to operate the cluster. Make sure you configure your SSH keys in ssh_public_keys . Important The Linux Ubuntu 20.04 LTS 64-bit image on Exoscale is regularly upgraded, which might cause unexpected changes during terraform apply . Consider uploading your own dated Ubuntu image to reduce the risk of downtime. Initialize and Apply Terraform \u00b6 for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do pushd contrib/terraform/exoscale terraform init terraform apply \\ -var-file = ../../../inventory/ $CLUSTER /default.tfvars \\ -state = ../../../inventory/ $CLUSTER /tfstate- $CLUSTER .tfstate cp inventory.ini ../../../inventory/ $CLUSTER / popd done Important The Terraform state is stored in inventory/$CLUSTER/tfstate-$CLUSTER.tfstate , where $CLUSTER is the cluster name. It is precious. Consider backing it up or using Terraform Cloud . You should now have inventory file named inventory/$CLUSTER/inventory.ini for each cluster that you can use with kubespray. Test access to all nodes \u00b6 for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do pushd inventory/ $CLUSTER ANSIBLE_HOST_KEY_CHECKING = False ansible all -i inventory.ini -m ping popd done Deploying vanilla Kubernetes clusters using Kubespray \u00b6 With the infrastructure provisioned, we can now deploy Kubernetes using kubespray. First, if you haven't done so already, install the pre-requisites and change to the compliantkubernetes-kubespray root directory. pip3 install -r requirements.txt cd .. Init the Kubespray config in your config path \u00b6 export DOMAIN = <your_domain> # DNS domain to expose the services inside the service cluster i.e. \"example.com\" export CK8S_CONFIG_PATH = ~/.ck8s/exoscale export CK8S_PGP_FP = <your GPG key fingerprint> # retrieve with gpg --list-secret-keys for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ./bin/ck8s-kubespray init $CLUSTER default $CK8S_PGP_FP done Copy the generated inventory files in the right location \u00b6 Please copy the two inventory files, kubespray/inventory/$CLUSTER/inventory.ini , generated by Terraform to ${CK8S_CONFIG_PATH}/$CLUSTER-config/ , where $CLUSTER the name of each cluster (i.e., testsc , testwc0 ). for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do cp kubespray/inventory/$CLUSTER/inventory.ini ${CK8S_CONFIG_PATH}/$CLUSTER-config/ done Run kubespray to deploy the Kubernetes clusters \u00b6 for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ./bin/ck8s-kubespray apply $CLUSTER --flush-cache done This may take up to 10 minutes for each cluster, 20 minutes in total. Correct the Kubernetes API IP addresses \u00b6 Locate the encrypted kubeconfigs in ${CK8S_CONFIG_PATH}/.state/kube_config_*.yaml and edit them using sops. Copy the public IP address of the load balancer from inventory files ${CK8S_CONFIG_PATH}/*-config/inventory.ini and replace the private IP address for the server field in ${CK8S_CONFIG_PATH}/.state/kube_config_*.yaml . for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml done Test access to the clusters as follows \u00b6 for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get nodes' done Create the DNS Records \u00b6 You will need to setup a number of DNS entries for traffic to be routed correctly. Determine the public IP of the load-balancer fronting the Ingress controller of the clusters from the Terraform state file generated during infrastructure setup. To get the load-balancer IP, run the following command: SC_INGRESS_LB_IP_ADDRESS = $( terraform output -state kubespray/inventory/ $SERVICE_CLUSTER /tfstate- $SERVICE_CLUSTER .tfstate -raw ingress_controller_lb_ip_address ) echo $SC_INGRESS_LB_IP_ADDRESS Configure the exoscale CLI: exo config Then point these domains to the service cluster using 'exoscale cli' as follows: exo dns add A $DOMAIN -a $SC_INGRESS_LB_IP_ADDRESS -n *.ops. $CK8S_ENVIRONMENT_NAME exo dns add A $DOMAIN -a $SC_INGRESS_LB_IP_ADDRESS -n *. $CK8S_ENVIRONMENT_NAME Deploy Rook \u00b6 To deploy Rook, please go to the compliantkubernetes-kubespray repo root directory and run the following. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops --decrypt ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml > $CLUSTER .yaml export KUBECONFIG = $CLUSTER .yaml ./rook/deploy-rook.sh shred -zu $CLUSTER .yaml done Please restart the operator pod, rook-ceph-operator* , if some pods stalls in initialization state as shown below: rook-ceph rook-ceph-crashcollector-minion-0-b75b9fc64-tv2vg 0/1 Init:0/2 0 24m rook-ceph rook-ceph-crashcollector-minion-1-5cfb88b66f-mggrh 0/1 Init:0/2 0 36m rook-ceph rook-ceph-crashcollector-minion-2-5c74ffffb6-jwk55 0/1 Init:0/2 0 14m Important Pods in pending state usually indicate resource shortage. In such cases you need to use bigger instances. Test Rook \u00b6 To test Rook, proceed as follows: for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml 'kubectl --kubeconfig {} apply -f https://raw.githubusercontent.com/rook/rook/release-1.5/cluster/examples/kubernetes/ceph/csi/rbd/pvc.yaml' ; done for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml 'kubectl --kubeconfig {} get pvc' ; done You should see PVCs in Bound state. If you want to clean the previously created PVCs: for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml 'kubectl --kubeconfig {} delete pvc rbd-pvc' ; done Deploying Compliant Kubernetes Apps \u00b6 Now that the Kubernetes clusters are up and running, we are ready to install the Compliant Kubernetes apps. Clone compliantkubernetes-apps and Install Pre-requisites \u00b6 If you haven't done so already, clone the compliantkubernetes-apps repo and install pre-requisites. git clone https://github.com/elastisys/compliantkubernetes-apps.git cd compliantkubernetes-apps ansible-playbook -e 'ansible_python_interpreter=/usr/bin/python3' --ask-become-pass --connection local --inventory 127 .0.0.1, get-requirements.yaml Initialize the apps configuration \u00b6 export CK8S_ENVIRONMENT_NAME = my-environment-name #export CK8S_FLAVOR=[dev|prod] # defaults to dev export CK8S_CONFIG_PATH = ~/.ck8s/my-cluster-path export CK8S_CLOUD_PROVIDER = # [exoscale|safespring|citycloud|aws|baremetal] export CK8S_PGP_FP = <your GPG key fingerprint> # retrieve with gpg --list-secret-keys ./bin/ck8s init This will initialise the configuration in the ${CK8S_CONFIG_PATH} directory. Generating configuration files sc-config.yaml and wc-config.yaml , as well as secrets with randomly generated passwords in secrets.yaml . This will also generate read-only default configuration under the directory defaults/ which can be used as a guide for available and suggested options. ls -l $CK8S_CONFIG_PATH Configure the apps \u00b6 Edit the configuration files ${CK8S_CONFIG_PATH}/sc-config.yaml , ${CK8S_CONFIG_PATH}/wc-config.yaml and ${CK8S_CONFIG_PATH}/secrets.yaml and set the appropriate values for some of the configuration fields. Note that, the latter is encrypted. vim ${ CK8S_CONFIG_PATH } /sc-config.yaml vim ${ CK8S_CONFIG_PATH } /wc-config.yaml sops ${ CK8S_CONFIG_PATH } /secrets.yaml Tip The default configuration for the service cluster and workload cluster are available in the directory ${CK8S_CONFIG_PATH}/defaults/ and can be used as a reference for available options. Warning Do not modify the read-only default configurations files found in the directory ${CK8S_CONFIG_PATH}/defaults/ . Instead configure the cluster by modifying the regular files ${CK8S_CONFIG_PATH}/sc-config.yaml and ${CK8S_CONFIG_PATH}/wc-config.yaml as they will override the default options. The following are the minimum change you should perform: # ${CK8S_CONFIG_PATH}/sc-config.yaml and ${CK8S_CONFIG_PATH}/wc-config.yaml global : baseDomain : set-me # set to $CK8S_ENVIRONMENT_NAME.$DOMAIN opsDomain : set-me # set to ops.$CK8S_ENVIRONMENT_NAME.$DOMAIN issuer : letsencrypt-prod objectStorage : # type: s3 # set as default for prod flavor, defaults to \"none\" for dev s3 : region : set-me # Region for S3 buckets, e.g. ch-gva-2 regionEndpoint : set-me # Region endpoint for S3 buckets, e.g. https://sos-ch-gva-2.exo.io # forcePathStyle: false # set as default clusterAdmin : users : # set to the cluster admin users - set-me - admin@example.com # ${CK8S_CONFIG_PATH}/sc-config.yaml (in addition to the changes above) user : grafana : oidc : allowedDomains : # set to your domain(s), or unset using [] to deny all - set-me - example.com harbor : # persistence: # type: objectStorage # set as default for prod flavor, defaults to \"filesystem\" for dev # disableRedirect: false # set as default oidc : groupClaimName : set-me # set to group claim name used by OIDC provider adminGroupName : set-me # name of the group that automatically will get admin elasticsearch : extraRoleMappings : # set to configure elasticsearch access, or unset using [] - mapping_name : kibana_user definition : users : - set-me - mapping_name : kubernetes_log_reader definition : users : - set-me - mapping_name : all_access definition : users : - set-me alerts : opsGenieHeartbeat : # enabled: true # set as default for prod flavour, defaults to \"false\" for dev name : set-me # set to name the heartbeat if enabled issuers : letsencrypt : prod : email : set-me # set this to an email to receive LetsEncrypt notifications staging : email : set-me # set this to an email to receive LetsEncrypt notifications # ${CK8S_CONFIG_PATH}/wc-config.yaml (in addition to the changes above) user : namespaces : # set this to create user namespaces, or unset using [] - set-me - production - staging adminUsers : # set this to create admins in the user namespaces, or unset using [] - set-me - admin@example.com adminGroups : # set this to create admin groups in the user namespaces, or unset using [] - set-me # alertmanager: # add this block to enable user accessible alertmanager # enabled: true # namespace: alertmanager # note that this namespace must be listed above under \"user.namespaces\" opa : imageRegistry : URL : # set this to the allowed image registry, or unset using [] to deny all - set-me - harbor.example.com # ${CK8S_CONFIG_PATH}/secrets.yaml objectStorage : s3 : accessKey : set-me # set to your s3 accesskey secretKey : set-me # set to your s3 secretKey Create S3 buckets \u00b6 You can use the following script to create required S3 buckets. The script uses s3cmd in the background and gets configuration and credentials for your S3 provider from ${HOME}/.s3cfg file. # Use your default s3cmd config file: ${HOME}/.s3cfg scripts/S3/entry.sh create Important You should not use your own credentials for S3. Rather create a new set of credentials with write-only access, when supported by the object storage provider ( check a feature matrix ). Test S3 \u00b6 To ensure that you have configured S3 correctly, run the following snippet: ( access_key = $( sops exec-file ${ CK8S_CONFIG_PATH } /secrets.yaml 'yq r {} \"objectStorage.s3.accessKey\"' ) secret_key = $( sops exec-file ${ CK8S_CONFIG_PATH } /secrets.yaml 'yq r {} \"objectStorage.s3.secretKey\"' ) sc_config = $( yq m ${ CK8S_CONFIG_PATH } /defaults/sc-config.yaml ${ CK8S_CONFIG_PATH } /sc-config.yaml -a overwrite -x ) region = $( echo ${ sc_config } | yq r - 'objectStorage.s3.region' ) host = $( echo ${ sc_config } | yq r - 'objectStorage.s3.regionEndpoint' ) for bucket in $( echo ${ sc_config } | yq r - 'objectStorage.buckets.*' ) ; do s3cmd --access_key = ${ access_key } --secret_key = ${ secret_key } \\ --region = ${ region } --host = ${ host } \\ ls s3:// ${ bucket } > /dev/null [ ${ ? } = 0 ] && echo \"Bucket ${ bucket } exists!\" done ) Install Compliant Kubernetes apps \u00b6 Start with the service cluster: ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ SERVICE_CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_sc.yaml ./bin/ck8s apply sc # Respond \"n\" if you get a WARN Then the workload clusters: for CLUSTER in \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_wc.yaml ./bin/ck8s apply wc # Respond \"n\" if you get a WARN done Settling \u00b6 Important Leave sufficient time for the system to settle, e.g., request TLS certificates from LetsEncrypt, perhaps as much as 20 minutes. You can check if the system settled as follows: for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get --all-namespaces pods' done Check the output of the command above. All Pods needs to be Running or Completed. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get --all-namespaces issuers,clusterissuers,certificates' done Check the output of the command above. All resources need to have the Ready column True. Testing \u00b6 After completing the installation step you can test if the apps are properly installed and ready using the commands below. Start with the service cluster: ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ SERVICE_CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_sc.yaml ./bin/ck8s test sc # Respond \"n\" if you get a WARN Then the workload clusters: for CLUSTER in \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_wc.yaml ./bin/ck8s test wc # Respond \"n\" if you get a WARN done Done. Navigate to the endpoints, for example grafana.$BASE_DOMAIN , kibana.$BASE_DOMAIN , harbor.$BASE_DOMAIN , etc. to discover Compliant Kubernetes's features. Teardown \u00b6 Removing Compliant Kubernetes Apps from your cluster \u00b6 To remove the applications added by compliant kubernetes you can use the two scripts clean-sc.sh and clean-wc.sh , they are located here in the scripts folder . They perform the following actions: Delete the added helm charts Delete the added namespaces Delete any remaining PersistentVolumes Delete the added CustomResourceDefinitions Note: if user namespaces are managed by Compliant Kubernetes apps then they will also be deleted if you clean up the workload cluster. Remove infrastructure \u00b6 To teardown the infrastructure, please switch to the root directory of the exoscale branch of the Kubespray repo (see the Terraform section). for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do pushd contrib/terraform/exoscale terraform init terraform destroy \\ -var-file = ../../../inventory/ $CLUSTER /default.tfvars \\ -state = ../../../inventory/ $CLUSTER /tfstate- $CLUSTER .tfstate popd done # Remove DNS records exo dns remove $DOMAIN *.ops. $CK8S_ENVIRONMENT_NAME exo dns remove $DOMAIN *. $CK8S_ENVIRONMENT_NAME Further Reading \u00b6 Elastisys Compliant Kubernetes Kubespray Kubernetes on Exoscale with Terraform Compliant Kubernetes apps repo Configurations option","title":"On Exoscale"},{"location":"operator-manual/faq/","text":"Kubernetes Administrator FAQ \u00b6 I updated some OpenSearch options but it didn't work, now what? \u00b6 If you update the OpenSearch securityConfig you will have to make sure that the master Pod(s) are restarted so that they pick up the new Secret and then run the securityadmin.sh script. This happens for example if you switch from non-SSO to SSO. To reload the configuration you need to run the following commands: # Make the script executable kubectl -n opensearch-system exec opensearch-master-0 -- chmod +x ./plugins/opensearch-security/tools/securityadmin.sh # Run the script to update the configuration kubectl -n opensearch-system exec opensearch-master-0 -- ./plugins/opensearch-security/tools/securityadmin.sh \\ -f plugins/opensearch-security/securityconfig/config.yml \\ -icl -nhnv \\ -cacert config/admin/ca.crt \\ -cert config/admin/tls.crt \\ -key config/admin/tls.key Note that the above only reloads the config.yml (as specified with the -f ). If you made changes to other parts of the system you will need to point to the relevant file to reload, or reload everything like this: # Run the script to update \"everything\" (internal users, roles, configuration, etc.) kubectl -n opensearch-system exec opensearch-master-0 -- ./plugins/opensearch-security/tools/securityadmin.sh \\ -cd plugins/opensearch-security/securityconfig/ \\ -icl -nhnv \\ -cacert config/admin/ca.crt \\ -cert config/admin/tls.crt \\ -key config/admin/tls.key When you update things other than config.yml you will also need to rerun the Configurer Job by syncing the opensearch-configurer chart.","title":"FAQ"},{"location":"operator-manual/gcp/","text":"This page is out of date We are currently working on internal documentation to streamline Compliant Kubernetes onboarding for selected cloud providers. Until those documents are ready, and until we have capacity to make parts of that documentation public, this page is out-of-date. Nevertheless, parts of it are useful. Use at your own risk and don't expect things to work smoothly. Compliant Kubernetes Deployment on GCP \u00b6 This document contains instructions on how to set up a Compliant Kubernetes environment (consisting of a service cluster and one or more workload clusters) on GCP. The document is split into two parts: Cluster setup (Setting up infrastructure and the Kubernetes clusters) Apps setup (including information about limitations) Before starting, make sure you have all necessary tools . In addition to these general tools, you will also need: A GCP project A JSON keyfile for running Terraform . SSH key that you will use to access GCP, which you have added to the metadata in your GCP project . (Optional) Another JSON keyfile for the GCP Persistent Disk CSI Driver . It is possible (but not recommended) to reuse the same JSON keyfile as you use for Terraform. Note This guide is written for compliantkubernetes-apps v0.13.0 Initial setup \u00b6 Choose names for your service cluster and workload cluster(s): SERVICE_CLUSTER = \"testsc\" WORKLOAD_CLUSTERS =( \"testwc0\" ) Cluster setup \u00b6 Clone the compliantkubernetes-kubespray repository: git clone --recursive https://github.com/elastisys/compliantkubernetes-kubespray cd compliantkubernetes-kubespray For all commands in the cluster setup part of this guide, your working directory is assumed to be the root directory of this repository. In config/gcp/group_vars/all/ck8s-gcp.yml , set the value of gcp_pd_csi_sa_cred_file to the path of your JSON keyfile for GCP Persistent Disk CSI Driver. Modify kubespray/contrib/terraform/gcp/tfvars.json in the following way: Set gcp_project_id to the ID of your GCP project. Set keyfile_location to the location of your JSON keyfile. This will be used as credentials for accessing the GCP API when running Terraform. Set ssh_pub_key to the path of your public SSH key. In ssh_whitelist , api_server_whitelist and nodeport_whitelist , add IP address(es) that you want to be able to access the cluster. Set up the nodes by performing the following steps: Make copies of the Terraform variables, one for each cluster: pushd kubespray/contrib/terraform/gcp for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do cp tfvars.json $CLUSTER -tfvars.json done popd Set up the nodes with Terraform. If desired, first modify \"machines\" in kubespray/contrib/terraform/gcp/$CLUSTER-tfvars.json to add/remove nodes, change node sizes, etc. (For setting up compliantkubernetes-apps in the service cluster, one n1-standard-8 worker and one n1-standard-4 worker is enough.) pushd kubespray/contrib/terraform/gcp for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do terraform init terraform apply -var-file $CLUSTER -tfvars.json -auto-approve -state $CLUSTER .tfstate -var prefix = $CLUSTER done popd Save the outputs from apply . (Alternatively, get them later by running terraform output -state $CLUSTER.tfstate in the folder with the state file) Generate inventory file: pushd kubespray/contrib/terraform/gcp for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do ./generate-inventory.sh $CLUSTER .tfstate > $CLUSTER -inventory.ini done popd Initialize the config: export CK8S_CONFIG_PATH = ~/.ck8s/<environment-name> ./bin/ck8s-kubespray init $CLUSTER gcp <path to SSH key> [ <SOPS fingerprint> ] path to SSH key should point to your private SSH key. It will be copied into your config path and encrypted with SOPS, the original file left as it were. SOPS fingerprint is the gpg fingerprint that will be used for SOPS encryption. You need to set this or the environment variable CK8S_PGP_FP the first time SOPS is used in your specified config path. Copy the inventory files: pushd kubespray/contrib/terraform/gcp for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do mv $CLUSTER -inventory.ini $CK8S_CONFIG_PATH / $CLUSTER -config/inventory.ini done popd Run kubespray to set up the kubernetes cluster: ./bin/ck8s-kubespray apply $CLUSTER In your config path, open .state/kube_config_$CLUSTER.yaml with SOPS and change clusters.cluster.server to the control_plane_lb_ip_address you got from terraform apply . Apps setup \u00b6 The following instructions were made for release v0.13.0 of compliantkubernetes-apps. There may be discrepancies with newer versions. Limitations \u00b6 Note that there are a few limitations when using compliantkubernetes-apps on GCP at the moment, due to lack of support for certain features: Backup retention for InfluxDB is disabled due to it only being supported with S3 as object storage. If you want to circumvent this, consider using S3 as object storage or deploying Minio as a gateway. Fluentd does not work due to a missing output plugin. If you want to circumvent this, consider using S3 as object storage or deploying Minio as a gateway. Alternatively, Fluentd can be disabled in the compliantkubernetes-apps configuration, which has the consequence of no logs being saved from the service cluster. For information on how to modify the configuration to use S3 as object storage, refer to the administrator manual for AWS or Exoscale , in the section for apps configuration. Setup \u00b6 Set up your DNS entries on a provider of your choice, using the ingress_controller_lb_ip_address from terraform apply as your loadbalancer IPs. You need the following entries: *.ops.<environment_name>. $DOMAIN A <service_cluster_lb_ip> grafana.<environment_name>. $DOMAIN A <service_cluster_lb_ip> harbor.<environment_name>. $DOMAIN A <service_cluster_lb_ip> kibana.<environment_name>. $DOMAIN A <service_cluster_lb_ip> dex.<environment_name>. $DOMAIN A <service_cluster_lb_ip> notary.harbor.<environment_name>. $DOMAIN A <service_cluster_lb_ip> Optionally, if alertmanager is enabled in the workload cluster, create the following DNS record: *.<environment_name>. $DOMAIN A <workload_cluster_lb_ip> In compliantkubernetes-apps , run: export CK8S_ENVIRONMENT_NAME = <environment-name> export CK8S_CLOUD_PROVIDER = baremetal ./bin/ck8s init You will need to modify secrets.yaml , sc-config.yaml and wc-config.yaml in your config path. secrets.yaml Uncomment objectStorage.gcs.keyfileData and paste the contents of your JSON keyfile as the value. sc-config.yaml AND wc-config.yaml Set global.baseDomain to <environment-name>.<dns-domain> and global.opsDomain to ops.<environment-name>.<dns-domain> . Set global.issuer to letsencrypt-prod . Set storageClasses.default to csi-gce-pd . Also set all storageClasses.*.enabled to false . Set objectStorage.type to \"gcs\" . Uncomment objectStorage.gcs.project and set it to the name of your GCP project. sc-config.yaml Set influxDB.backupRetention.enabled to false . Set ingressNginx.controller.service.type to this-is-not-used Set ingressNginx.controller.service.annotations to this-is-not-used Set harbor.oidc.groupClaimName to set-me Set issuers.letsencrypt.prod.email and issuers.letsencrypt.staging.email to email addresses of choice. Create buckets for storage on GCP (found under \"Storage\"). The names must match the bucket names found in your sc-config.yaml and wc-config.yaml in the config path. Set the default storageclass by running the following command: bin/ck8s ops kubectl sc \"patch storageclass csi-gce-pd -p '{\\\"metadata\\\": {\\\"annotations\\\":{\\\"storageclass.kubernetes.io/is-default-class\\\":\\\"true\\\"}}}'\" bin/ck8s ops kubectl wc \"patch storageclass csi-gce-pd -p '{\\\"metadata\\\": {\\\"annotations\\\":{\\\"storageclass.kubernetes.io/is-default-class\\\":\\\"true\\\"}}}'\" Apply the apps: bin/ck8s apply sc bin/ck8s apply wc Done. You should now have a functioning Compliant Kubernetes environment.","title":"On GCP"},{"location":"operator-manual/getting-started/","text":"Getting Started \u00b6 Setting up Compliant Kubernetes consists of two parts: setting up at least two vanilla Kubernetes clusters and deploying compliantkubernetes-apps on top of them. Pre-requisites for Creating Vanilla Kubernetes clusters \u00b6 In theory, any vanilla Kubernetes cluster can be used for Compliant Kubernetes. We suggest the kubespray way. To this end, you need: Git Python3 pip Terraform Ansible pwgen Ansible is best installed as follows: git clone --recursive https://github.com/elastisys/compliantkubernetes-kubespray cd compliantkubernetes-kubespray pip3 install -r kubespray/requirements.txt Optional: For debugging, you may want CLI tools to interact with your chosen cloud provider: AWS CLI Exoscale CLI OpenStack Client VMware vSphere CLI (govmomi) Pre-requisites for compliantkubernetes-apps \u00b6 Using Ansible, these can be retrieved as follows: git clone https://github.com/elastisys/compliantkubernetes-apps cd compliantkubernetes-apps ansible-playbook -e 'ansible_python_interpreter=/usr/bin/python3' --ask-become-pass --connection local --inventory 127 .0.0.1, get-requirements.yaml Misc \u00b6 Compliant Kubernetes relies on SSH for accessing nodes. If you haven't already done so, generate an SSH key as follows: ssh-keygen Configuration secrets in Compliant Kubernetes are encrypted using SOPS . We currently only support using PGP when encrypting secrets. If you haven't already done so, generate your own PGP key as follows: gpg --full-generate-key","title":"Getting Started"},{"location":"operator-manual/ingress/","text":"Ingress \u00b6 Compliant Kubernetes (CK8s) uses the Nginx Ingress controller to route external traffic to the correct Service inside the cluster. CK8s can configure the Ingress controller in two different ways depending on the underlying infrastructure. Using a Service of type LoadBalancer \u00b6 When using a cloud provider with a Kubernetes cloud integration such as AWS, Azure and Google cloud the Ingress controller can be exposed with a Service of type LoadBalancer. This will create an external load balancer in the cloud provider with an external ip-address. Any dns records should be pointed to the ip-address of the load balancer. Note This is only currently supported in CK8s for AWS. It is however possible to configure this for Azure and Google cloud as well but it's not done by default Using the host network \u00b6 For any cloud provider (or bare metal) not supporting these kind of public load balancers the Ingress controller uses the host network instead. This is done by configuring the Ingress controller as a DaemonSet so one Pod is created on each node. The Pods are configured to use the host network, so all traffic received on the node on port 80 and 443 will be intercepted by the Ingress controller Pod and then routed to the desired Service. On some clouds providers there is load balancing available for the worker nodes. For example Exoscale uses an \"elastic ip\" which provides one external ip which load balances to the available worker nodes. For these cloud providers this external ip of the load balancers should be used as the entry point in the dns. For the cloud providers where this is not available the easiest option is to just point the dns to the ip of any, or all, of the worker nodes. This is of course not a optimal solution because it adds a single point of failure on the worker node which is selected by the dns. Another option is to use any existing load balancer service if this is available. Installation \u00b6 The Nginx ingress is currently configured and installed by the compliantkubernetes-apps repository. The configuration is set in sc-config.yaml and wc-config.yaml under: ingressNginx : useHostPort : \"\" service : enabled : \"\" type : \"\" If the apps repository is initiated with the correct cloud provider these config options will get the correct defaults. For more ways to install the Nginx Ingress controller see https://kubernetes.github.io/ingress-nginx/deploy Ingress resource \u00b6 The Ingress resource is used to later route traffic to the desired Service. For more information about this see the official documentation .","title":"Ingress"},{"location":"operator-manual/maintenance/","tags":["ISO 27001 A.12.6.1","BSI IT-Grundschutz APP.4.4.A21"],"text":"Maintaining and Upgrading your Compliant Kubernetes environment. \u00b6 In order to keep your Compliant Kubernetes environment running smoothly, and to assure that you are up to date with the latest patches you need to perform regular maintenance on it. This guide assumes that: Your Compliant Kubernetes environment is running normally, if not, please see the Troubleshooting guide . Your Compliant Kubernetes environment is properly sized You have performed the actions in the Go-live Checklist as failure to do so might cause downtime during maintenance. Compliance needs \u00b6 Many regulations require you to secure your information system against unauthorized access, data loss, and breaches. An important part of this is keeping your Compliant Kubernetes environment up to date with the latest security patches not run outdated versions of components that are no longer supported. This maps to objectives in ISO Annex A.12.6.1 Management of Technical Vulnerabilities . What maintenance do I need to do and how? \u00b6 In short, there are three levels of maintenane that should be performed on a regular basis. Patching the underlying OS on the nodes Upgrading the Compliant Kubernetes application stack Upgrading Compliant Kubernetes-Kubespray Let's go through them one by one. Patching the nodes \u00b6 Security patches for the underlying OS on the nodes is constantly being released, and to ensure your environment is secured, the nodes that run Compliant Kubernetes must be updated with these patches. We recommend that you use the AutomaticSecurityUpdates feature that is available in Ubuntu (similar feature exist in other distros) to install these updates. Note that the nodes still need to be rebooted for some of these updates to be applied. In order to reboot the nodes, you can either use a tool like kured or you can do it manually by logging on to the nodes and rebooting them manually. When doing that, reboot one node at the time and make sure that the rebooted node is 'Ready' and that pods are scheduled to it before you move on to the next, or you risk downtime. There is a playbook in the compliantkubernetes-kubespray repo that can assist with the reboot of nodes. It will cordon and reboot the nodes one by one. ./bin/ck8s-kubespray reboot-nodes <prefix> [ --extra-vars manual_prompt = true ] [ <options> ] Upgrading the Compliant Kubernetes application stack \u00b6 Compliant Kuberenets consists of a multitude of open source components that interact to form a smooth end user experience. In order to free you of the burden of keeping track of when to upgrade the various components, new versions of Complaint Kubernetes are regularly release. When a new version is released, it becomes available as a tagged release in the github repo. Before upgrading to a new release, please review the changelog if possible, apply the upgrade to a staging environment before upgrading any environments with production data. Upgrading compliantkubernetes-apps \u00b6 For security, compliance, and support reasons, environments should stay up to date with the latest version of compliankubernetes-apps . Note what version of compliantkubernetes-apps that is currently used and the version that you want to upgrade to. Then check the release notes for each version in between to see if there are anything that might cause any problems, if so then consult the rest of the operations team before proceeding. You should never upgrade more than one minor version of compliantkubernetes-apps at a time. Checkout the next compliantkubernetes-apps version # you should be in the root folder of compliantkubernetes-apps git checkout <next-version> Check if there is a migration document for the relesae you want to upgrade to, (e.g. for upgrade to 0.11.0 ) and follow the instructions there. Note that you should check the documentation at the release tag instead of main to be sure that it's correct. If there is no relevant migration document, first do a dry-run. ./bin/ck8s dry-run sc ./bin/ck8s dry-run wc If dry-run reports no errors, proceed with the upgrade. ./bin/ck8s apply sc ./bin/ck8s apply wc Verify that everything is running after the upgrade. At the minimum, at least run the tests in compliantkubernetes-apps. ./bin/ck8s test sc ./bin/ck8s test wc Go back to step 1 and repeat one new release of compliantkubernetes-apps at a time until you are at the latest release. Upgrading Kubespray/Kubernetes \u00b6 All clusters should stay up to date with the latest Kubespray version used in compliantkubernetes-kubespray . Note what version of Kubespray that is currently used in the cluster and the Kubespray version we want to upgrade to. Then check the release notes for each version in between to see if there are anything that might cause any problems, if so then consult the rest of the operations team before proceeding. Also check if the newer Kubespray version would upgrade Kubernetes to a new minor version, if so then the customer should get a notice of x weeks before proceeding to let them check for any deprecated APIs that they might be using. You should never upgrade more than one patch version of Kubespray at a time. E.g. if you are at Kubespray version 2.13.3 and are going to 2.15.0 then the upgrade path would be 2.13.3 -> 2.13.4 -> 2.14.0 -> 2.14.1 -> 2.14.2 -> 2.15.0. Patches that are released to an older minor version can be skipped, e.g. new patches to 2.14 after 2.15 has been released. Read more about Kubespray upgrades in their documentation . Checkout the next Kubespray version by checking out the last compliantkubernetes-kubespray commit (the commit is next-version in the snippet below) that used that version and updating the submodule. # you should be in the root folder of compliantkubernetes-kubespray git checkout <next-version> git submodule update Upgrade compliantkubernetes-kubespray by following the relevant documentation (e.g. for upgrade to v2.17.x-ck8s1 ). After doing any upgrades or maintenance \u00b6 It is a good idea to perform a log review after each maintenance. The purpose is two-fold: To catch potential issues caused by the maintenance. To piggy-back log review on top of a known-good process. For instructions on how to do this, please follow the log review guide .","title":"Maintenance"},{"location":"operator-manual/openstack/","text":"This page is out of date We are currently working on internal documentation to streamline Compliant Kubernetes onboarding for selected cloud providers. Until those documents are ready, and until we have capacity to make parts of that documentation public, this page is out-of-date. Nevertheless, parts of it are useful. Use at your own risk and don't expect things to work smoothly. Compliant Kubernetes on Openstack \u00b6 This document contains instructions on how to set up a Compliant Kubernetes environment (consisting of a service cluster and one or more workload clusters) on Openstack. Infrastructure setup for two clusters: one service and one workload cluster Deploying Compliant Kubernetes on top of the two clusters. Creating DNS Records Deploying Compliant Kubernetes apps Before starting, make sure you have all necessary tools . In addition to these general tools, you will also need: - Openstack credentials (either using openrc or the clouds.yaml configuration file) for setting up the infrastructure. Note Although recommended OpenStack authentication method is clouds.yaml , it is more convenient to use the openrc method with Compliant Kubernetes as it works both with Kubespray and Terraform. If you are using the clouds.yaml method, at the moment, Kubespray will still expect you to set a few environment variables. Note This guide is written for compliantkubernetes-apps v0.17.0 Setup \u00b6 Choose names for your service cluster and workload cluster(s): SERVICE_CLUSTER = \"sc\" WORKLOAD_CLUSTERS =( \"wc0\" \"wc1\" ) export CK8S_CONFIG_PATH = ~/.ck8s/<environment-name> export SOPS_FP = <PGP-fingerprint> # retrieve with gpg --list-secret-keys Infrastructure setup using Terraform \u00b6 Before trying any of the steps, clone the Elastisys Compliant Kubernetes Kubespray repo as follows: git clone --recursive https://github.com/elastisys/compliantkubernetes-kubespray Expose Openstack credentials to Terraform \u00b6 Terraform will need access to Openstack credentials in order to create the infrastructure. More details can be found here . We will be using the declarative option with the open.rc file. Expose Openstack credentials to Terraform For authentication create or download, from your provider, the file openstack-rc and source path/to/your/openstack-rc . The file should contain the following variables: export OS_USERNAME = export OS_PASSWORD = export OS_AUTH_URL = export OS_USER_DOMAIN_NAME = export OS_PROJECT_DOMAIN_NAME = export OS_REGION_NAME = export OS_PROJECT_NAME = export OS_TENANT_NAME = export OS_AUTH_VERSION = export OS_IDENTITY_API_VERSION = export OS_PROJECT_ID = Customize your infrastructure \u00b6 Start by initializing a Compliant Kubernetes environment using Compliant Kubernetes Kubespray. All of this is done from the root of the compliantkubernetes-kubespray repository. for CLUSTER in \" ${ SERVICE_CLUSTER } \" \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ./bin/ck8s-kubespray init \" ${ CLUSTER } \" openstack \" ${ SOPS_FP } \" done Configure Terraform by creating a cluster.tfvars file for each cluster. The available options can be seen in kubespray/contrib/terraform/openstack/variables.tf . There is a sample file that can be copied to get something to start from. for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do cp kubespray/contrib/terraform/openstack/sample-inventory/cluster.tfvars \" ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/cluster.tfvars\" done Note You really must edit the values in these files. There is no way to set sane defaults for what flavor to use, what availability zones or networks are available across providers. In the section below some guidance and samples are provided but remember that they might be useless to you depending on your needs and setup. Infrastructure guidance \u00b6 The minimum infrastructure sizing requirements are at least three worker nodes with 4 cores and 8 GB memory each, and we recommend you to have at least 2 cores and 4 GB for your control plane nodes. Note A recommended production infrastructure sizing is available in the architecture diagram . Below is example cluster.tfvars for a few select openstack providers. The examples are copy-pastable, but you might want to change cluster_name and network_name (if neutron is used!). Citycloud Fra1 # your Kubernetes cluster name here cluster_name = \"your-cluster-name\" # list of availability zones available in your OpenStack cluster #az_list = [\"nova\"] # SSH key to use for access to nodes public_key_path = \"~/.ssh/id_rsa.pub\" # image to use for bastion, masters, standalone etcd instances, and nodes image = \"Ubuntu 20.04 Focal Fossa 20200423\" # user on the node (ex. core on Container Linux, ubuntu on Ubuntu, etc.) ssh_user = \"ubuntu\" # 0|1 bastion nodes number_of_bastions = 0 # standalone etcds number_of_etcd = 0 # masters number_of_k8s_masters = 1 number_of_k8s_masters_no_etcd = 0 number_of_k8s_masters_no_floating_ip = 0 number_of_k8s_masters_no_floating_ip_no_etcd = 0 # Flavor depends on your openstack installation # you can get available flavor IDs through `openstack flavor list` flavor_k8s_master = \"89afeed0-9e41-4091-af73-727298a5d959\"\" # nodes number_of_k8s_nodes = 3 number_of_k8s_nodes_no_floating_ip = 0 # Flavor depends on your openstack installation # you can get available flavor IDs through `openstack flavor list` flavor_k8s_node = \"ecd976c3-c71c-4096-b138-e4d964c0b27f\" # networking # ssh access to nodes k8s_allowed_remote_ips = [\"0.0.0.0/0\"] # List of CIDR blocks allowed to initiate an API connection master_allowed_remote_ips = [\"0.0.0.0/0\"] worker_allowed_ports = [ { # Node ports \"protocol\" = \"tcp\" \"port_range_min\" = 30000 \"port_range_max\" = 32767 \"remote_ip_prefix\" = \"0.0.0.0/0\" }, { # HTTP \"protocol\" = \"tcp\" \"port_range_min\" = 80 \"port_range_max\" = 80 \"remote_ip_prefix\" = \"0.0.0.0/0\" }, { # HTTPS \"protocol\" = \"tcp\" \"port_range_min\" = 443 \"port_range_max\" = 443 \"remote_ip_prefix\" = \"0.0.0.0/0\" } ] # use `openstack network list` to list the available external networks network_name = \"name-of-your-network\" # UUID of the external network that will be routed to external_net = \"your-external-network-uuid\" floatingip_pool = \"ext-net\" # If 1, nodes with floating IPs will transmit internal cluster traffic via floating IPs; if 0 private IPs will be used instead. Default value is 1. use_access_ip = 0 # Create and use openstack nova servergroups, default: false use_server_groups = true subnet_cidr = \"172.16.0.0/24\" Citycloud Kna1 # your Kubernetes cluster name here cluster_name = \"your-cluster-name\" # list of availability zones available in your OpenStack cluster #az_list = [\"nova\"] # SSH key to use for access to nodes public_key_path = \"~/.ssh/id_rsa.pub\" # image to use for bastion, masters, standalone etcd instances, and nodes image = \"Ubuntu 20.04 Focal Fossa 20200423\" # user on the node (ex. core on Container Linux, ubuntu on Ubuntu, etc.) ssh_user = \"ubuntu\" # 0|1 bastion nodes number_of_bastions = 0 # standalone etcds number_of_etcd = 0 # masters number_of_k8s_masters = 1 number_of_k8s_masters_no_etcd = 0 number_of_k8s_masters_no_floating_ip = 0 number_of_k8s_masters_no_floating_ip_no_etcd = 0 # Flavor depends on your openstack installation # you can get available flavor IDs through `openstack flavor list` flavor_k8s_master = \"96c7903e-32f0-421d-b6a2-a45c97b15665\" # nodes number_of_k8s_nodes = 3 number_of_k8s_nodes_no_floating_ip = 0 # Flavor depends on your openstack installation # you can get available flavor IDs through `openstack flavor list` flavor_k8s_node = \"572a3b2e-6329-4053-b872-aecb1e70d8a6\" # networking # ssh access to nodes k8s_allowed_remote_ips = [\"0.0.0.0/0\"] # List of CIDR blocks allowed to initiate an API connection master_allowed_remote_ips = [\"0.0.0.0/0\"] worker_allowed_ports = [ { # Node ports \"protocol\" = \"tcp\" \"port_range_min\" = 30000 \"port_range_max\" = 32767 \"remote_ip_prefix\" = \"0.0.0.0/0\" }, { # HTTP \"protocol\" = \"tcp\" \"port_range_min\" = 80 \"port_range_max\" = 80 \"remote_ip_prefix\" = \"0.0.0.0/0\" }, { # HTTPS \"protocol\" = \"tcp\" \"port_range_min\" = 443 \"port_range_max\" = 443 \"remote_ip_prefix\" = \"0.0.0.0/0\" } ] # use `openstack network list` to list the available external networks network_name = \"name-of-your-network\" # UUID of the external network that will be routed to external_net = \"your-external-network-uuid\" floatingip_pool = \"ext-net\" # If 1, nodes with floating IPs will transmit internal cluster traffic via floating IPs; if 0 private IPs will be used instead. Default value is 1. use_access_ip = 0 # Create and use openstack nova servergroups, default: false use_server_groups = true subnet_cidr = \"172.16.0.0/24\" Safespring sto1 # your Kubernetes cluster name here cluster_name = \"your-cluster-name\" # SSH key to use for access to nodes public_key_path = \"~/.ssh/id_rsa.pub\" # image to use for bastion, masters, standalone etcd instances, and nodes image = \"ubuntu-20.04\" # user on the node (ex. core on Container Linux, ubuntu on Ubuntu, etc.) ssh_user = \"ubuntu\" # 0|1 bastion nodes number_of_bastions = 0 use_neutron = 0 # standalone etcds number_of_etcd = 0 # masters number_of_k8s_masters = 0 number_of_k8s_masters_no_etcd = 0 number_of_k8s_masters_no_floating_ip = 1 number_of_k8s_masters_no_floating_ip_no_etcd = 0 # Flavor depends on your openstack installation # you can get available flavor IDs through `openstack flavor list` flavor_k8s_master = \"8a707999-0bce-4f2f-8243-b4253ba7c473\" # nodes number_of_k8s_nodes = 0 number_of_k8s_nodes_no_floating_ip = 3 # Flavor depends on your openstack installation # you can get available flavor IDs through `openstack flavor list` flavor_k8s_node = \"5b40af67-9d11-45ed-a44f-e876766160a5\" # networking # ssh access to nodes k8s_allowed_remote_ips = [\"0.0.0.0/0\"] # List of CIDR blocks allowed to initiate an API connection master_allowed_remote_ips = [\"0.0.0.0/0\"] worker_allowed_ports = [ { # Node ports \"protocol\" = \"tcp\" \"port_range_min\" = 30000 \"port_range_max\" = 32767 \"remote_ip_prefix\" = \"0.0.0.0/0\" }, { # HTTP \"protocol\" = \"tcp\" \"port_range_min\" = 80 \"port_range_max\" = 80 \"remote_ip_prefix\" = \"0.0.0.0/0\" }, { # HTTPS \"protocol\" = \"tcp\" \"port_range_min\" = 443 \"port_range_max\" = 443 \"remote_ip_prefix\" = \"0.0.0.0/0\" } ] # use `openstack network list` to list the available external networks network_name = \"public\" # UUID of the external network that will be routed to external_net = \"your-external-network-uuid\" # If 1, nodes with floating IPs will transmit internal cluster traffic via floating IPs; if 0 private IPs will be used instead. Default value is 1. use_access_ip = 1 # Create and use openstack nova servergroups, default: false use_server_groups = true subnet_cidr = \"172.16.0.0/24\" Initialize and apply Terraform \u00b6 MODULE_PATH = \" $( pwd ) /kubespray/contrib/terraform/openstack\" for CLUSTER in \" ${ SERVICE_CLUSTER } \" \" ${ WORKLOAD_CLUSTERS [@] } \" ; do pushd \" ${ MODULE_PATH } \" terraform init terraform apply -var-file = \" ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/cluster.tfvars\" -state = \" ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/terraform.tfstate\" popd done Warning The above will not work well if you are using a bastion host. This is due to some hard coded paths . This is fixed in kubespray release-2.17 . If you are using an older version of kubespray, you may link the kubespray/contrib folder to the correct relative path, or make sure your CK8S_CONFIG_PATH is already at a proper place relative to the same. Deploying Compliant Kubernetes using Kubespray \u00b6 Before we can run Kubespray, we will need to go through the relevant variables. Additionally we will need to expose some credentials so that Kubespray can set up cloud provider integration. You will need to change at least one value: kube_oidc_url in ${CK8S_CONFIG_PATH}/${CLUSTER}-config/group_vars/k8s_cluster/ck8s-k8s-cluster.yaml , normally this should be set to https://dex.BASE_DOMAIN . For cloud provider integration, you have a few options as described here . We will be going with the external cloud provider and simply source the Openstack credentials. Setting up Kubespray variables \u00b6 Below are some examples for ${CK8S_CONFIG_PATH}/${CLUSTER}-config/group_vars/k8s_cluster/ck8s-k8s-cluster-openstack.yaml for a few selected openstack providers. The examples are copy-pastable, but you will have to change some of the values. Citycloud Fra1 etcd_kubeadm_enabled: true cloud_provider: external external_cloud_provider: openstack calico_mtu: 1480 cinder_csi_enabled: true persistent_volumes_enabled: true expand_persistent_volumes: true openstack_blockstorage_ignore_volume_az: true ## Cinder CSI is enabled by default along with the configuration options to enable persistent volumes and the expansion of these volumes. ## It is also set to ignore the volume availability zone to allow volumes to attach to nodes in different or mismatching zones. The default works well with both CityCloud and SafeSpring. storage_classes: - name: cinder-csi is_default: true parameters: availability: nova allowVolumeExpansion: true ## openstack volume type list type: default_encrypted ## If you want to set up LBaaS in your cluster, you can add the following config: external_openstack_cloud_controller_extra_args: ## Must be different for every cluster in the same openstack project cluster-name: \"<your-cluster-name>.cluster.local\" ## use `openstack subnet list` to list the available subnets external_openstack_lbaas_subnet_id: \"your-cluster-subnet-uuid\" ## use `openstack network list` to list the available external networks external_openstack_lbaas_floating_network_id: \"your-external-network-uuid\" external_openstack_lbaas_method: \"ROUND_ROBIN\" external_openstack_lbaas_provider: \"octavia\" external_openstack_lbaas_use_octavia: true external_openstack_lbaas_create_monitor: true external_openstack_lbaas_monitor_delay: \"1m\" external_openstack_lbaas_monitor_timeout: \"30s\" external_openstack_lbaas_monitor_max_retries: \"3\" external_openstack_network_public_networks: - \"ext-net\" ## if you have use_access_ip = 0 in cluster.tfvars, you should add the public ip address of the master nodes to this variable supplementary_addresses_in_ssl_keys: [\"master-ip-address1\", \"master-ip-address2\", ...] Citycloud Kna1 etcd_kubeadm_enabled: true cloud_provider: external external_cloud_provider: openstack calico_mtu: 1480 cinder_csi_enabled: true persistent_volumes_enabled: true expand_persistent_volumes: true openstack_blockstorage_ignore_volume_az: true storage_classes: - name: cinder-csi is_default: true parameters: availability: nova allowVolumeExpansion: true ## openstack volume type list type: ceph_hdd_encrypted external_openstack_cloud_controller_extra_args: ## Must be different for every cluster in the same openstack project cluster-name: \"<your-cluster-name>.cluster.local\" ## use `openstack subnet list` to list the available subnets external_openstack_lbaas_subnet_id: \"your-cluster-subnet-uuid\" ## use `openstack network list` to list the available external networks external_openstack_lbaas_floating_network_id: \"your-external-network-uuid\" external_openstack_lbaas_method: \"ROUND_ROBIN\" external_openstack_lbaas_provider: \"octavia\" external_openstack_lbaas_use_octavia: true external_openstack_lbaas_create_monitor: true external_openstack_lbaas_monitor_delay: \"1m\" external_openstack_lbaas_monitor_timeout: \"30s\" external_openstack_lbaas_monitor_max_retries: \"3\" external_openstack_network_public_networks: - \"ext-net\" ## if you have use_access_ip = 0 in cluster.tfvars, you should add the public ip address of the master nodes to this variable supplementary_addresses_in_ssl_keys: [\"master-ip-address1\", \"master-ip-address2\", ...] Note At this point if the cluster is running on Safespring and you are using kubespray v2.17.0+ it is possible to create an application credential. Which will give the cluster its own set of credentials instead of using your own. To create a set of credentials use the following command: openstack application credential create <name> And set the following environment variables export OS_APPLICATION_CREDENTIAL_NAME: <name> export OS_APPLICATION_CREDENTIAL_ID: <project_id> export OS_APPLICATION_CREDENTIAL_SECRET: <secret> Run Kubespray \u00b6 Copy the script for generating dynamic ansible inventories: for CLUSTER in \" ${ SERVICE_CLUSTER } \" \" ${ WORKLOAD_CLUSTERS [@] } \" ; do cp kubespray/contrib/terraform/terraform.py \" ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/inventory.ini\" chmod +x \" ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/inventory.ini\" done Now it is time to run the Kubespray playbook! for CLUSTER in \" ${ SERVICE_CLUSTER } \" \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ./bin/ck8s-kubespray apply \" ${ CLUSTER } \" --flush-cache done Correct the Kubernetes API IP addresses \u00b6 Locate the encrypted kubeconfigs in ${CK8S_CONFIG_PATH}/.state/kube_config_*.yaml and edit them using sops. Copy the public IP address of the load balancer (usually one of the masters public IP address) and replace the private IP address for the server field in ${CK8S_CONFIG_PATH}/.state/kube_config_*.yaml . for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml done Test access to the clusters as follows \u00b6 You should now have an encrypted kubeconfig file for each cluster under $CK8S_CONFIG_PATH/.state . Check that they work like this: for CLUSTER in \" ${ SERVICE_CLUSTER } \" \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file \" ${ CK8S_CONFIG_PATH } /.state/kube_config_ ${ CLUSTER } .yaml\" \\ 'kubectl --kubeconfig {} get nodes' done Deploying Compliant Kubernetes Apps \u00b6 Now that the Kubernetes clusters are up and running, we are ready to install the Compliant Kubernetes apps. Clone compliantkubernetes-apps and Install Pre-requisites \u00b6 If you haven't done so already, clone the compliantkubernetes-apps repo and install pre-requisites. git clone https://github.com/elastisys/compliantkubernetes-apps.git cd compliantkubernetes-apps ansible-playbook -e 'ansible_python_interpreter=/usr/bin/python3' --ask-become-pass --connection local --inventory 127 .0.0.1, get-requirements.yaml Initialize the apps configuration \u00b6 export CK8S_ENVIRONMENT_NAME = my-environment-name #export CK8S_FLAVOR=[dev|prod] # defaults to dev export CK8S_CONFIG_PATH = ~/.ck8s/my-cluster-path export CK8S_CLOUD_PROVIDER = # [exoscale|safespring|citycloud|aws|baremetal] export CK8S_PGP_FP = <your GPG key fingerprint> # retrieve with gpg --list-secret-keys ./bin/ck8s init This will initialise the configuration in the ${CK8S_CONFIG_PATH} directory. Generating configuration files sc-config.yaml and wc-config.yaml , as well as secrets with randomly generated passwords in secrets.yaml . This will also generate read-only default configuration under the directory defaults/ which can be used as a guide for available and suggested options. ls -l $CK8S_CONFIG_PATH Configure the apps \u00b6 Edit the configuration files ${CK8S_CONFIG_PATH}/sc-config.yaml , ${CK8S_CONFIG_PATH}/wc-config.yaml and ${CK8S_CONFIG_PATH}/secrets.yaml and set the appropriate values for some of the configuration fields. Note that, the latter is encrypted. vim ${ CK8S_CONFIG_PATH } /sc-config.yaml vim ${ CK8S_CONFIG_PATH } /wc-config.yaml sops ${ CK8S_CONFIG_PATH } /secrets.yaml Tip The default configuration for the service cluster and workload cluster are available in the directory ${CK8S_CONFIG_PATH}/defaults/ and can be used as a reference for available options. Warning Do not modify the read-only default configurations files found in the directory ${CK8S_CONFIG_PATH}/defaults/ . Instead configure the cluster by modifying the regular files ${CK8S_CONFIG_PATH}/sc-config.yaml and ${CK8S_CONFIG_PATH}/wc-config.yaml as they will override the default options. The following are the minimum change you should perform: Citycloud Fra1, Kna1 # ${CK8S_CONFIG_PATH}/sc-config.yaml and ${CK8S_CONFIG_PATH}/wc-config.yaml global : baseDomain : set-me # set to $CK8S_ENVIRONMENT_NAME.$DOMAIN opsDomain : set-me # set to ops.$CK8S_ENVIRONMENT_NAME.$DOMAIN # issuer: letsencrypt-prod # set as default for prod flavor, defaults to \"letsencrypt-staging\" for dev objectStorage : # type: s3 # set as default for prod flavor, defaults to \"none\" for dev s3 : region : set-me # Kna1 for Karlskrona/Sweden, Fra1 for Frankfurt/Germany regionEndpoint : set-me # https://s3-<region>.citycloud.com:8080 # kna1 or fra1 # forcePathStyle: true # set as default ## This block is set as default for using service load balancers # ingressNginx: # controller: # useHostPort: false # service: # enabled: true # type: LoadBalancer # annotations: \"\" clusterAdmin : users : # set to the cluster admin users - set-me - admin@example.com # ${CK8S_CONFIG_PATH}/sc-config.yaml (in addition to the changes above) user : grafana : oidc : allowedDomains : # set to your domain(s), or unset using [] to deny all - set-me - example.com harbor : persistence : # type: swift # set as default for prod flavor, defaults to \"filesystem\" for dev # disableRedirect: true # set as default swift : authURL : set-me # https://<region>.citycloud.com:5000 # kna1 or fra1 regionName : set-me # Kna1 for Karlskrona/Sweden, Fra1 for Frankfurt/Germany projectDomainName : set-me userDomainName : set-me projectName : set-me projectID : set-me tenantName : set-me oidc : groupClaimName : set-me # set to group claim name used by OIDC provider adminGroupName : set-me # name of the group that automatically will get admin elasticsearch : extraRoleMappings : # set to configure elasticsearch access, or unset using [] - mapping_name : kibana_user definition : users : - set-me - mapping_name : kubernetes_log_reader definition : users : - set-me - mapping_name : all_access definition : users : - set-me alerts : opsGenieHeartbeat : # enabled: true # set as default for prod flavour, defaults to \"false\" for dev name : set-me # set to name the heartbeat if enabled issuers : letsencrypt : prod : email : set-me # set this to an email to receive LetsEncrypt notifications staging : email : set-me # set this to an email to receive LetsEncrypt notifications # ${CK8S_CONFIG_PATH}/wc-config.yaml (in addition to the changes above) user : namespaces : # set this to create user namespaces, or unset using [] - set-me - production - staging adminUsers : # set this to create admins in the user namespaces, or unset using [] - set-me - admin@example.com adminGroups : # set this to create admin groups in the user namespaces, or unset using [] - set-me # alertmanager: # add this block to enable user accessible alertmanager # enabled: true # namespace: alertmanager # note that this namespace must be listed above under \"user.namespaces\" opa : imageRegistry : URL : # set this to the allowed image registry, or unset using [] to deny all - set-me - harbor.example.com # ${CK8S_CONFIG_PATH}/secrets.yaml objectStorage : s3 : accessKey : set-me # set to your s3 accesskey secretKey : set-me # set to your s3 secretKey Safespring sto1 # ${CK8S_CONFIG_PATH}/sc-config.yaml and ${CK8S_CONFIG_PATH}/wc-config.yaml global : baseDomain : set-me # set to $CK8S_ENVIRONMENT_NAME.$DOMAIN opsDomain : set-me # set to ops.$CK8S_ENVIRONMENT_NAME.$DOMAIN # issuer: letsencrypt-prod # set as default for prod flavor, defaults to \"letsencrypt-staging\" for dev objectStorage : # type: s3 # set as default for prod flavor, defaults to \"none\" for dev s3 : region : sto2 regionEndpoint : https://s3.sto2.safedc.net # forcePathStyle: true # set as default clusterAdmin : users : # set to the cluster admin users - set-me - admin@example.com # ${CK8S_CONFIG_PATH}/sc-config.yaml (in addition to the changes above) user : grafana : oidc : allowedDomains : # set to your domain(s), or unset using [] to deny all - set-me - example.com harbor : # persistence: # type: objectStorage # set as default for prod flavor, defaults to \"filesystem\" for dev # disableRedirect: true # set as default oidc : groupClaimName : set-me # set to group claim name used by OIDC provider adminGroupName : set-me # name of the group that automatically will get admin elasticsearch : extraRoleMappings : # set to configure elasticsearch access, or unset using [] - mapping_name : kibana_user definition : users : - set-me - mapping_name : kubernetes_log_reader definition : users : - set-me - mapping_name : all_access definition : users : - set-me alerts : opsGenieHeartbeat : # enabled: true # set as default for prod flavour, defaults to \"false\" for dev name : set-me # set to name the heartbeat if enabled issuers : letsencrypt : prod : email : set-me # set this to an email to receive LetsEncrypt notifications staging : email : set-me # set this to an email to receive LetsEncrypt notifications # ${CK8S_CONFIG_PATH}/wc-config.yaml (in addition to the changes above) user : namespaces : # set this to create user namespaces, or unset using [] - set-me - production - staging adminUsers : # set this to create admins in the user namespaces, or unset using [] - set-me - admin@example.com adminGroups : # set this to create admin groups in the user namespaces, or unset using [] - set-me # alertmanager: # add this block to enable user accessible alertmanager # enabled: true # namespace: alertmanager # note that this namespace must be listed above under \"user.namespaces\" opa : imageRegistry : URL : # set this to the allowed image registry, or unset using [] to deny all - set-me - harbor.example.com # ${CK8S_CONFIG_PATH}/secrets.yaml objectStorage : s3 : accessKey : set-me # set to your s3 accesskey secretKey : set-me # set to your s3 secretKey Create S3 buckets \u00b6 You can use the following script to create required S3 buckets. The script uses s3cmd in the background and gets configuration and credentials for your S3 provider from $CK8S_CONFIG_PATH/.state/s3cfg.ini file. Citycloud Fra1, Kna1 # To get your s3 access and secret keys run: openstack --os-interface public ec2 credentials list # If you don't have any create them with: openstack --os-interface public ec2 credentials create # Use your default s3cmd config file: \"$CK8S_CONFIG_PATH/.state/s3cfg.ini\" that should contain: access_key = secret_key = host_base = s3-<region>.citycloud.com:8080 host_bucket = s3-<region>.citycloud.com:8080 signurl_use_https = True use_https = True ./scripts/S3/entry.sh --s3cfg \"$CK8S_CONFIG_PATH/.state/s3cfg.ini\" create DNS \u00b6 If are using service loadbalancers on citycloud you must provision it before you can setup the DNS. You can do that by running the following: # for the service cluster bin/ck8s bootstrap sc bin/ck8s ops helmfile sc -l app = common-psp-rbac -l app = service-cluster-psp-rbac apply bin/ck8s ops helmfile sc -l app = kube-prometheus-stack apply bin/ck8s ops helmfile sc -l app = ingress-nginx apply bin/ck8s ops kubectl sc get svc -n ingress-nginx # for the workload clusters for CLUSTER in \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_wc.yaml bin/ck8s bootstrap wc bin/ck8s ops helmfile wc -l app = common-psp-rbac -l app = workload-cluster-psp-rbac apply bin/ck8s ops helmfile wc -l app = kube-prometheus-stack apply bin/ck8s ops helmfile wc -l app = ingress-nginx apply bin/ck8s ops kubectl wc get svc -n ingress-nginx done Now that we have the loadbalancer public IPs we can setup the DNS. If you are using Exoscale as your DNS provider make sure that your have Exoscale cli installed and you can follow this guide for more details. If you are using AWS make sure you have AWS cli installed and follow the below instructions: vim ${ CK8S_CONFIG_PATH } /dns.json # add this lines { \"Comment\" : \"Manage test cluster DNS records\" , \"Changes\" : [ { \"Action\" : \"UPSERT\" , \"ResourceRecordSet\" : { \"Name\" : \"*.CK8S_ENVIRONMENT_NAME.DOMAIN\" , \"Type\" : \"A\" , \"TTL\" : 300 , \"ResourceRecords\" : [{ \"Value\" : \"<wc_cluster_lb_ip>\" }] } } , { \"Action\" : \"UPSERT\" , \"ResourceRecordSet\" : { \"Name\" : \"*.ops.CK8S_ENVIRONMENT_NAME.DOMAIN\" , \"Type\" : \"A\" , \"TTL\" : 300 , \"ResourceRecords\" : [{ \"Value\" : \"<sc_cluster_lb_ip>\" }] } } , { \"Action\" : \"UPSERT\" , \"ResourceRecordSet\" : { \"Name\" : \"grafana.CK8S_ENVIRONMENT_NAME.DOMAIN\" , \"Type\" : \"A\" , \"TTL\" : 300 , \"ResourceRecords\" : [{ \"Value\" : \"<sc_cluster_lb_ip>\" }] } } , { \"Action\" : \"UPSERT\" , \"ResourceRecordSet\" : { \"Name\" : \"harbor.CK8S_ENVIRONMENT_NAME.DOMAIN\" , \"Type\" : \"A\" , \"TTL\" : 300 , \"ResourceRecords\" : [{ \"Value\" : \"<sc_cluster_lb_ip>\" }] } } , { \"Action\" : \"UPSERT\" , \"ResourceRecordSet\" : { \"Name\" : \"notary.harbor.CK8S_ENVIRONMENT_NAME.DOMAIN\" , \"Type\" : \"A\" , \"TTL\" : 300 , \"ResourceRecords\" : [{ \"Value\" : \"<sc_cluster_lb_ip>\" }] } } , { \"Action\" : \"UPSERT\" , \"ResourceRecordSet\" : { \"Name\" : \"kibana.CK8S_ENVIRONMENT_NAME.DOMAIN\" , \"Type\" : \"A\" , \"TTL\" : 300 , \"ResourceRecords\" : [{ \"Value\" : \"<sc_cluster_lb_ip>\" }] } } , { \"Action\" : \"UPSERT\" , \"ResourceRecordSet\" : { \"Name\" : \"dex.CK8S_ENVIRONMENT_NAME.DOMAIN\" , \"Type\" : \"A\" , \"TTL\" : 300 , \"ResourceRecords\" : [{ \"Value\" : \"<sc_cluster_lb_ip>\" }] } } ] } # set your profile credentials AWS_ACCESS_KEY_ID = 'my-access-key' AWS_SECRET_ACCESS_KEY = 'my-secret-key' aws --configure default ${ AWS_ACCESS_KEY_ID } ${ AWS_SECRET_ACCESS_KEY } aws configure set region <region_name> # get your hosted zone id aws route53 list-hosted-zones # apply the DNS changes aws route53 change-resource-record-sets --hosted-zone-id <hosted_zone_id> --change-batch file:// ${ CK8S_CONFIG_PATH } /dns.json Install Compliant Kubernetes apps \u00b6 Start with the service cluster: ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ SERVICE_CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_sc.yaml ./bin/ck8s apply sc # Respond \"n\" if you get a WARN Then the workload clusters: for CLUSTER in \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_wc.yaml ./bin/ck8s apply wc # Respond \"n\" if you get a WARN done Settling \u00b6 Important Leave sufficient time for the system to settle, e.g., request TLS certificates from LetsEncrypt, perhaps as much as 20 minutes. You can check if the system settled as follows: for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get --all-namespaces pods' done Check the output of the command above. All Pods needs to be Running or Completed. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get --all-namespaces issuers,clusterissuers,certificates' done Check the output of the command above. All resources need to have the Ready column True. Testing \u00b6 After completing the installation step you can test if the apps are properly installed and ready using the commands below. Start with the service cluster: ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ SERVICE_CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_sc.yaml ./bin/ck8s test sc # Respond \"n\" if you get a WARN Then the workload clusters: for CLUSTER in \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_wc.yaml ./bin/ck8s test wc # Respond \"n\" if you get a WARN done Done. Navigate to the endpoints, for example grafana.$BASE_DOMAIN , kibana.$BASE_DOMAIN , harbor.$BASE_DOMAIN , etc. to discover Compliant Kubernetes's features. Teardown \u00b6 Removing Compliant Kubernetes Apps from your cluster \u00b6 To remove the applications added by compliant kubernetes you can use the two scripts clean-sc.sh and clean-wc.sh , they are located here in the scripts folder . They perform the following actions: Delete the added helm charts Delete the added namespaces Delete any remaining PersistentVolumes Delete the added CustomResourceDefinitions Note: if user namespaces are managed by Compliant Kubernetes apps then they will also be deleted if you clean up the workload cluster. Remove infrastructure \u00b6 To teardown the infrastructure, please switch to the root directory of the Kubespray repo (see the Terraform section). Make sure you remove all PersistentVolumes and Services with type=LoadBalancer . These objects may create cloud resources that are not managed by Terraform, and therefore would not be removed when we destroy the infrastructure. MODULE_PATH = \" $( pwd ) /kubespray/contrib/terraform/openstack\" for CLUSTER in \" ${ SERVICE_CLUSTER } \" \" ${ WORKLOAD_CLUSTERS [@] } \" ; do pushd \" ${ MODULE_PATH } \" terraform init terraform destroy -var-file = \" ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/cluster.tfvars\" -state = \" ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/terraform.tfstate\" popd done # Remove DNS records Don't forget to remove any DNS records and object storage buckets that you may have created.","title":"On Openstack"},{"location":"operator-manual/ovh-managed-kubernetes/","text":"This page is out of date We are currently working on internal documentation to streamline Compliant Kubernetes onboarding for selected cloud providers. Until those documents are ready, and until we have capacity to make parts of that documentation public, this page is out-of-date. Nevertheless, parts of it are useful. Use at your own risk and don't expect things to work smoothly. Compliant Kubernetes Deployment on OVH Managed Kubernetes \u00b6 This document contains instructions on how to setup a service cluster and a workload cluster in OVH . The following are the main tasks addressed in this document: Setting up Compliant Kubernetes for OVH Managed Kubernetes Deploying Compliant Kubernetes on top of two Kubernetes clusters. The instructions below are just samples, you need to update them according to your requirements. Before starting, make sure you have all necessary tools . Note This guide is written for compliantkubernetes-apps v0.13.0 Setup \u00b6 Create two Kubernetes clusters in OVH, follow this guide . Sizing hint For the service cluster you can start with creating 3 nodes of size B2-7 and add a B2-15 node. The workload cluster is fine with 3 B2-7 nodes. Configure Compliant Kubernetes \u00b6 Start by preparing your shell with some variables that will be used. REGION=\"waw\" # Region for the cluster ISSUER_MAIL=\"user@example.com\" # Mail that will be used for the LetsEncrypt certificate issuer export CK8S_CONFIG_PATH=~/.ck8s/my-ovh-cluster # Path for the configuration export CK8S_ENVIRONMENT_NAME=my-ovh-cluster # Name of the environment export CK8S_PGP_FP=\"FOOBAR1234567\" # Fingerprint of your PGP key, retrieve with gpg --list-secret-keys export CK8S_CLOUD_PROVIDER=baremetal # We don't have a OVH flavor, but baremetal is fine export CK8S_FLAVOR=dev # Change to \"prod\" if it's a production cluster you're setting up S3_ACCESS_KEY=\"foo\" # Access key for S3, see https://docs.ovh.com/gb/en/public-cloud/getting_started_with_the_swift_S3_API/#create-ec2-credentials S3_SECRET_KEY=\"bar\" # Secret key for S3 Download the kubeconfig and set them up for Compliant Kubernetes by following these steps: Create the path where they're going to be stored \u00b6 mkdir -p \"${CK8S_CONFIG_PATH}/.state\" Download the kubeconfig from OVH for the service cluster and run: \u00b6 mv ~/Downloads/kubeconfig.yml \"${CK8S_CONFIG_PATH}/.state/kube_config_sc.yaml\" sops --encrypt --in-place --pgp \"${CK8S_PGP_FP}\" \"${CK8S_CONFIG_PATH}/.state/kube_config_sc.yaml\" Download the kubeconfig from OVH for the workload cluster and run: \u00b6 mv ~/Downloads/kubeconfig.yml \"${CK8S_CONFIG_PATH}/.state/kube_config_wc.yaml\" sops --encrypt --in-place --pgp \"${CK8S_PGP_FP}\" \"${CK8S_CONFIG_PATH}/.state/kube_config_wc.yaml\" Prepare DNS records \u00b6 Set up DNS records in OVH. Run this snippet and append it into \"Change in text format\" in your domain in OVH. IP=\"203.0.113.123\" cat <<EOF | envsubst *.ops.${CK8S_ENVIRONMENT_NAME} 60 IN A ${IP} *.${CK8S_ENVIRONMENT_NAME} 60 IN A ${IP} grafana.${CK8S_ENVIRONMENT_NAME} 60 IN A ${IP} harbor.${CK8S_ENVIRONMENT_NAME} 60 IN A ${IP} kibana.${CK8S_ENVIRONMENT_NAME} 60 IN A ${IP} dex.${CK8S_ENVIRONMENT_NAME} 60 IN A ${IP} notary.harbor.${CK8S_ENVIRONMENT_NAME} 60 IN A ${IP} EOF Since we don't know the IP for the loadbalancer yet, you can set them to 203.0.113.123 (TEST-NET-3) . Create required buckets that Compliant Kubernetes will use. cat <<EOF | sops --encrypt --pgp \"${CK8S_PGP_FP}\" --output-type ini --input-type ini /dev/stdin > s3cmd.ini [default] use_https = True host_base = s3.${REGION}.cloud.ovh.net host_bucket = s3.${REGION}.cloud.ovh.net access_key = ${S3_ACCESS_KEY} secret_key = ${S3_SECRET_KEY} EOF sops exec-file --no-fifo s3cfg.ini \"s3cmd --config {} mb s3://${CK8S_ENVIRONMENT_NAME}-harbor\" sops exec-file --no-fifo s3cfg.ini \"s3cmd --config {} mb s3://${CK8S_ENVIRONMENT_NAME}-velero\" sops exec-file --no-fifo s3cfg.ini \"s3cmd --config {} mb s3://${CK8S_ENVIRONMENT_NAME}-es-backup\" sops exec-file --no-fifo s3cfg.ini \"s3cmd --config {} mb s3://${CK8S_ENVIRONMENT_NAME}-influxdb\" sops exec-file --no-fifo s3cfg.ini \"s3cmd --config {} mb s3://${CK8S_ENVIRONMENT_NAME}-sc-logs\" Download Compliant Kubernetes and checkout the latest version. git clone git@github.com:elastisys/compliantkubernetes-apps.git cd compliantkubernetes-apps git checkout v0.13.0 Initialize the config. bin/ck8s init Update the service cluster configuration for OVH yq write --inplace sc-config.yaml global.baseDomain \"${CK8S_ENVIRONMENT_NAME}.${DOMAIN}\" yq write --inplace sc-config.yaml global.opsDomain \"ops.${CK8S_ENVIRONMENT_NAME}.${DOMAIN}\" yq write --inplace sc-config.yaml global.issuer \"letsencrypt-prod\" yq write --inplace sc-config.yaml storageClasses.default \"csi-cinder-high-speed\" yq write --inplace sc-config.yaml storageClasses.local.enabled \"false\" yq write --inplace sc-config.yaml objectStorage.s3.region \"${REGION}\" yq write --inplace sc-config.yaml objectStorage.s3.regionEndpoint \"https://s3.${REGION}.cloud.ovh.net/\" yq write --inplace sc-config.yaml fluentd.forwarder.useRegionEndpoint \"false\" yq write --inplace sc-config.yaml nfsProvisioner.server \"not-used\" yq write --inplace sc-config.yaml ingressNginx.controller.useHostPort \"false\" yq write --inplace sc-config.yaml ingressNginx.controller.service.enabled \"true\" yq write --inplace sc-config.yaml ingressNginx.controller.service.type \"LoadBalancer\" yq write --inplace sc-config.yaml ingressNginx.controller.service.annotations \"\" yq write --inplace sc-config.yaml issuers.letsencrypt.prod.email \"${ISSUER_MAIL}\" yq write --inplace sc-config.yaml issuers.letsencrypt.staging.email \"${ISSUER_MAIL}\" yq write --inplace sc-config.yaml metricsServer.enabled \"false\" Update the workload cluster configuration for OVH yq write --inplace wc-config.yaml global.baseDomain \"${CK8S_ENVIRONMENT_NAME}.${DOMAIN}\" yq write --inplace wc-config.yaml global.opsDomain \"ops.${CK8S_ENVIRONMENT_NAME}.${DOMAIN}\" yq write --inplace wc-config.yaml global.issuer \"letsencrypt-prod\" yq write --inplace wc-config.yaml storageClasses.default \"csi-cinder-high-speed\" yq write --inplace wc-config.yaml storageClasses.local.enabled \"false\" yq write --inplace wc-config.yaml objectStorage.s3.region \"${REGION}\" yq write --inplace wc-config.yaml objectStorage.s3.regionEndpoint \"https://s3.${REGION}.cloud.ovh.net/\" yq write --inplace wc-config.yaml ingressNginx.controller.useHostPort \"false\" yq write --inplace wc-config.yaml ingressNginx.controller.service.enabled \"true\" yq write --inplace wc-config.yaml ingressNginx.controller.service.type \"LoadBalancer\" yq write --inplace wc-config.yaml ingressNginx.controller.service.annotations \"\" yq write --inplace wc-config.yaml metricsServer.enabled \"false\" Set s3 credentials sops --set '[\"objectStorage\"][\"s3\"][\"accessKey\"] \"'\"${S3_ACCESS_KEY}\"'\"' secrets.yaml sops --set '[\"objectStorage\"][\"s3\"][\"secretKey\"] \"'\"${S3_SECRET_KEY}\"'\"' secrets.yaml Deploy Compliant Kubernetes \u00b6 Now you're ready to deploy Compliant Kubernetes. When the apply command is done, fetch the external IP assigned to the loadbalancer service and update the DNS record to match this. bin/ck8s apply sc bin/ck8s ops kubectl sc get svc -n ingress-nginx ingress-nginx-controller Run this snippet and update the DNS records you added previously to match the external IP of the service cluster load balancer. IP=\"SERVICE_CLUSTER_LB_IP\" cat <<EOF | envsubst *.ops.${CK8S_ENVIRONMENT_NAME} IN A ${IP} grafana.${CK8S_ENVIRONMENT_NAME} IN A ${IP} harbor.${CK8S_ENVIRONMENT_NAME} IN A ${IP} kibana.${CK8S_ENVIRONMENT_NAME} IN A ${IP} dex.${CK8S_ENVIRONMENT_NAME} IN A ${IP} notary.harbor.${CK8S_ENVIRONMENT_NAME} IN A ${IP} EOF Next, do the same for the workload cluster. bin/ck8s apply wc bin/ck8s ops kubectl wc get svc -n ingress-nginx ingress-nginx-controller Run this snippet and update the DNS records you added previously to match the external IP of the workload cluster load balancer. IP=\"WORKLOAD_CLUSTER_LB_IP\" cat <<EOF | envsubst *.${CK8S_ENVIRONMENT_NAME} IN A ${IP} EOF Limitations \u00b6 At the time of writing, there's some issues with velero and fluentd. Workarounds for making this work until fix is out Add s3_endpoint to fluentd, fixed with this issue \u00b6 To add this, you need to edit the config map fluentd uses. Find the <match **> tag and add s3_endpoint https://s3.REGION.cloud.ovh.net/ Where REGION matches what region you're using ( waw in the example). After that, delete the fluentd pod to force it to reload the configuration. $ bin/ck8s ops kubectl sc edit cm -n fluentd fluentd-aggregator-configmap . . . <match **> @id output-s3 @type s3 aws_key_id \"#{ENV['AWS_ACCESS_KEY_ID']}\" aws_sec_key \"#{ENV['AWS_ACCESS_SECRET_KEY']}\" s3_endpoint https://s3.waw.cloud.ovh.net/ # <--- Add this line s3_region waw . . $ bin/ck8s ops kubectl sc delete pod -n fluentd fluentd-0 Update velero to use 1.3.0 instead of 1.2.0, fixed with this issue \u00b6 For velero to work with OVH S3, velero needs to run with version 1.3.0 . bin/ck8s ops kubectl sc patch deployment -n velero velero -p '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"velero\",\"image\":\"velero/velero:v1.3.0\"}]}}}}' bin/ck8s ops kubectl sc patch daemonset -n velero restic -p '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"velero\",\"image\":\"velero/velero:v1.3.0\"}]}}}}' bin/ck8s ops kubectl wc patch deployment -n velero velero -p '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"velero\",\"image\":\"velero/velero:v1.3.0\"}]}}}}' bin/ck8s ops kubectl wc patch daemonset -n velero restic -p '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"velero\",\"image\":\"velero/velero:v1.3.0\"}]}}}}'","title":"On OVH Managed Kubernetes"},{"location":"operator-manual/provider-audit/","tags":["ISO 27001 A.12.4.4","ISO 27001 A.15"],"text":"Cloud Provider Audit \u00b6 Note This section helps you implement ISO 27001, specifically: A.12.4.4 Clock Synchronization A.15 Supplier Relationships This page will help you do your due diligence and ensure you choose a cloud provider that provides a solid foundation for Compliant Kubernetes and your application. Elastisys regularly uses this template to validate cloud partners, as required for ISO 27001 certification. Rationale \u00b6 Compliant Kubernetes is designed to build upon the security and compliance of the underlying cloud provider. If you cannot trust the underlying provider with controls such as physical security to the servers, safe disposal of hard drives, access control to infrastructure control plane, then no technical measure will help you achieve your security and compliance goals. Trying to take preventive measures in Compliant Kubernetes -- i.e., at the platform level -- is inefficient at best and downright dangerous at worst. Failing to due your due diligence will end up in security theatre , putting your reputation at risk. Overview \u00b6 The remainder of this page contains open questions that you should ask your cloud provider. Notice the following: Make sure you ask open questions and note down the answers. Burden of proof lies with the provider that they do an excellent job with protecting data. Ask all questions, then evaluate the provider's suitability. It is unlikely that you'll find the perfect provider, but you'll likely find one that is sufficient for your present and future needs. The least expected the answer, the more \"digging\" is needed. \"You\" represents the cloud provider and \"I\" represents the Compliant Kubernetes administrator. Technical Capability Questionnaire \u00b6 Availability Zones: Where are your data centers located? How are they presented, i.e., single API vs. multiple independent APIs? Services: What services do you offer? (e.g., VMs, object storage) Are all your services available in all zones? Identity and Access Management (IAM): Do you offer IAM? How can I create roles? Via an API? Via a UI? Via a Terraform provider? What services can I configure role-based access control for? Can IAM be configured via API? Can IAM be configured via Terraform? Can one single user be given access to multiple projects? Infrastructure aaS: Which IaaS engine do you use? (e.g., OpenStack, VMware, proprietary) Do you have a Terraform provider for your API? Do you have pre-uploaded Ubuntu images? Which? Do these images have AutomaticSecurityUpdates by default ? Do these images have NTP enabled by default ? Do you have a Kubernetes integration for your IaaS? Can I use a cloud-controller for automatic discovery of Nodes and labeling Nodes with the right Zone? Can you handle large diurnal capacity changes, a.k.a., auto-scaling? E.g., 40 VMs from 6.00 to 10.00, but only 10 VMs from 10.00-6.00. Can I reserve VMs? How do you bill for reserved but unused VMs? What technical implementation do you recommend? E.g., pause/unpause VMs, stop/start VMs, terminate/recreate VMs. Storage capabilities: Do you offer Object Storage as a Service (OSaaS)? Can I use the object storage via an S3-compatible API? Can I create buckets via API? Can I create bucket credentials via API? Do you have a Terraform provider for your API? In which zones? Do you have immutable storage or object lock? Is OSaaS stretched across zones? Is object storage replicated across zones? Do you offer Block storage as a Service (BLaaS)? Which API (OpenStack, VMware)? In which zones? Can I use a Container Storage Interface (CSI) driver for automatic creating of PersistentVolumes? Is BSaaS stretched across zones? Is block storage replicated across zones? Do you offer encryption-at-rest? Encrypted object storage: Do you offer this by default? Encrypted block storage: Do you offer this by default? Encrypted boot discs: Do you offer this by default? Networking capabilities: Can the VMs be set up on a private network? Do you have a Terraform provider for your API? Is your private network stretched across zones? Do you trust the network between your data centers? Does the private network overlap: The default Docker network ( 172.17.0.0/16 )? The default Kubernetes Service network ( 10.233.0.0/18 )? The default Kubernetes Pod network ( 10.233.64.0/18 )? Firewall aaS Are Firewall aaS available? What API? (e.g., OpenStack, VMware) Do you have a Terraform provider for your API? Do you offer Load Balancer aaS (LBaaS)? Can I create a LB via API? Do you have a Terraform provider for your API? Can I use a cloud-controller for automatic creation of external LoadBalancers ? Can I set up a LB across zones? Via API? Can VMs see themselves via the LB's IP? (If not, then VMs need a minor fix .) Do your LBs preserve source IPs? Usually, this involves clever DNAT or PROXY protocol support . Do you offer IPv6 support? By default? Do you offer DNS as a Service? Which API? Network security: Do you allow NTP (UDP port 123) for clock synchronization to the Internet? If not, do you have a private NTP server? Do you allow ACME (TCP port 80) for automated certificate provisioning via LetsEncrypt ? If not, how will you provision certificates? Organizational capabilities \u00b6 What regulations are your existing customers subject to? (e.g., GDPR, public sector regulations, some ISO 27001 profile) Can you show us your ISO-27001 certification? Which profile? Which organization made the audit? Can we get a copy of the Statement of Applicability (SoA) ? Who is overall responsible with compliance in your organization? How do you implement regulatory and contractual requirements? How is a new requirement discovered? What is the journey that a requirement takes from discovery, to updating policies, to training employees, to implementation, to evidence of implementation? How is physical security handled? How do you handle incidents and deviations? What response times / time to resolution do you offer? What are your actual response times / time to resolution? What is your change management process? How do you handle technical vulnerabilities? How do you handle capacity management? In case of a breach, how long until you notify your customers? What SLA do you offer? What uptime do you offer? What is your measured uptime? Do you have a public status page? How do you handle access control? Does your operation team have individual accounts? How do you handle team member onboarding / offboarding? How do you communicate credentials to your customers? Do you have audit logs? How long do you store audit logs? Who has access to them? How are they protected against disclosure and tampering? How do you handle business continuity? How often do you test fail-over? How did the last test go? How do you handle disaster recovery? How often do you test disaster recovery? How did the last test go? Legal issues \u00b6 Do you fully operate under EU jurisdiction? Is your ownership fully under EU jurisdiction? Are your suppliers fully under EU jurisdiction? Even the web fonts and analytics code on your front-page? Do you have a DPO? Is this an internal employee or outsourced? Collaboration \u00b6 How can we collaborate with your on-call team? What collaboration channels do you offer? (e.g., Slack, Teams, phone, service desk) What response times can we expect? Is your on-call team available 24/7? Are you open to having quarterly operations (engineering) retrospectives? Our engineering team wants to keep a close loop with vendors and regularly discuss what went well, what can be improved, and devise a concrete action plan. Are you open to having quarterly roadmap discussions?","title":"Provider Audit"},{"location":"operator-manual/qa/","text":"Quality Assurance \u00b6 When you have created your Compliant Kubernetes cluster it can be wise to run some checks to ensure that it works as expected. This document details some snippets that you can follow in order to ensure some functionality of the cluster. Customer API and Harbor access \u00b6 Pre-requisites \u00b6 You've got Docker installed. You've exported CK8S_CONFIG_PATH in your shell. You've set baseDomain in your shell to what's used in your cluster. Your current working directory is the compliantkubernets-apps repository. You've installed the kubectl plugin kubelogin . See instructions on how to install it. Create and set user kubeconfig \u00b6 ./bin/ck8s user-kubeconfig sops -d -i --config ${CK8S_CONFIG_PATH}/.sops.yaml ${CK8S_CONFIG_PATH}/user/kubeconfig.yaml export KUBECONFIG=${CK8S_CONFIG_PATH}/user/kubeconfig.yaml Authenticate by issuing any kubectl command, e.g. kubectl get pods Your browser will be opened and you'll be asked to login through Dex. Login to Harbor GUI and create 'test' project \u00b6 Go to https://harbor.${baseDomain} , and login though OIDC. Create project 'test'. Click on your user in the top right corner and select User profile. Copy CLI secret. Push image to Harbor and scan it \u00b6 Pull Nginx from dockerhub docker pull nginx . Login to the Harbor registry docker login https://harbor.${baseDomain} Enter your Harbor username and the copied CLI secret. Prepare Nginx image for pushing to Harbor registry docker tag nginx harbor.${baseDomain}/test/nginx Push image to Harbor docker push harbor.${baseDomain}/test/nginx Enter 'test' project in the Harbor GUI, select the newly pushed image and scan it. Create secret for pulling images from harbor \u00b6 kubectl create secret generic regcred \\ --from-file=.dockerconfigjson=${HOME}/.docker/config.json \\ --type=kubernetes.io/dockerconfigjson kubectl patch serviceaccount default -p '{\"imagePullSecrets\": [{\"name\": \"regcred\"}]}' Test pulling from Harbor and start privileged and unprivileged pods \u00b6 kubectl run --image nginxinc/nginx-unprivileged nginx-unprivileged kubectl run --image harbor.${baseDomain}/test/nginx nginx-privileged # You should see that both pods and that nginx-unprivileged eventually becomes running while nginx-privileged does not. kubectl get pods # Check events from the nginx-privileged. kubectl describe pod nginx-privileged # You should see 'Error: container has runAsNonRoot and image will run as root'. Cleanup of created Kubernetes resources \u00b6 kubectl delete pod --all kubectl delete secret regcred","title":"QA"},{"location":"operator-manual/safespring/","text":"This page is out of date We are currently working on internal documentation to streamline Compliant Kubernetes onboarding for selected cloud providers. Until those documents are ready, and until we have capacity to make parts of that documentation public, this page is out-of-date. Nevertheless, parts of it are useful. Use at your own risk and don't expect things to work smoothly. Compliant Kubernetes Deployment on Safespring \u00b6 This document contains instructions on how to set up a Compliant Kubernetes environment (consisting of a service cluster and one or more workload clusters) on Safespring. Note This guide is written for compliantkubernetes-apps v0.13.0 TODO: The document is split into two parts: Cluster setup (setting up infrastructure and the Kubernetes clusters). We will be using the Terraform module for Openstack that can be found in the Kubespray repository . Please refer to it if you need more details about this part of the setup. Apps setup (including information about limitations) Before starting, make sure you have all necessary tools . In addition to these general tools, you will also need: Openstack credentials (either using openrc or the clouds.yaml configuration file) for setting up the infrastructure. For Safespring, you can get these by logging into the Safespring Openstack dashboard and download either the clouds.yaml or OpenRC file from the API Access page. Note Although recommended OpenStack authentication method is clouds.yaml it is more convenient to use the openrc method with compliant kubernetes as it works both with kubespray and terraform. If you are using the clouds.yaml method, at the moment, kubespray will still expect you to set a few environment variables. Initialize configuration folder \u00b6 Choose names for your service cluster and workload cluster(s): SERVICE_CLUSTER = \"testsc\" WORKLOAD_CLUSTERS =( \"testwc0\" \"testwc1\" ) Start by initializing a Compliant Kubernetes environment using Compliant Kubernetes Kubespray. All of this is done from the root of the compliantkubernetes-kubespray repository. export CK8S_CONFIG_PATH = ~/.ck8s/<environment-name> export SOPS_FP = <PGP-fingerprint> for CLUSTER in $SERVICE_CLUSTER \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ./bin/ck8s-kubespray init \" ${ CLUSTER } \" openstack \" ${ SOPS_FP } \" done Infrastructure setup using Terraform \u00b6 Configure Terraform by creating a cluster.tfvars file for each cluster. The available options can be seen in kubespray/contrib/terraform/openstack/variables.tf . There is a sample file that can be copied to get something to start from. for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do cp kubespray/contrib/terraform/openstack/sample-inventory/cluster.tfvars \" ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/cluster.tfvars\" done Note You really must edit the values in these files. There is no way to set sane defaults for what flavor to use, what availability zones or networks are available across providers. In the section below some guidance and samples are provided but remember that they might be useless to you depending on your needs and setup. Infrastructure guidance \u00b6 We recommend you to have at least three worker nodes with 4 cores and 8 GB memory each, and we recommend you to have at least 2 cores and 4 GB for your control plane nodes. Below is example cluster.tfvars for a few select openstack providers. The examples are copy-pastable, but you might want to change cluster_name and network_name (if neutron is used!). Safespring sto1 with public IPs assigned to VMs (no floating IP) # your Kubernetes cluster name here cluster_name = \"your-cluster-name\" # image to use for bastion, masters, standalone etcd instances, and nodes image = \"ubuntu-20.04\" # 0|1 bastion nodes number_of_bastions = 0 use_neutron = 0 # standalone etcds number_of_etcd = 0 # masters number_of_k8s_masters = 0 number_of_k8s_masters_no_etcd = 0 number_of_k8s_masters_no_floating_ip = 1 number_of_k8s_masters_no_floating_ip_no_etcd = 0 flavor_k8s_master = \"8a707999-0bce-4f2f-8243-b4253ba7c473\" # nodes number_of_k8s_nodes = 0 number_of_k8s_nodes_no_floating_ip = 3 flavor_k8s_node = \"5b40af67-9d11-45ed-a44f-e876766160a5\" # networking # ssh access to nodes k8s_allowed_remote_ips = [\"0.0.0.0/0\"] worker_allowed_ports = [ { # Node ports \"protocol\" = \"tcp\" \"port_range_min\" = 30000 \"port_range_max\" = 32767 \"remote_ip_prefix\" = \"0.0.0.0/0\" }, { # HTTP \"protocol\" = \"tcp\" \"port_range_min\" = 80 \"port_range_max\" = 80 \"remote_ip_prefix\" = \"0.0.0.0/0\" }, { # HTTPS \"protocol\" = \"tcp\" \"port_range_min\" = 443 \"port_range_max\" = 443 \"remote_ip_prefix\" = \"0.0.0.0/0\" } ] external_net = \"b19680b3-c00e-40f0-ad77-4448e81ae226\" use_access_ip = 1 network_name = \"public\" use_server_groups = true # Comment this out if you don't mind that the VMs are schedules on different physical machines # Also, comment this out if you run into problems on Safespring because of shortage of physical machines Safespring sto1 with private IPs and floating IPs assigned to VMs For this to work, make sure that you can create networks and floating IPs on Safespring. ``` hcl # your Kubernetes cluster name here cluster_name = \"your-cluster-name\" # image to use for bastion, masters, standalone etcd instances, and nodes image = \"ubuntu-20.04\" # 0|1 bastion nodes number_of_bastions = 0 use_neutron = 0 # standalone etcds number_of_etcd = 0 # masters number_of_k8s_masters = 1 number_of_k8s_masters_no_etcd = 0 number_of_k8s_masters_no_floating_ip = 0 number_of_k8s_masters_no_floating_ip_no_etcd = 0 flavor_k8s_master = \"8a707999-0bce-4f2f-8243-b4253ba7c473\" # nodes number_of_k8s_nodes = 3 number_of_k8s_nodes_no_floating_ip = 0 flavor_k8s_node = \"5b40af67-9d11-45ed-a44f-e876766160a5\" # networking # ssh access to nodes k8s_allowed_remote_ips = [\"0.0.0.0/0\"] worker_allowed_ports = [ { # Node ports \"protocol\" = \"tcp\" \"port_range_min\" = 30000 \"port_range_max\" = 32767 \"remote_ip_prefix\" = \"0.0.0.0/0\" }, { # HTTP \"protocol\" = \"tcp\" \"port_range_min\" = 80 \"port_range_max\" = 80 \"remote_ip_prefix\" = \"0.0.0.0/0\" }, { # HTTPS \"protocol\" = \"tcp\" \"port_range_min\" = 443 \"port_range_max\" = 443 \"remote_ip_prefix\" = \"0.0.0.0/0\" } ] external_net = \"b19680b3-c00e-40f0-ad77-4448e81ae226\" use_access_ip = 1 network_name = \"private\" # Change this if you would like a new network to be created use_server_groups = true # Comment this out if you don't mind that the VMs are schedules on different physical machines # Also, comment this out if you run into problems on Safespring because of shortage of physical machines ``` Expose Openstack credentials to Terraform \u00b6 Terraform will need access to Openstack credentials in order to create the infrastructure. More details can be found here . We will be using the declarative option with the clouds.yaml file. Since this file can contain credentials for multiple environments, we specify the name of the one we want to use in the environment variable OS_CLOUD : export OS_CLOUD = <name-of-openstack-cloud-environment> If you use the clouds.yaml file copy it in the compliantkubernetes-kubespray/kubespray/contrib/terraform/openstack/ and edit the file to add your password password: your-safespring-openstack-password in the auth: section. Alternatively, using the openrc file: source /path/to/your/openrc/file Initialize and apply Terraform \u00b6 MODULE_PATH = \" $( pwd ) /kubespray/contrib/terraform/openstack\" pushd \" ${ MODULE_PATH } \" for CLUSTER in $SERVICE_CLUSTER \" ${ WORKLOAD_CLUSTERS [@] } \" ; do terraform init terraform apply -var-file = \" ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/cluster.tfvars\" -state = \" ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/terraform.tfstate\" done popd Warning The above will not work well if you are using a bastion host. This is due to some hard coded paths . To work around it, you may link the kubespray/contrib folder to the correct relative path, or make sure your CK8S_CONFIG_PATH is already at a proper place relative to the same. Install Kubernetes using Kubespray \u00b6 Before we can run Kubespray, we will need to go through the relevant variables. Additionally we will need to expose some credentials so that Kubespray can set up cloud provider integration. You will need to change at least one value: kube_oidc_url in group_vars/k8s_cluster/ck8s-k8s_cluster.yaml , normally this should be set to https://dex.BASE_DOMAIN . Note If you have use_access_ip = 0 in cluster.tfvars , you should add the public ip address of the master nodes to the variable supplementary_addresses_in_ssl_keys = [\"<master-0-ip-address>\",...] somewhere under group_vars/ . For cloud provider integration, you have a few options as described here . We will be going with the external cloud provider and simply source the Openstack credentials. See below for how to modify the variables that need to be modified. Setting up Kubespray variables \u00b6 In ${CK8S_CONFIG_PATH}/$CLUSTER-config/group_vars/k8s_cluster/ck8s-k8s-cluster-openstack.yaml , the default variables should look like this: etcd_kubeadm_enabled : true cloud_provider : external external_cloud_provider : openstack calico_mtu : 1480 external_openstack_cloud_controller_extra_args : # Must be different for every cluster in the same openstack project cluster-name : \"set-me\" cinder_csi_enabled : true persistent_volumes_enabled : true expand_persistent_volumes : true openstack_blockstorage_ignore_volume_az : true storage_classes : - name : cinder-csi is_default : true parameters : allowVolumeExpansion : true availability : nova cluster-name should be set to a name that is unique in the Openstack project you're deploying your clusters in. If you don't have any other clusters in the project, just make sure that the service cluster and workload clusters have different names. Cinder CSI is enabled by default along with the configuration options to enable persistent volumes and the expansion of these volumes. It is also set to ignore the volume availability zone to allow volumes to attach to nodes in different or mismatching zones. The default works well with both CityCloud and SafeSpring. If you want to set up LBaaS in your cluster, you can add the following config: external_openstack_lbaas_create_monitor : false external_openstack_lbaas_monitor_delay : \"1m\" external_openstack_lbaas_monitor_timeout : \"30s\" external_openstack_lbaas_monitor_max_retries : \"3\" external_openstack_lbaas_provider : octavia external_openstack_lbaas_use_octavia : true # external_openstack_lbaas_network_id: \"Neutron network ID to create LBaaS VIP\" external_openstack_lbaas_subnet_id : \"Neutron subnet ID to create LBaaS VIP\" external_openstack_lbaas_floating_network_id : \"Neutron network ID to get floating IP from\" # external_openstack_lbaas_floating_subnet_id: \"Neutron subnet ID to get floating IP from\" external_openstack_lbaas_method : \"ROUND_ROBIN\" external_openstack_lbaas_manage_security_groups : false external_openstack_lbaas_internal_lb : false The network_id and subnet_id variables need to be set by you, depending on whether or not you used floating IP. network_id should match the external_net variable in your Terraform variables, whereas the subnet_id should match the subnet ID that Terraform outputs after it is applied. Additionally, when you later set up compliantkubernetes-apps in your cluster, you should set ingressNginx.controller.service.enabled to true and ingressNginx.controller.service.type to LoadBalancer in both your sc-config.yaml and wc-config.yaml . Use the IP of the ingress-nginx-controller service in your cluster when you set up your DNS. Note At this point if the cluster is running on Safespring and you are using kubespray v2.17.0+ it is possible to create an application credential. Which will give the cluster its own set of credentials instead of using your own. To create a set of credentials use the following command: openstack application credential create <name> And set the following environment variables export OS_APPLICATION_CREDENTIAL_NAME: <name> export OS_APPLICATION_CREDENTIAL_ID: <project_id> export OS_APPLICATION_CREDENTIAL_SECRET: <secret> Run Kubespray \u00b6 Copy the script for generating dynamic ansible inventories: for CLUSTER in $SERVICE_CLUSTER \" ${ WORKLOAD_CLUSTERS [@] } \" ; do cp kubespray/contrib/terraform/terraform.py \" ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/inventory.ini\" chmod +x \" ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/inventory.ini\" done Now it is time to run the Kubespray playbook! for CLUSTER in $SERVICE_CLUSTER \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ./bin/ck8s-kubespray apply \" ${ CLUSTER } \" done Test access to the Kubernetes API \u00b6 You should now have an encrypted kubeconfig file for each cluster under $CK8S_CONFIG_PATH/.state . Check that they work like this: for CLUSTER in $SERVICE_CLUSTER \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file \" ${ CK8S_CONFIG_PATH } /.state/kube_config_ ${ CLUSTER } .yaml\" \"kubectl --kubeconfig {} cluster-info\" done The output should be similar to this. Kubernetes control plane is running at https://<public-ip>:6443 To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. Kubernetes control plane is running at https://<public-ip>:6443 To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. Create the DNS Records \u00b6 You will need to setup a number of DNS entries for traffic to be routed correctly. Determine the public IP of one or more of the nodes in the service cluster. Then point these domains to the services. Since Safespring does not have a domain name service, use alternatives such as AWS Route53. Deploying Compliant Kubernetes Apps \u00b6 Now that the Kubernetes clusters are up and running, we are ready to install the Compliant Kubernetes apps. Clone compliantkubernetes-apps and Install Pre-requisites \u00b6 If you haven't done so already, clone the compliantkubernetes-apps repo and install pre-requisites. git clone https://github.com/elastisys/compliantkubernetes-apps.git cd compliantkubernetes-apps ansible-playbook -e 'ansible_python_interpreter=/usr/bin/python3' --ask-become-pass --connection local --inventory 127 .0.0.1, get-requirements.yaml Initialize the apps configuration \u00b6 export CK8S_ENVIRONMENT_NAME = my-environment-name #export CK8S_FLAVOR=[dev|prod] # defaults to dev export CK8S_CONFIG_PATH = ~/.ck8s/my-cluster-path export CK8S_CLOUD_PROVIDER = # [exoscale|safespring|citycloud|aws|baremetal] export CK8S_PGP_FP = <your GPG key fingerprint> # retrieve with gpg --list-secret-keys ./bin/ck8s init This will initialise the configuration in the ${CK8S_CONFIG_PATH} directory. Generating configuration files sc-config.yaml and wc-config.yaml , as well as secrets with randomly generated passwords in secrets.yaml . This will also generate read-only default configuration under the directory defaults/ which can be used as a guide for available and suggested options. ls -l $CK8S_CONFIG_PATH Configure the apps \u00b6 Edit the configuration files ${CK8S_CONFIG_PATH}/sc-config.yaml , ${CK8S_CONFIG_PATH}/wc-config.yaml and ${CK8S_CONFIG_PATH}/secrets.yaml and set the appropriate values for some of the configuration fields. Note that, the latter is encrypted. vim ${ CK8S_CONFIG_PATH } /sc-config.yaml vim ${ CK8S_CONFIG_PATH } /wc-config.yaml sops ${ CK8S_CONFIG_PATH } /secrets.yaml Tip The default configuration for the service cluster and workload cluster are available in the directory ${CK8S_CONFIG_PATH}/defaults/ and can be used as a reference for available options. Warning Do not modify the read-only default configurations files found in the directory ${CK8S_CONFIG_PATH}/defaults/ . Instead configure the cluster by modifying the regular files ${CK8S_CONFIG_PATH}/sc-config.yaml and ${CK8S_CONFIG_PATH}/wc-config.yaml as they will override the default options. The following are the minimum change you should perform: # ${CK8S_CONFIG_PATH}/sc-config.yaml and ${CK8S_CONFIG_PATH}/wc-config.yaml global : baseDomain : \"set-me\" # set to $CK8S_ENVIRONMENT_NAME.$DOMAIN opsDomain : \"set-me\" # set to ops.$CK8S_ENVIRONMENT_NAME.$DOMAIN issuer : letsencrypt-prod objectStorage : type : \"s3\" s3 : region : \"set-me\" # Region for S3 buckets, e.g, west-1 regionEndpoint : \"set-me\" # e.g., https://s3.us-west-1.amazonaws.com storageClasses : default : cinder-csi nfs : enabled : false cinder : enabled : true local : enabled : false ebs : enabled : false # ${CK8S_CONFIG_PATH}/sc-config.yaml (in addition to the changes above) ingressNginx : controller : service : type : \"this-is-not-used\" annotations : \"this-is-not-used\" harbor : oidc : groupClaimName : \"set-me\" # set to group claim name used by OIDC provider issuers : letsencrypt : prod : email : \"set-me\" # set this to an email to receive LetsEncrypt notifications staging : email : \"set-me\" # set this to an email to receive LetsEncrypt notifications Edit ${CK8S_CONFIG_PATH}/secrets.yaml with sops sops ${CK8S_CONFIG_PATH}/secrets.yaml # ${CK8S_CONFIG_PATH}/secrets.yaml objectStorage : s3 : accessKey : \"set-me\" # set to your s3 accesskey secretKey : \"set-me\" # set to your s3 secretKey Create S3 buckets \u00b6 Create object storage buckets for backups and container registry storage (if desired). For this you need to obtain access keys from Safespring to be able to create S3 buckets. cd compliantkubernetes-apps git checkout v0.17.0 AWS_ACCESS_KEY=set-me AWS_ACCESS_SECRET_KEY=set-me scripts/S3/generate-s3cfg.sh safespring ${AWS_ACCESS_KEY} ${AWS_ACCESS_SECRET_KEY} s3.sto2.safedc.net sto2 > ~/.s3cfg scripts/S3/entry.sh create Test S3 \u00b6 To ensure that you have configured S3 correctly, run the following snippet: ( access_key = $( sops exec-file ${ CK8S_CONFIG_PATH } /secrets.yaml 'yq r {} \"objectStorage.s3.accessKey\"' ) secret_key = $( sops exec-file ${ CK8S_CONFIG_PATH } /secrets.yaml 'yq r {} \"objectStorage.s3.secretKey\"' ) sc_config = $( yq m ${ CK8S_CONFIG_PATH } /defaults/sc-config.yaml ${ CK8S_CONFIG_PATH } /sc-config.yaml -a overwrite -x ) region = $( echo ${ sc_config } | yq r - 'objectStorage.s3.region' ) host = $( echo ${ sc_config } | yq r - 'objectStorage.s3.regionEndpoint' ) for bucket in $( echo ${ sc_config } | yq r - 'objectStorage.buckets.*' ) ; do s3cmd --access_key = ${ access_key } --secret_key = ${ secret_key } \\ --region = ${ region } --host = ${ host } \\ ls s3:// ${ bucket } > /dev/null [ ${ ? } = 0 ] && echo \"Bucket ${ bucket } exists!\" done ) Prepare for Compliant Kubernetes Apps \u00b6 To make the kubeconfig files work with Compliant Kubernetes Apps, you will need to rename or copy them, since Compliant Kubernetes Apps currently only support clusters named sc and wc . If you have multiple workload clusters, you can make this work by setting CK8S_CONFIG_PATH to each $CK8S_CONFIG_PATH/$CLUSTER-config in turn. I.e. CK8S_CONFIG_PATH will be different for compliantkubernetes-kubespray and compliantkubernetes-apps. # In compliantkubernetes-kubespray CK8S_CONFIG_PATH=~/.ck8s/<environment-name> # In compliantkubernetes-apps (one config path per workload cluster) CK8S_CONFIG_PATH=~/.ck8s/<environment-name>/<prefix>-config Copy the kubeconfig files to a path that Apps can find. Option 1 - A single workload cluster: cp \" ${ CK8S_CONFIG_PATH } /.state/kube_config_ ${ SERVICE_CLUSTER } .yaml\" \" ${ CK8S_CONFIG_PATH } /.state/kube_config_sc.yaml\" cp \" ${ CK8S_CONFIG_PATH } /.state/kube_config_ ${ WORKLOAD_CLUSTERS [@] } .yaml\" \" ${ CK8S_CONFIG_PATH } /.state/kube_config_wc.yaml\" You can now use the same CK8S_CONFIG_PATH for Apps as for compliantkubernetes-kubespray. Option 2 - A multiple workload cluster: mkdir -p \" ${ CK8S_CONFIG_PATH } / ${ SERVICE_CLUSTER } -config/.state\" cp \" ${ CK8S_CONFIG_PATH } /.state/kube_config_ ${ SERVICE_CLUSTER } .yaml\" \" ${ CK8S_CONFIG_PATH } / ${ SERVICE_CLUSTER } -config/.state/kube_config_sc.yaml\" for CLUSTER in \" ${ WORKLOAD_CLUSTERS [@] } \" ; do mkdir -p \" ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/.state\" cp \" ${ CK8S_CONFIG_PATH } /.state/kube_config_ ${ CLUSTERS } .yaml\" \" ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/.state/kube_config_wc.yaml\" done You will then need to set CK8S_CONFIG_PATH to each $CLUSTER-config folder in turn, in order to install Apps on the service cluster and workload clusters. With this you should be ready to install Compliant Kubernetes Apps on top of the clusters. Edit the storage class manifests \u00b6 ./bin/ck8s ops kubectl sc edit storageclass cinder-csi ./bin/ck8s ops kubectl wc edit storageclass cinder-csi Set storageclass.kubernetes.io/is-default-class: \"true\" Install Compliant Kubernetes Apps! \u00b6 Start with the service cluster: ln -sf $CK8S_CONFIG_PATH/.state/kube_config_${SERVICE_CLUSTER}.yaml $CK8S_CONFIG_PATH/.state/kube_config_sc.yaml ./bin/ck8s apply sc # Respond \"n\" if you get a WARN Then the workload clusters: for CLUSTER in \"${WORKLOAD_CLUSTERS[@]}\"; do ln -sf $CK8S_CONFIG_PATH/.state/kube_config_${CLUSTER}.yaml $CK8S_CONFIG_PATH/.state/kube_config_wc.yaml ./bin/ck8s apply wc # Respond \"n\" if you get a WARN done Settling \u00b6 Important Leave sufficient time for the system to settle, e.g., request TLS certificates from LetsEncrypt, perhaps as much as 20 minutes. You can check if the system settled as follows: for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get --all-namespaces pods' done Check the output of the command above. All Pods needs to be Running or Completed. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get --all-namespaces issuers,clusterissuers,certificates' done Check the output of the command above. All resources need to have the Ready column True. Testing \u00b6 After completing the installation step you can test if the apps are properly installed and ready using the commands below. Start with the service cluster: ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ SERVICE_CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_sc.yaml ./bin/ck8s test sc # Respond \"n\" if you get a WARN Then the workload clusters: for CLUSTER in \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_wc.yaml ./bin/ck8s test wc # Respond \"n\" if you get a WARN done Done. Navigate to the endpoints, for example grafana.$BASE_DOMAIN , kibana.$BASE_DOMAIN , harbor.$BASE_DOMAIN , etc. to discover Compliant Kubernetes's features. Removing Compliant Kubernetes Apps from your cluster \u00b6 To remove the applications added by compliant kubernetes you can use the two scripts clean-sc.sh and clean-wc.sh , they are located here in the scripts folder . They perform the following actions: Delete the added helm charts Delete the added namespaces Delete any remaining PersistentVolumes Delete the added CustomResourceDefinitions Note: if user namespaces are managed by Compliant Kubernetes apps then they will also be deleted if you clean up the workload cluster. Remove the infrastructure \u00b6 Destroy the infrastructure using Terraform, the same way you created it: cd compliantkubernetes-kubespray/ MODULE_PATH = \" $( pwd ) /kubespray/contrib/terraform/openstack\" pushd \" ${ MODULE_PATH } \" for CLUSTER in $SERVICE_CLUSTER \" ${ WORKLOAD_CLUSTERS [@] } \" ; do terraform init terraform destroy -var-file = \" ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/cluster.tfvars\" -state = \" ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/terraform.tfstate\" done popd Don't forget to remove any DNS records and object storage buckets that you may have created.","title":"On Safespring"},{"location":"operator-manual/troubleshooting/","text":"Troubleshooting Tools \u00b6 Help! Something is wrong with my Compliant Kubernetes cluster. Fear no more, this guide will help you make sense. This guide assumes that: You have pre-requisites installed. Your environment variables, in particular CK8S_CONFIG_PATH is set. Your config folder (e.g. for OpenStack ) is available. compliantkubernetes-apps and compliantkubernetes-kubespray is available. I have no clue where to start \u00b6 If you get lost, start checking from the \"physical layer\" and up. Are the Nodes still accessible via SSH? \u00b6 for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ansible -i ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/inventory.ini all -m ping done Are the Nodes \"doing fine\"? \u00b6 Dmesg should not display unexpected messages. OOM will show up here. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ansible -i ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/inventory.ini all -m shell -a 'echo; hostname; dmesg | tail -n 10' done Uptime should show high uptime (e.g., days) and low load (e.g., less than 3): for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ansible -i $CK8S_CONFIG_PATH / ${ CLUSTER } -config/inventory.ini all -m shell -a 'echo; hostname; uptime' done Any process that uses too much CPU? for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ansible -i $CK8S_CONFIG_PATH / ${ CLUSTER } -config/inventory.ini all -m shell -a 'echo; hostname; ps -Ao user,uid,comm,pid,pcpu,tty --sort=-pcpu | head -n 6' done Is there enough disk space? All writeable file-systems should have at least 30% free. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ansible -i $CK8S_CONFIG_PATH / ${ CLUSTER } -config/inventory.ini all -m shell -a 'echo; hostname; df -h' done Is there enough available memory? There should be at least a few GB of available memory. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ansible -i $CK8S_CONFIG_PATH / ${ CLUSTER } -config/inventory.ini all -m shell -a 'echo; hostname; cat /proc/meminfo | grep Available' done Can Nodes access the Internet? for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ansible -i $CK8S_CONFIG_PATH / ${ CLUSTER } -config/inventory.ini all -m shell -a 'echo; hostname; curl --silent https://checkip.amazonaws.com' done Are the Nodes having the proper time? You should see System clock synchronized: yes and NTP service: active . for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ansible -i $CK8S_CONFIG_PATH / ${ CLUSTER } -config/inventory.ini all -m shell -a 'echo; timedatectl status' done Is the base OS doing fine? \u00b6 We generally run the latest Ubuntu LTS , at the time of this writing Ubuntu 20.04 LTS. You can confirm this by doing: for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ansible -i $CK8S_CONFIG_PATH / ${ CLUSTER } -config/inventory.ini all -m shell -a 'cat /etc/lsb-release' done Are systemd units running fine? You should see running and not degraded . for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ansible -i $CK8S_CONFIG_PATH / ${ CLUSTER } -config/inventory.ini all -m shell -a 'systemctl is-system-running' done Are the Kubernetes clusters doing fine? \u00b6 Are the Nodes reporting in on Kubernetes? All Kubernetes Nodes, both control-plane and workers, should be Ready : for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get nodes' done Is Rook doing fine? \u00b6 If Rook is installed, is Rook doing fine? You should see HEALTH_OK . for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} -n rook-ceph apply -f ./compliantkubernetes-kubespray/rook/toolbox-deploy.yaml' done for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} -n rook-ceph exec deploy/rook-ceph-tools -- ceph status' done Are Kubernetes Pods doing fine? \u00b6 Pods should be Running or Completed , and fully Ready (e.g., 1/1 or 6/6 )? for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get --all-namespaces pods' done Are all Deployments fine? Deployments should show all Pods Ready, Up-to-date and Available (e.g., 2/2 2 2 ). for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get --all-namespaces deployments' done Are all DaemonSets fine? DaemonSets should show as many Pods Desired, Current, Ready and Up-to-date, as Desired. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get --all-namespaces ds' done Are Helm Releases fine? \u00b6 All Releases should be deployed . for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do export KUBECONFIG = kube_config_ $CLUSTER .yaml sops -d ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml > $KUBECONFIG helm list --all --all-namespaces shred $KUBECONFIG done Is cert-manager doing fine? \u00b6 Are (Cluster)Issuers fine? All Resources should be READY=True or valid . for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do export KUBECONFIG = kube_config_ $CLUSTER .yaml sops -d ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml > $KUBECONFIG kubectl get clusterissuers,issuers,certificates,orders,challenges --all-namespaces shred $KUBECONFIG done Where do I find the Nodes public and private IP? \u00b6 find . -name inventory.ini or for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ansible-inventory -i ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/inventory.ini --list all done ansible_host is usually the public IP, while ip is usually the private IP. Node cannot be access via SSH \u00b6 Important Make sure it is \"not you\". Are you well connected to the VPN? Is this the only Node which lost SSH access? Important If you are using Rook, it is usually set up with replication 2, which means it can tolerate one restarting Node. Make sure that, either Rook is healthy or that you are really sure you are restarting the right Node. Try connecting to the unhealthy Node via a different Node and internal IP: UNHEALTHY_NODE = 172 .0.10.205 # You lost access to this one JUMP_NODE = 89 .145.xxx.yyy # You have access to this one ssh -J ubuntu@ $JUMP_NODE ubuntu@ $UNHEALTHY_NODE Try rebooting the Node via cloud provider specific CLI: UNHEALTHY_NODE = cksc-worker-2 # Example for ExoScale exo vm reboot --force $UNHEALTHY_NODE If using Rook make sure its health goes back to HEALTH_OK . A Node has incorrect time \u00b6 Incorrect time on a Node can have sever consequences with replication and monitoring. In fact, if you follow ISO 27001, A.12.4.4 Clock Synchronisation requires you to ensure clocks are synchronized. These days, Linux distributions should come out-of-the-box with timesyncd for time synchronization via NTP. To figure out what is wrong, SSH into the target Node and try the following: sudo systemctl status systemd-timesyncd sudo journalctl --unit systemd-timesyncd sudo timedatectl status sudo timedatectl timesync-status sudo timedatectl show-timesync Possible causes include incorrect NTP server settings, or NTP being blocked by firewall. For reminder, NTP works over UDP port 123. Node seems not fine \u00b6 Important If you are using Rook, it is usually set up with replication 2, which means it can tolerate one restarting Node. Make sure that, either Rook is healthy or that you are really sure you are restarting the right Node. Try rebooting the Node: UNHEALTHY_NODE = 89 .145.xxx.yyy ssh ubuntu@ $UNHEALTHY_NODE sudo reboot If using Rook make sure its health goes back to HEALTH_OK . Node seems really not fine. I want a new one. \u00b6 Is it 2AM? Do not replace Nodes, instead simply add a new one. You might run out of capacity, you might lose redundancy, you might replace the wrong Node. Prefer to add a Node and see if that solves the problem. Okay, I want to add a new Node. \u00b6 Prefer this option if you \"quickly\" need to add CPU, memory or storage (i.e., Rook) capacity. First, check for infrastructure drift, as shown here . Depending on your provider: Add a new Node by editing the *.tfvars . Re-apply Terraform. Re-create the inventory.ini (skip this step if the cluster is using a dynamic inventory). Re-apply Kubespray. Re-fix the Kubernetes API URL. Check that the new Node joined the cluster, as shown here . A systemd unit failed \u00b6 SSH into the Node. Check which systemd unit is failing: systemctl --failed Gather more information: FAILED_UNIT = fwupd-refresh.service systemctl status $FAILED_UNIT journalctl --unit $FAILED_UNIT Rook seems not fine \u00b6 Please check the following upstream documents: Rook Common Issues Ceph Common Issues Pod seems not fine \u00b6 Before starting, set up a handy environment: CLUSTER = cksc # Cluster containing the unhealthy Pod export KUBECONFIG = kube_config_ $CLUSTER .yaml sops -d ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml > $KUBECONFIG Check that you are on the right cluster: kubectl get nodes Find the name of the Pod which is not fine: kubectl get pod -A # Copy-paste the Pod and Pod namespace below UNHEALTHY_POD = prometheus-kube-prometheus-stack-prometheus-0 UNHEALTHY_POD_NAMESPACE = monitoring Gather some \"evidence\" for later diagnostics, when the heat is over: kubectl describe pod -n $UNHEALTHY_POD_NAMESPACE $UNHEALTHY_POD kubectl logs -n $UNHEALTHY_POD_NAMESPACE $UNHEALTHY_POD Try to kill and check if the underlying Deployment, StatefulSet or DaemonSet will restart it: kubectl delete pod -n $UNHEALTHY_POD_NAMESPACE $UNHEALTHY_POD kubectl get pod -A --watch Helm Release is failed \u00b6 Before starting, set up a handy environment: CLUSTER = cksc # Cluster containing the failed Release export KUBECONFIG = kube_config_ $CLUSTER .yaml sops -d ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml > $KUBECONFIG Check that you are on the right cluster: kubectl get nodes Find the failed Release: helm ls --all-namespaces --all FAILED_RELEASE = user-rbac FAILED_RELEASE_NAMESPACE = kube-system Just to make sure, do a drift check, as shown here . Remove the failed Release: helm uninstall -n $FAILED_RELEASE_NAMESPACE $FAILED_RELEASE Re-apply apps according to documentation. cert-manager is not fine \u00b6 Follow cert-manager's troubleshooting, specifically: Troubleshooting Troubleshooting Issuing ACME Certificates Failed to perform self check: no such host \u00b6 If with kubectl describe challenges -A you get an error similar to below: Waiting for HTTP-01 challenge propagation: failed to perform self check GET request ''http://url/.well-known/acme-challenge/xVfDZoLlqs4tad2qOiCT4sjChNRausd5iNpbWuGm5ls'': Get \"http://url/.well-known/acme-challenge/xVfDZoLlqs4tad2qOiCT4sjChNRausd5iNpbWuGm5ls\": dial tcp: lookup opensearch.domain on 10.177.0.3:53: no such host' Then you might have a DNS issue inside your cluster. Make sure that global.clusterDns in common-config.yaml is set to the CoreDNS Service IP returned by kubectl get svc -n kube-system coredns . Failed to perform self check: connection timed out \u00b6 If with kubectl describe challenges -A you get an error similar to below: Reason: Waiting for http-01 challenge propagation: failed to perform self check GET request 'http://abc.com/.well-known/acme-challenge/Oej8tloD2wuHNBWS6eVhSKmGkZNfjLRemPmpJoHOPkA': Get \"http://abc.com/.well-known/acme-challenge/Oej8tloD2wuHNBWS6eVhSKmGkZNfjLRemPmpJoHOPkA\": dial tcp 18.192.17.98:80: connect: connection timed out Then your Kubernetes data plane Nodes cannot connect to themselves with the IP address of the load-balancer that fronts them. The easiest is to configure the load-balancer's IP address on the loopback interface of each Nodes. (See example here .) How do I check if infrastructure drifted due to manual intervention? \u00b6 Go to the docs of the cloud provider and run Terraform plan instead of apply . For Exoscale, it looks as follows: TF_SCRIPTS_DIR = $( readlink -f compliantkubernetes-kubespray/kubespray/contrib/terraform/exoscale ) for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do pushd ${ TF_SCRIPTS_DIR } export TF_VAR_inventory_file = ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/inventory.ini terraform init terraform plan \\ -var-file = ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/cluster.tfvars \\ -state = ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/terraform.tfstate popd done How do I check if the Kubespray setup drifted due to manual intervention? \u00b6 At the time of this writing, this cannot be done, but efforts are underway . How do I check if apps drifted due to manual intervention? \u00b6 # For service cluster ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ SERVICE_CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_sc.yaml ./compliantkubernetes-apps/bin/ck8s ops helmfile sc diff # Respond \"n\" if you get WARN # For the workload clusters for CLUSTER in \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_wc.yaml ./compliantkubernetes-apps/bin/ck8s ops helmfile wc diff # Respond \"n\" if you get WARN done Velero backup stuck in progress \u00b6 Velero is known to get stuck InProgress when doing backups velero backup get NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR velero-daily-backup-20211005143248 InProgress 0 0 2021 -10-05 14 :32:48 +0200 CEST 29d default !nobackup First try to delete the backup ./velero backup delete velero-daily-backup-20211005143248 Then kill all the pods under the velero namespace ./compliantkubernetes-apps/bin/ck8s ops kubectl wc delete pods -n velero --all Check that the backup is gone velero backup get NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR Recreate the backup from a schedule velero backup create --from-schedule velero-daily-backup","title":"Troubleshooting"},{"location":"operator-manual/user-alerts/","text":"Enabling user alerts \u00b6 This is administrator-facing documentation associated with this user guide . Please read that one first. Important Alertmanager should no longer be access via an Ingress, since this circumvents access control and audit logs. Please find the updated access method in the user guide linked above. Perform the following configuration changes in wc-config.yaml : Set user.alertmanager.enabled=true . Ensure user.alertmanager.ingress.enabled is false . For v0.18 and below include the following changes: Update the user.namespaces list to include alertmanager . Set user.alertmanager.namespace=alertmanager . Then apply ck8s-apps. Example \u00b6 Please find below an example taken from wc-config.yaml , which was tested with compliantkubernetes-apps v0.17.0 and also applies to version v0.18: user : ## This only controls if the namespaces should be created, user RBAC is always created. createNamespaces : true ## List of user namespaces to create. namespaces : - alertmanager - demo1 - demo2 - demo3 ## List of users to create RBAC rules for. adminUsers : - cristian.klein@elastisys.com - lars.larsson@elastisys.com - admin@example.com ## User controlled alertmanager configuration. alertmanager : enabled : true ## Namespace in which to install alertmanager namespace : alertmanager ## Create basic-auth protected ingress to alertmanager ingress : enabled : false Note For versions after v0.18 alertmanager may not be listed under user.namespaces and the option user.alertmanager.namespace is deprecated.","title":"User Alerts"},{"location":"release-notes/ck8s/","text":"Release Notes \u00b6 Compliant Kubernetes \u00b6 v0.25.0 - 2022-08-25 v0.24.1 - 2022-08-01 v0.24.0 - 2022-07-25 v0.23.0 - 2022-07-06 v0.22.0 - 2022-06-01 v0.21.0 - 2022-05-04 v0.20.0 - 2022-03-21 v0.19.1 - 2022-03-01 v0.19.0 - 2022-02-01 v0.18.2 - 2021-12-16 v0.17.2 - 2021-12-16 v0.18.1 - 2021-12-08 v0.17.1 - 2021-12-08 v0.18.0 - 2021-11-04 v0.17.0 - 2021-06-29 v0.16.0 - 2021-05-27 Note For a more detailed look check out the full changelog . Robin v4 \u00b6 iiiiiiii Robin v3 \u00b6 iiiii robin v2 \u00b6 robin v1 \u00b6 v0.25.0 \u00b6 Released 2022-08-25 Added \u00b6 Added Hierarchical Namespace Controller Allowing users to create and manage subnamespaces, namespaces within namespaces. You can read more about this in our FAQ . Added support for custom solvers in cluster issuers Allowing DNS01 challenges for certificate requests. Added support for running Harbor in High Availability Updated \u00b6 Updated cert-manager from v1.6.1 to v1.8.2 API versions v1alpha2 , v1alpha3 , and v1beta1 have been removed from the custom resource definitions (CRDs), certificate rotation policy will now be validated. See their changelog for more details. Updated OpenSearch with new usability improvements and features Checkout their launch announcement . Changed \u00b6 New additions to the Kubernetes cluster status Grafana dashboard It now shows information about resource requests and limits per node, and resource usage vs request per pod. v0.24.1 \u00b6 Released 2022-08-01 Required patch to be able to use release v0.24.0 Fixed \u00b6 Fixed a formatting issue with harbor s3 configuration. v0.24.0 \u00b6 Released 2022-07-25 Updated \u00b6 Upgraded Helm stack Upgrades for Helm, Helmfile and Helm-secrets. Image upgrade to node-local-dns Changed \u00b6 Improved stability to automatic node reboots Added \u00b6 Further configurability to ingress-nginx v0.23.0 \u00b6 Released 2022-07-06 Updated \u00b6 Updated the ingress controller ingress-nginx to image version v1.2.1 You can find the changelog here . Changed \u00b6 Added support for accessing Alertmanager via port-forward Added \u00b6 Backups can now be encrypted before they are replicated to an off-site S3 service. Improved metrics and alerting for OpenSearch. Fixed \u00b6 The deployment of Dex is now properly configured to be HA, ensuring that the Dex instances are placed on different Kubernetes worker nodes. v0.22.0 \u00b6 Released 2022-06-01 Added \u00b6 Added support for Elastx and UpCloud! New 'Welcoming' dashboard in OpenSearch and Grafana. Users can now access public docs and different urls to the services provided by Compliant Kubernetes. Improved availability of metrics and alerting. Alertmanager now runs with two replicas by default, Prometheus can now be run in HA mode. Added Falco rules to reduce alerts for services in Compliant Kubernetes. Falco now alerts less on operations that are expected out of these services. Fixed \u00b6 Fixed a bug where users couldn't silence alerts when portforwarding to alertmanager. Improved logging stack and fixed a number of issues to ensure reliability. v0.21.0 \u00b6 Released 2022-05-04 Changed \u00b6 Users can now view ClusterIssuers. User admins can now add users to the ClusterRole user-view. This is done by adding users to the ClusterRoleBinding extra-user-view . User can now get ClusterIssuers. Ensured all CISO dashboards are available to users. All the grafana dashboards in our CISO docs are now available. Better stability for dex Dex now runs with two replicas and has been updated. Updated \u00b6 Image upgrades to reduce number of vulnerabilities Upgrades for fluentd, grafana, and harbor chartmuseum. v0.20.0 \u00b6 Released 2022-03-21 Added \u00b6 Added kured - Kubernetes Reboot Daemon. This enables automatic node reboots and security patching of the underlying base Operating System image, container runtime and Kubernetes cluster components. Added fluentd grafana dashboard and alerts. Added RBAC for admin users. Admin users can now list pods cluster wide and run the kubectl top command. Added containerd support for fluentd. Changed \u00b6 Added the new OPA policy. To disallow the latest image tag. Persist Dex state in Kubernetes. This ensure the JWT token received from an OpenID provider is valid even after security patching of Kubernetes cluster components. Add ingressClassName in ingresses where that configuration option is available. Thanos is now enabled by default. Updated \u00b6 Upgraded nginx-ingress helm chart to v4.0.17 This upgrades nginx-ingress to v1.1.1. When upgrading an ingressClass object called nginx will be installed, this class has been set as the default class in Kubernetes. Ingress-nginx has been configured to still handle existing ingress objects that do not specify any ingressClassName. Upgraded starboard-operator helm chart to v0.9.1 This is upgrading starboard-operator to v0.14.1 Removed \u00b6 Removed influxDB and dependent helm charts. v0.19.1 \u00b6 Released 2022-03-01 Fixed \u00b6 Fixed critical stability issue related to Prometheus rules being evaluated without metrics. v0.19.0 \u00b6 Released 2022-02-01 Added \u00b6 Added Thanos as a new metrics backend. Provides a much more efficient and reliable platform for long-term metrics, with the capabilities to keep metrics for much longer time periods than previously possible. InfluxDB will still be supported in this release. Added a new feature to enable off-site replication of backups. Synchronizes S3 buckets across regions or clouds to keep an off-site backup. Added a new feature to create and log into separate indices per namespace. Currently considered to be an alpha feature. Changed \u00b6 Replacing Open Distro for Elasticsearch with OpenSearch. In this release, since the Open Distro project has reached end of life , Elasticsearch is replaced with OpenSearch and Kibana with OpenSearch Dashboards. OpenSearch is a fully open source fork of Elasticsearch with a compatible API and familiar User Experience. Note that recent versions of official Elasticsearch clients and tools will not work with OpenSearch as they employ a product check, compatible versions can be found here . Enforcing OPA policies by default. Provides strict safeguards by default. Allowing viewers to inspect and temporarily edit panels in Grafana. Gives more insight to the metrics and data shown. Setting Fluentd to log the reason why when it can't push logs to OpenSearch. Updated \u00b6 Large number of application and service updates, keeping up to date with new security fixes and changes. v0.18.2 \u00b6 Released 2021-12-16. Changes: Updated Open Distro for Elasticsearch to 1.13.3 to mitigate CVE-2021-44228 & CVE-2021-45046 v0.17.2 \u00b6 Released 2021-12-16. Changes: Updated Open Distro for Elasticsearch to 1.13.3 to mitigate CVE-2021-44228 & CVE-2021-45046 v0.18.1 \u00b6 Released 2021-12-08. Changes: updated Grafana to 8.0.7 in order to fix CVE-2021-43798 v0.17.1 \u00b6 Released 2021-12-08. Changes: updated Grafana to 8.0.7 in order to fix CVE-2021-43798 v0.18.0 \u00b6 Released 2021-11-04. Changes: Ingress-nginx-controller has been updated from v0.28.0 to v0.49.3, bringing various updates. Additionally, the configuration option allow-snippet-annotations has been set to false to mitigate known security issue CVE-2021-25742 Fixes, minor version upgrades, improvements to resource requests and limits for applications, improvements to stability. v0.17.0 \u00b6 Released 2021-06-29. Changes: The dashboard tool Grafana has been updated to a new major version of 8.x.x. This introduces new features and fixes, as well as some possibly breaking changes. See their release notes for more information. The single-sign-on service Dex has been updated, bringing small changes and better consistency to the UI. Fixes, improvements to resource limits, resource usage, and stability. v0.16.0 \u00b6 Released 2021-05-27. Changes: The default retention values have been changed and streamlined for authlog* and other* . The former will be kept for a longer period of time while the latter for shorter, both have reduced sized according to their actual usage. Updates, fixes, and features to improve the security of the platform.","title":"Compliant Kubernetes"},{"location":"release-notes/kubespray/","text":"Release Notes \u00b6 Compliant Kubernetes Kubespray \u00b6 v2.19.0-ck8s2 - 2022-07-22 v2.19.0-ck8s1 - 2022-06-27 v2.18.1-ck8s1 - 2022-04-26 v2.18.0-ck8s1 - 2022-02-18 v2.17.1-ck8s1 - 2021-11-11 v2.17.0-ck8s1 - 2021-10-21 v2.16.0-ck8s1 - 2021-07-02 v2.15.0-ck8s1 - 2021-05-27 Note For a more detailed look check out the full changelog . v2.19.0-ck8s2 \u00b6 Released 2022-07-22 Changes: Added option to clusteradmin kubeconfigs to use OIDC for authentication. New ansible playbooks to manage kubeconfigs and some RBAC. v2.19.0-ck8s1 \u00b6 Released 2022-06-27. Changes: Kubespray updated to v2.19.0 Kubernetes version upgrade to version 1.23.7 . v2.18.1-ck8s1 \u00b6 Released 2022-04-26. Changes: Kubespray updated to v2.18.1 This introduces some fixes for cluster using containerd as container manager. Changed default etcd version to 3.5.3 This fixes an issue where etcd data might get corrupted v2.18.0-ck8s1 \u00b6 Released 2022-02-18. Changes: Kubespray updated, including a new Kubernetes version upgrade to version 1.22.5 . This introduces new features and fixes, including security updates. There's also a lot of deprecated API's that were removed in this version so take a good look at these notes before upgrading. v2.17.1-ck8s1 \u00b6 Released 2021-11-11. Changes: Kubespray updated, including a new Kubernetes version upgrade to version 1.21.6 . This patch is mostly minor fixes. v2.17.0-ck8s1 \u00b6 Released 2021-10-21. Changes: Kubespray updated, including a new Kubernetes version upgrade to version 1.21.5 . This introduces new features and fixes, including security updates and storage capacity tracking. v2.16.0-ck8s1 \u00b6 Released 2021-07-02. Changes: Kubespray updated, including Kubernetes upgrade to version 1.20.7 . This introduces new features and fixes, including API and component updates. v2.15.0-ck8s1 \u00b6 Released 2021-05-27. First stable release!","title":"CK8S Kubespray"},{"location":"release-notes/postgres/","text":"Release Notes \u00b6 Compliant Kubernetes PostgreSQL \u00b6 v1.8.2-ck8s1 - 2022-08-24 v1.7.1-ck8s2 - 2022-04-26 v1.7.1-ck8s1 - 2021-12-21 Note These are only the user-facing changes. v1.8.2-ck8s1 \u00b6 Released 2022-08-24 Changes: Upgraded postgres-operator to version v1.8.2 Added a service which allows users to port-forward to the service instead of directly to pods v1.7.1-ck8s2 \u00b6 Released 2022-04-26 Changes: Fixed a vulnerability with logical backups v1.7.1-ck8s1 \u00b6 Released 2021-12-21 First stable release!","title":"CK8S PostgreSQL"},{"location":"release-notes/rabbitmq/","text":"Release Notes \u00b6 Compliant Kubernetes RabbitMQ \u00b6 v1.11.1-ck8s2 - 2022-06-08 v1.11.1-ck8s1 - 2022-03-11 v1.7.0-ck8s1 - 2021-12-23 Note These are only the user-facing changes. v1.11.1-ck8s2 \u00b6 Released 2022-06-08 Changes: Added a dashboard that shows metrics per queue v1.11.1-ck8s1 \u00b6 Released 2022-03-11 Changes: Upgraded rabbitmq-operator to version v1.11.1 v1.7.0-ck8s1 \u00b6 Released 2021-12-23 First stable release!","title":"CK8S RabbitMQ"},{"location":"release-notes/redis/","text":"Release Notes \u00b6 Compliant Kubernetes Redis \u00b6 v1.1.1-ck8s2 - 2022-08-23 v1.1.1-ck8s1 - 2022-03-07 v1.0.0-ck8s1 - 2021-12-23 Note These are only the user-facing changes. v1.1.1-ck8s2 \u00b6 Released 2022-08-23 Changes: Improved support for running multiple Redis clusters in one Kubernetes environment. v1.1.1-ck8s1 \u00b6 Released 2022-03-07 Changes: Upgraded redis-operator to v1.1.1 v1.0.0-ck8s1 \u00b6 Released 2021-12-23 First stable release!","title":"CK8S Redis"},{"location":"user-guide/","text":"The Journey for Application Developers \u00b6 Head over to Step 1: Prepare to get started on your journey!","title":"The Journey for Application Developers"},{"location":"user-guide/alerts/","tags":["ISO 27001 A.16"],"text":"Alerts \u00b6 Compliant Kubernetes (CK8S) includes alerts via Alertmanager . Important By default, you will get some platform alerts. This may benefit you, by giving you improved \"situational awareness\". Please decide if these alerts are of interest to you or not. Feel free to silence them, as the Compliant Kubernetes administrator will take responsibility for them. Your focus should be on user alerts or application-level alerts , i.e., alerts under the control and responsibility of the Compliant Kubernetes user. We will focus on user alerts in this document. Compliance needs \u00b6 Many regulations require you to have an incident management process. Alerts help you discover abnormal application behavior that need attention. This maps to ISO 27001 \u2013 Annex A.16: Information Security Incident Management . Enabling user alerts \u00b6 User alerts are handled by a project called AlertManager , which needs to be enabled by the administrator. Get in touch with the administrator and they will be happy to help. Configuring user alerts \u00b6 User alerts are configured via the Secret alertmanager-alertmanager located in the alertmanager namespace. This configuration file is specified here . # retrieve the old configuration kubectl get -n alertmanager secret alertmanager-alertmanager -o jsonpath = '{.data.alertmanager\\.yaml}' | base64 -d > alertmanager.yaml # edit alertmanager.yaml as needed # patch the new configuration kubectl patch -n alertmanager secret alertmanager-alertmanager -p \"'{\\\"data\\\":{\\\"alertmanager.yaml\\\":\\\" $( base64 -w 0 < alertmanager.yaml ) \\\"}}'\" Make sure to configure and test a receiver for you alerts, e.g., Slack or OpsGenie. Note If you get an access denied error, check with your Compliant Kubernetes administrator. Accessing user AlertManager \u00b6 If you want to access AlertManager, for example to confirm that its configuration was picked up correctly, proceed as follows: Type: kubectl proxy . Open this link in your browser. Setting up an alert \u00b6 Before setting up an alert, you must create a ServiceMonitor to collect metrics from your application. Then, create a PrometheusRule following the example below: apiVersion : monitoring.coreos.com/v1 kind : PrometheusRule metadata : creationTimestamp : null labels : prometheus : example role : alert-rules name : prometheus-example-rules spec : groups : - name : ./example.rules rules : - alert : ExampleAlert expr : vector(1) Running Example \u00b6 The user demo already includes a PrometheusRule , to configure an alert: {{ - if .Values.prometheusRule.enabled - }} apiVersion : monitoring.coreos.com/v1 kind : PrometheusRule metadata : name : {{ include \"ck8s-user-demo.fullname\" . }} labels : {{ - include \"ck8s-user-demo.labels\" . | nindent 4 }} spec : groups : - name : ./example.rules rules : - alert : ApplicationIsActuallyUsed expr : rate(http_request_duration_seconds_count[1m])>1 {{ - end }} The screenshot below gives an example of the application alert, as seen in AlertManager.","title":"Metric Alerts"},{"location":"user-guide/backup/","tags":["ISO 27001 A.12.3.1","BSI IT-Grundschutz APP.4.4.A5"],"text":"Backups \u00b6 Compliant Kubernetes (CK8S) includes backup functionality through Velero, a backup tool for Kubernetes Resources and Persistent Volumes. For backup of container images, Harbor is used instead. Compliance needs \u00b6 The requirements to comply with ISO 27001 are stated in ISO 27001:2013 . The annexes that are relevant to backups are: Annex 12 , article A.12.3.1 \"Information Backup\". What is Velero? \u00b6 Velero is an open source, cloud native tool for backing up and migrating Kubernetes Resources and Persistent Volumes. It has been developed by VMware since 2017. It allows for both manual and scheduled backups, and also allows for subsets of Resources in a cluster to be backed up rather than necessarily backing up everything. Usage \u00b6 The following are instructions for backing up and restoring resources. Backing up \u00b6 Compliant Kubernetes takes a daily backup of all Kubernetes Resources in all user namespaces. Persistent Volumes will be backed up if they are tied to a Pod. If backups are not wanted the label compliantkubernetes.io/nobackup can be added to opt-out of the daily backups. Application metrics (Grafana) and application log (Kibana) dashboards are also backup up by default. By default, backups are stored for 720 hours (30 days). Restoring \u00b6 Restoring from a backup with Velero is meant to be a type of disaster recovery. Velero will not overwrite existing Resources when restoring. As such, if you want to restore the state of a Resource that is still running, the Resource must be deleted first. To restore a backup on demand, contact your Compliant Kubernetes administrator. Protection of Backups \u00b6 The Compliant Kubernetes administrator will take the following measure to ensure backups are protected: Backups are encrypted at rest, if the underlying infrastructure provider supports it. Why? This ensures backups remain confidential, even if, e.g., hard drives are not safely disposed. Backups are replicated to an off-site location, if requested. This process is performed from outside the cluster, hence the users -- or attackers gaining access to their application -- cannot access the off-site replicas. Why? This ensures backups are available even if the primary location is subject to a disaster, such as extreme weather. The backups also remain available -- though unlikely confidential -- in case an attacker manages to gain access to the cluster.","title":"Backups"},{"location":"user-guide/ci-cd/","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10"],"text":"CI/CD Integration \u00b6 Compliant Kubernetes does not come with a CI/CD solution. Fortunately, it can be easily integrated with your existing CI/CD solution. Important Access control is an extremely important topic for passing an audit for compliance with data privacy and data security regulations. For example, Swedish patient data law requires all persons to be identified with individual credentials and that logs should capture who did what. Therefore, Compliant Kubernetes has put significant thought into how to do proper access control. As a consequence, CI/CD solutions that require cluster-wide permissions and/or introduce their own notion of access control are highly discouraged. Make sure you thoroughly evaluate your CI/CD solution with your CISO before investing in it. Background \u00b6 For the purpose of Compliant Kubernetes, one can distinguish between two \"styles\" of CI/CD: push-style and pull-style. Push-style CI/CD -- like GitLab CI or GitHub Actions -- means that a commit will trigger some commands on a CI/CD worker, which will push changes into the Compliant Kubernetes cluster. The CI/CD worker generally runs outside the Kubernetes cluster. Push-style CI/CD solutions should work out-of-the-box and require no special considerations for Compliant Kubernetes. Pull-styles CI/CD -- like ArgoCD or Flux -- means that a special controller is installed inside the cluster, which monitors a Git repository. When a change is detected the controller \"pulls\" changes into the cluster from the Git repository. The special controller often requires considerable permissions and introduces a new notion of access control, which is problematic from a compliance perspective. As shown below, some pull-style CI/CD solutions can be used with Compliant Kubernetes, others not. Push-style CI/CD \u00b6 Push-style CI/CD works pretty much as if you would access Compliant Kubernetes from your laptop, running kubectl or helm against the cluster, as required to deploy your application. However, for improved access control, the KUBECONFIG provided to your CI/CD pipeline should employ a ServiceAccount which is used only by your CI/CD pipeline. This ServiceAccount should be bound to a Role which gets the least permissions possible. For example, if your application only consists of a Deployment, Service and Ingress, those should be the only resources available to the Role. To create a KUBECONFIG for your CI/CD pipeline, proceed as shown below. Pre-verification \u00b6 First, make sure you are in the right namespace on the right cluster: kubectl get nodes kubectl config view --minify --output 'jsonpath={..namespace}' ; echo You can only create a Role which is as powerful as you (see Privilege escalation prevention ). Therefore, check what permissions you have and ensure they are sufficient for your CI/CD: kubectl auth can-i --list Note What permissions you need depends on your application. For example, the user demo creates Deployments, HorizontalPodAutoscalers, Ingresses, PrometheusRules, Services and ServiceMonitors. If unsure, simply continue. RBAC permissions errors are fairly actionable. Create a Role \u00b6 Next, create a Role for you CI/CD pipeline. If unsure, start from the example Role that the user demo's CI/CD pipeline needs. kubectl apply -f ci-cd-role.yaml Dealing with Forbidden or RBAC permissions errors Error from server (Forbidden): error when creating \"STDIN\": roles.rbac.authorization.k8s.io \"ci-cd\" is forbidden: user \"demo@example.com\" (groups=[\"system:authenticated\"]) is attempting to grant RBAC permissions not currently held: If you get an error like the one above, then it means you have insufficient permissions on the Compliant Kubernetes cluster. Contact your administrator. Create a ServiceAccount \u00b6 User accounts are for humans, service accounts for robots. See User accounts versus service accounts . Hence, you should employ a ServiceAccount for your CI/CD pipeline. The following command creates a ServiceAccount for your CI/CD pipeline: kubectl create serviceaccount ci-cd Create a RoleBinding \u00b6 Now create a RoleBinding to bind the CI/CD ServiceAccount to the Role, so as to grant it associated permissions: NAMESPACE = $( kubectl config view --minify --output 'jsonpath={..namespace}' ) kubectl create rolebinding ci-cd --role ci-cd --serviceaccount = $NAMESPACE :ci-cd Extract the KUBECONFIG \u00b6 You can now extract the KUBECONFIG of the ServiceAccount: SECRET_NAME = $( kubectl get sa ci-cd -o json | jq -r .secrets [] .name ) server = $( kubectl config view --minify --output 'jsonpath={..cluster.server}' ) cluster = $( kubectl config view --minify --output 'jsonpath={..context.cluster}' ) ca = $( kubectl get secret $SECRET_NAME -o jsonpath = '{.data.ca\\.crt}' ) token = $( kubectl get secret $SECRET_NAME -o jsonpath = '{.data.token}' | base64 --decode ) namespace = $( kubectl get secret $SECRET_NAME -o jsonpath = '{.data.namespace}' | base64 --decode ) echo \"\\ apiVersion: v1 kind: Config clusters: - name: ${ cluster } cluster: certificate-authority-data: ${ ca } server: ${ server } contexts: - name: default-context context: cluster: ${ cluster } namespace: ${ namespace } user: default-user current-context: default-context users: - name: default-user user: token: ${ token } \" > kubeconfig_ci_cd.yaml The generated kubeconfig_ci_cd.yaml can then be used in your CI/CD pipeline. Note that, KUBECONFIG s -- especially the token -- must be treated as a secret and injected into the CI/CD pipeline via a proper secrets handing feature, such as GitLab CI's protected variable and GitHub Action's secrets . Example: GitHub Actions \u00b6 Please find a concrete example for GitHub Actions here . Below is the produced output: ArgoCD \u00b6 By default, ArgoCD installs a ClusterRole with wide permissions , which can be used to bypass Compliant Kubernetes's access control. Using it as-is might be non-compliant with various regulations. Instead, edit the default ArgoCD manifest to create a very restricted Role that only operates in the target namespace. Flux v1 \u00b6 Flux v1 is in maintenance mode and might become obsolete soon. Flux v2 \u00b6 Flux v2 brings is own notion of access control and requires special considerations to ensure it obey Compliant Kubernetes access control. Installing it can only be done by the administrator of the Compliant Kubernetes cluster, after having made a thorough risk-reward analysis. At the time of this writing, due to these special considerations, we discourage Flux v2.","title":"CI/CD"},{"location":"user-guide/delegation/","tags":["ISO 27001 A.9.4.1","BSI IT-Grundschutz APP.4.4.A3"],"text":"How to Delegate? \u00b6 Note This section helps you implement ISO 27001, specifically: A.9.4.1 Information Access Restriction Now that you are almost ready to go live, you will certainly want to delegate some permissions to other team members or IT systems in your organization. This page shows you how to do that. Authentication vs. Access Control \u00b6 Authentication is the act of proving your identity. Compliant Kubernetes is usually configured to use your organization's Identity Provider (IdP). Examples of supported IdPs includes Google, Active Directory, Okta or Jump Cloud . The email and group provided by your IdP are used for access control in various components. Next sections will explain how to handle access control in each user-facing Compliant Kubernetes component. Container registry (Harbor) \u00b6 Compliant Kubernetes uses Harbor as container registry. For access control, Harbor defines the concepts of: user and group -- for human access; robot account -- for IT system access. You don't need to create Harbor users or groups. Compliant Kubernetes configures Harbor in \"OIDC authentication mode\", which means that Harbor will automatically onboard users logging in via your IdP and will automatically get the group from your IdP. In contrast, you need to create robot accounts, as these only exist within Harbor. Your administrator will have configured one of your IdP groups as the \"Harbor system administrator\" group. Please read the upstream documentation linked below to learn how a Harbor admin can: manage user permissions by role and create robot accounts . Note You can either add users or groups to a project with various roles. To simplify access control, consider only using groups and assigning users to groups from your IdP. Kubernetes API \u00b6 Kubernetes uses the following concepts for access control : users and groups -- these are provided by your IdP; ServiceAccounts -- these are configured within Kubernetes and are used by IT systems; Roles (and ClusterRoles) -- these define a set of permissions, i.e., allowed API operations; RoleBindings (and ClusterRoleBindings) -- these associate Roles, i.e., a set of permissions, with users, groups or ServiceAccounts. For delegating permissions to ServiceAccounts, follow the example on the CI/CD page . The next section present delegation to users and groups. Pre-verification \u00b6 First, make sure you are in the right namespace on the right cluster: kubectl get nodes kubectl config view --minify --output 'jsonpath={..namespace}' ; echo You can only delegate as much permission as you have (see Privilege escalation prevention ). Therefore, check what permissions you have: kubectl auth can-i --list Create a Role \u00b6 Next, create a Role capturing the set of permissions you want to delegate. If unsure, start from the example Role that the user demo's CI/CD pipeline needs. kubectl apply -f ci-cd-role.yaml Delegate to a Group \u00b6 Prefer delegating to a group, so that access control is centralized in your IdP. ROLE = my-role # Role created above GROUP = my-group # As set in your IdP kubectl create rolebinding $ROLE --role $ROLE --group = $GROUP --dry-run = client -o yaml > my-role-binding.yaml # review my-role-binding.yaml kubectl apply -f my-role-binding.yaml Add a User admin \u00b6 In Compliant Kubernetes v0.21.0 User admins can now add more User admins themselves. Edit the clusterrolebinding user-admin-cluster-wide-delegation and add the desired users or groups under subjects . If unsure, look at an example subject from the official kubernetes documentation. kubectl edit clusterrolebinding user-admin-cluster-wide-delegation Application Metrics (Grafana) \u00b6 Your administrator will have mapped your IdP groups to the Grafana viewer, editor and admin roles. Please read the upstream documentation to learn more. Application Logs (OpenSearch Dashboards) \u00b6 Note Compliant Kubernetes is currently migrating from OpenDistro for Elasticsearch to OpenSearch. As a result, Kibana will be replaced with OpenSearch Dashboards. You can track progress here . This section of the documentation will be updated once the migration is completed. TBD","title":"How to Delegate"},{"location":"user-guide/demarcation/","tags":["BSI IT-Grundschutz APP.4.4.A3"],"text":"Can I? \u00b6 Compliant Kubernetes comes with a lot of safeguards to ensure you protect your business reputation and earn the trust of your customers. Furthermore, it is a good idea to keep regulators happy, since they bring public trust into digitalization. Public trust is necessary to shift customers away from pen-and-paper to drive usage of your amazing application. If you used Kubernetes before, especially if you acted as a Kubernetes administrator, then being a Compliant Kubernetes user might feel a bit limiting. For example, you might not be able to run containers with root ( uid=0 ) as you were used to. Again, these are not limitations, rather safeguards. Why? \u00b6 As previously reported, Kubernetes is not secure by default, nor by itself . This is due to the fact that Kubernetes prefers to keep its \"wow, it just works\" experience. This might be fine for a company that does not process personal data. However, if you are in a regulated industry, for example, because you process personal data or health information, your regulators will be extremely unhappy to learn that your platform does not conform to security best practices. In case of Compliant Kubernetes this implies a clear separation of roles and responsibilities between Compliant Kubernetes users and administrators. The mission of administrators is to make you, the Compliant Kubernetes user, succeed. Besides allowing you to develop features as fast as possible, the administrator also needs to ensure that you build on top of a platform that lives up to regulatory requirements, specifically data privacy and data security regulations. General Principle \u00b6 Compliant Kubernetes does not allow users to make any changes which may compromise the security of the platform. This includes compromising or working around access control, logging, monitoring, backups, alerting, etc. For example, accidental deletion of the CustomResourceDefinitions of Prometheus would prevent administrators from getting alerts and fixing cluster issues before your application is impacted. Similarly, accidentally deleting fluentd Pods would make it impossible to capture the Kubernetes audit log and investigate data breaches. Specifics \u00b6 To stick to the general principles above, Compliant Kubernetes puts the following technical safeguards. This list may be updated in the future to take into account the fast evolving risk and technological landscape. More technically, Compliant Kubernetes does not allow users to: change the Kubernetes API through CustomResourceDefinitions or Dynamic Webhooks ; gain more container execution permissions by mutating PodSecurityPolicies ; this implies that you cannot run container images as root or mount hostPaths ; mutate ClusterRoles or Roles so as to escalate privileges ; mutate Kubernetes resources in administrator-owned namespaces, such as monitoring or kube-system ; re-configure system Pods, such as Prometheus or fluentd; access the hosts directly. But what if I really need to? \u00b6 Unfortunately, many application asks for more permissions than Compliant Kubernetes allows by default. When looking at the Kubernetes resources, the following are problematic: ClusterRoles, ClusterRoleBindings Too permissive Roles and RoleBindings PodSecurityPolicy and/or use of privileged PodSecurityPolicy CustomResourceDefinitions WebhookConfiguration In such a case, ask your administrator to make a risk-reward analysis. As long as they stick to the general principles, this should be fine. However, as much as they want to help, they might not be allowed to say \"yes\". Remember, administrators are there to help you focus on application development, but at the same time they are responsible to protect your application against security risks.","title":"Demarcation"},{"location":"user-guide/deploy/","text":"Step 2: Deploy \u00b6 Hello again, Application Developer! In this step, we will walk you through what is needed to deploy your application on Elastisys Compliant Kubernetes. Demo Application Available \u00b6 In case you are just reading along, or do not already have a containerized application prepared , we have developed a demo application which allows you to quickly explore the benfits of Elastisys Compliant Kubernetes. The provided artifacts, including Dockerfile and Helm Chart, allow you to quickly get started on your journey to become an agile organization with zero compromise on compliance with data protection regulations. We have versions of it for Node JS and .NET available. You will note that once built and containerized, they deploy exactly the same. Push Your Container Images \u00b6 Configure container registry credentials \u00b6 First, retrieve your Harbor CLI secret and configure your local Docker client. In your browser, type harbor.$DOMAIN where $DOMAIN is the information you retrieved from your administrator. Log into Harbor using Single Sign-On (SSO) via OpenID. In the right-top corner, click on your username, then \"User Profile\". Copy your CLI secret. Now log into the container registry: docker login harbor.$DOMAIN . You should see Login Succeeded . Create a registry project \u00b6 Example Here is an example Dockerfile and .dockerignore to get you started. Don't forget to run as non-root. If you haven't already done so, create a project called demo via the Harbor UI, which you have accessed in the previous step. Clone the user demo \u00b6 If you haven't done so already, clone the user demo: git clone https://github.com/elastisys/compliantkubernetes/ cd compliantkubernetes/user-demo Build and push the image \u00b6 REGISTRY_PROJECT = demo # Name of the project, created above TAG = v1 # Container image tag docker build -t harbor. $DOMAIN / $REGISTRY_PROJECT /ck8s-user-demo: $TAG . docker push harbor. $DOMAIN / $REGISTRY_PROJECT /ck8s-user-demo: $TAG You should see no error message. Note down the sha256 of the image. Verification \u00b6 Go to harbor.$DOMAIN . Choose the demo project. Check if the image was uploaded successfully, by comparing the tag's sha256 with the one returned by the docker push command above. (Optional) While you're at it, why not run the vulnerability scanner on the image you just pushed. Deploy Your Application \u00b6 Pre-verification \u00b6 Make sure you are in the right namespace on the right cluster: kubectl get nodes kubectl config view --minify --output 'jsonpath={..namespace}' ; echo Configure an Image Pull Secret \u00b6 To start, make sure you configure the Kubernetes cluster with an image pull secret. Ideally, you should create a container registry Robot Account , which only has pull permissions and use its token. Important Using your own registry credentials as an image pull secret, instead of creating a robot account, is against best practices and may violate data privacy regulations. Your registry credentials identify you and allow you to both push and pull images. A robot account should identify the Kubernetes cluster and be only allowed to pull images. DOCKER_USER = 'robot$name' # enter robot account name DOCKER_PASSWORD = # enter robot secret Now create a pull secret and (optionally) use it by default in the current namespace. # Create a pull secret kubectl create secret docker-registry pull-secret \\ --docker-server = harbor. $DOMAIN \\ --docker-username = $DOCKER_USER \\ --docker-password = $DOCKER_PASSWORD # Set default pull secret in current namespace kubectl patch serviceaccount default -p '{\"imagePullSecrets\": [{\"name\": \"pull-secret\"}]}' Note For each Kubernetes namespace, you will have to create an image pull secret and configure it to be default. Aim to have a one-to-one-to-one mapping between Kubernetes namespaces, container registry projects and robot accounts. Deploy user demo \u00b6 Example Here is an example Helm Chart to get you started. If you haven't done so already, clone the user demo and ensure you are in the right folder: git clone https://github.com/elastisys/compliantkubernetes/ cd compliantkubernetes/user-demo Ensure you use the right registry project and image tag, i.e., those that you pushed in the previous example : REGISTRY_PROJECT = demo TAG = v1 You are ready to deploy the application. helm upgrade \\ --install \\ myapp \\ deploy/ck8s-user-demo/ \\ --set image.repository = harbor. $DOMAIN / $REGISTRY_PROJECT /ck8s-user-demo \\ --set image.tag = $TAG \\ --set ingress.hostname = demo. $DOMAIN Verification \u00b6 Verify that the application was deployed successfully: kubectl get pods # Wait until the status of your Pod is Running. Verify that the certificate was issued successfully: kubectl get certificate # Wait until your certificate shows READY True. Verify that your application is online. You may use your browser or curl : curl --include https://demo. $DOMAIN # First line should be HTTP/2 200 Do not expose $DOMAIN to your users. Although your administrator will set *.$DOMAIN to point to your applications, prefer to buy a branded domain. For example, register the domain myapp.com and point it via a CNAME or ALIAS record to myapp.$DOMAIN . View Application Logs \u00b6 The user demo application already includes structured logging: For each HTTP request, it logs the URL, the user agent, etc. Compliant Kubernetes further adds the Pod name, Helm Chart name, Helm Release name, etc. to each log entry. The screenshot below gives an example of log entries produced by the user demo application. It was obtained by using the index pattern kubernetes* and the filter kubernetes.labels.app_kubernetes_io/instance:myapp . Note You may want to save frequently used searches as dashboards. Compliant Kubernetes saves and backs these up for you. Next step? Operating! \u00b6 Now that you have deployed your containerized application and know how to look at its logs, what's next? Head over to the next step, where you learn how to operate and monitor it!","title":"Step 2: Deploy"},{"location":"user-guide/faq/","text":"Application Developer FAQ \u00b6 Why can't I kubectl run ? \u00b6 To increase security, Compliance Kubernetes does not allow by default to run containers as root. Additionally, the container image is not allowed to be pulled from a public docker hub registry and all Pods are required to be selected by some NetworkPolicy. This ensures that an active decision has been made for what network access the Pod should have and helps avoid running \"obscure things found on the internet\". Considering the above, you should start by pushing the container image you want to use to Harbor and make sure it doesn't run as root . See this document for how to use OIDC with docker. With that in place, you will need to create a NetworkPolicy for the Pod you want to run. Here is an example of how to create a NetworkPolicy that allows all TCP traffic (in and out) for Pods with the label run: blah . Note This is just an example, not a good idea! You should limit the policy to whatever your application really needs. kubectl apply -f - <<EOF apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: blah spec: podSelector: matchLabels: run: blah policyTypes: - Ingress - Egress ingress: # Allow all incoming traffic - {} egress: # Allow all outgoing traffic - {} EOF Now you are ready to run a Pod! Make sure you match the name with the label you used for the NetworkPolicy. Kubectl will automatically set the label run: <name-of-pod> when you create a Pod with kubectl run <name-of-pod> . Here is an example command (please replace the $MY_HARBOR_IMAGE ): kubectl run blah --rm -ti --image = $MY_HARBOR_IMAGE If your image runs as root by defaults, but can handle running as another user, you may override the user by adding a flag like this to the above command: --overrides='{ \"spec\": { \"securityContext\": \"runAsUser\": 1000, \"runAsGroup\": 1000 } }' How do I give access to a new application developer to a Compliant Kubernetes environment? \u00b6 Add the new user to the correct group via your Identity Provider (IdP) , and Compliant Kubernetes will automatically pick it up. Feeling lost? To find out what users and groups currently have access to your Compliant Kubernetes environment, type: kubectl get rolebindings.rbac.authorization.k8s.io workload-admin -o yaml # look at the 'subjects' field If you are not using groups, contact your administrator. How do I add a new namespace? \u00b6 In Compliant Kubernetes v0.25.0 and later Hierarchical Namespace Controller (HNC) is included, and it allows the super application developer to manage namespaces as subnamespaces and delegates access automatically. From the perspective of Kubernetes these are regular namespaces, but these can be modified via a namespaced resource by the user. In Compliant Kubernetes versions prior to v0.25.0 HNC is not included, for these versions please ask your administrator for creating a new namespace. Note When a subnamespace is created all roles and rolebindings will propagate from the parent namespace to the descendant namespace to ensure that correct access is set. Propagated copies cannot be modified, these types of resources cannot be created in a parent namespace if it conflicts with a resource in a descendant namespace. To put an exception annotate the role or rolebinding with propagate.hnc.x-k8s.io/none: \"true\" to prevent if from being propagated at all. Or to only propagate to selected descendant namespaces use propagate.hnc.x-k8s.io/treeSelect: ... , include descendant namespaces with <descendant-namespace> or exclude namespaces with !<descendant-namespace> . Creating a subnamespace: kubectl apply -f - <<EOF apiVersion: hnc.x-k8s.io/v1alpha2 kind: SubnamespaceAnchor metadata: name: <descendant-namespace> namespace: <parent-namespace> EOF Verify that it gets created: kubectl get ns <descendant-namespace> Verify that it gets configured: $ kubectl get subns -n <parent-namespace> <descendant-namespace> -o yaml apiVersion: hnc.x-k8s.io/v1alpha2 kind: SubnamespaceAnchor metadata: ... name: <descendant-namespace> namespace: <parent-namespace> ... status: status: Ok If the status is Ok then the subnamespace is ready to go. Tip HNC also comes with the HNS kubectl plugin . Using this plugin creating subnamespaces is as easy as: kubectl hns create -n <parent-namespace> <descendant-namespace> And provides more detailed information using: kubectl hns describe <namespace> kubectl hns tree <namespace> For more information about how to use HNC see their user documentation . Why can't I access my cluster? 'Bad Request Unregistered redirect_uri (\"http://localhost:18000\").' \u00b6 Port 8000 is the only allowed port for OpenID callback URL and is needed by the kubectl OpenID plugin. If that port is used locally, then kubectl will try to bind to port 18000 which is not allowed due to security concerns. Make sure that nothing is running locally that is using port 8000. Why can't I access Kibana or Elasticsearch? \u00b6 In Compliant Kubernetes v0.19.0 or later Elasticsearch and Kibana is no used and has been replaced by Opensearch and Opensearch Dashboards respectively. These can be accessed with domain prefix opensearch . What is encrypted at rest? \u00b6 Compliant Kubernetes encrypts everything at rest, including Kubernetes resources, PersistentVolumeClaims, logs, metrics and backups, if the underlying cloud provider supports it . Get in touch with your administrator to check the status. They are responsible for performing a provider audit . Why does Compliant Kubernetes not offer encryption-at-rest at the platform level? TL;DR : operational scalability and to avoid security theatre . We are frequently asked why we don't simply do full-disk encryption at the VM level, using something like cryptsetup . Let us explain our rationale. The reason why people want encryption-at-rest is to add another safeguard to data confidentiality. Encryption-at-rest is a must-have for laptops, as they can easily be stolen or get lost. However, it is a nice-to-have addition for servers, which are supposed to be in a physically protected data-center, with disks being safely disposed. This is verified during a provider audit . At any rate, if encryption-at-rest is deployed it must: (a) actually safeguard data confidentiality; (b) without prohibitive costs in terms of administration. A Compliant Kubernetes environment may comprise as many as 10 Nodes, i.e., VMs. These Nodes need to be frequently rebooted, to ensure Operating System (OS) security patches are applied. This is especially important for Linux kernel, container runtime (Docker) and Kubernetes security patches. Thanks to the power of Kubernetes, a carefully engineered and deployed application can tolerate such reboots with zero downtime. (See the go-live checklist .) The challenge is how to deliver the disk encryption key to the VM when they are booting. Let us explore a few options: Non-option 1: Store the encryption key on the VM's /boot disk. This is obvious security theatre. For example, if server disks are stolen, the VM's data is in the hands of the thiefs. Non-option 2: Let admins type the encryption key on the VM's console. Asking admins to do this is time-consuming, error-prone, effectivly jeopardizing uptime. Instead, Compliant Kubernetes recommends automatic VM reboots during application \"quiet times\", such as at night, to ensure the OS is patched without sacrificing uptime. Non-option 3: Let the VM pull the encryption key via instance metadata or instance configuration . This would imply storing the encryption key on the cloud provider. If the cloud provider doesn't have encryption-at-rest, then the encryption key is also stored unencrypted, likely on the same server as the VM is running. Hence, this quickly ends up being security theatre. Non-option 4: Let the VM pull the encryption key from an external location which features encryption-at-rest. This would imply that the VM needs some kind of credentials to authenticate to the external location. Again these credentials are stored unencrypted on the cloud provider, so we are back to non-option 3. Okay, so what is the real option, then? The only real option is to rely on support from the cloud provider. The latest generation (physical) servers feature a TPM to store the disk encryption key. This can be securely release to the Linux kernel thanks to pre-boot authentication . This process is performance-neutral and fully transparent to the VMs running on top of the servers. And that is why Compliant Kubernetes encrypts everything at rest, only if the underlying cloud provider supports it .","title":"FAQ"},{"location":"user-guide/go-live/","text":"Go-live Checklist \u00b6 The administrator set up a shiny new Compliant Kubernetes environment. You containerized your application, deployed it, configured a working CI/CD pipeline, configured application alerts, etc. etc. All seems fine, but somehow you feel anxious about going into production 24/7. To move from production anxiety to production karma, here is a checklist to go through before going live 24/7. Make sure to perform this checklist in a shared session with the administrator. Load testing was performed. Why? This ensures that enough capacity was allocated for the environment. How? Set up a synthetic workload generator or replay a relevant workload. Ask the administrator to monitor your environment's capacity usage, including that related to components necessary for application logs and application metrics. Desired outcome : Allocated capacity is sufficient. Possible resolution : Ensure the application has proper resource requests and limits (see our user demo as an example ). Load testing was performed while updating the application. Why? This ensures that the application can be updated without downtime. How? Make a trivial change to your application, e.g., add \"Lorem ipsum\" in the output of some API, and redeploy. Desired outcome : Measured downtime is acceptable. Possible resolutions : Make sure you have the right deployment strategy . Prefer RollingUpdate over Recreate . Ensure other parameters of the deployment strategy are tuned as needed. Load testing was performed while doing a rolling reboot of Nodes: Why? Node failure may cause application downtime. Said downtime can be large if it happens at night, when administrators need to wake up before they can respond. Also, administrators need some extra capacity for performing critical security updates on the base operating system of the Nodes. How? As above, but now ask the administrator to perform a rolling reboot of Nodes. Desired outcome : The measured downtime (due to Pod migration) during Node failure or drain is acceptable. Capacity is sufficient to tolerate one Node failure or drain. Possible resolution : Ensure the application has proper resource requests and limits (see our user demo for an example ). Ensure the application has at least two replicas (see our user demo for an example ). Ensure the application has topologySpreadConstraints to ensure Pods do not end up on the same Node (see our user demo for an example ). [For multi-Zone environments] Load testing was performed while failing an entire Zone: Why? If a multi-Zone environment was requested, then the additional resilience must be tested. Otherwise, Zone failure may cause application downtime. How? As above, but now ask the administrator to fail an entire Zone. Desired outcome : The measured downtime (due to Pod migration) during Zode failure is acceptable. Capacity is sufficient to tolerate one Zode failure. Possible resolution : Ensure the application has proper resource requests and limits (see our user demo for an example ). Ensure the application has at least two replicas (see our user demo for an example ). Ensure the application has topologySpreadConstraints to ensure Pods do not end up on the same Zone (see our user demo for an example ). Disaster recovery testing was performed: Why? This ensures that the application and platform team agreed on who backs up what, instead of ending up thinking that \"backing up this thingy\" is the other team's problem. How? Ask the administrator to destroy the environment and restore from off-site backups. Check if your application is back up and its data is restored as expected. Desired outcome : Measured recovery point and recovery time is acceptable. Possible resolution : Ensure you store application either in PersistentVolumes -- these are backed up by default in Compliant Kubernetes -- or a managed database hosted inside Compliant Kubernetes. Redeployment of the application from scratch works. Why? This ensures that no tribal knowledge exists and your Git repository is truly the only source of truth. How? Ask your administrator to \"reset\" the environment, i.e., remove all container images, remove all cached container images, remove all Kubernetes resources, etc. Redeploy your application. Desired outcome : Measured setup time is acceptable. Possible resolutions : Make sure to add all code and Kubernetes manifests to your Git repository. Make sure that relevant documentation exists.","title":"Go-live Checklist"},{"location":"user-guide/kubernetes-api/","text":"Kubernetes API \u00b6 The Kubernetes API is the entrypoint to managing your Kubernetes resources. Your Compliant Kubernetes administrator will provide you with a kubeconfig file upon onboarding, which is required to access the API. The following sections describe how to access the cluster in order to manage your Kubernetes resources. Authentication and Access Control in Compliant Kubernetes \u00b6 In order to facilitate access control and audit logging, Compliant Kubernetes imposes a certain way to access the Kubernetes API. The kubeconfig file provides individual access to the Kubernetes API through dex . Normally, you should authenticate using your organizations identity provider connected to dex, but it is also possible for your administrator to configure static usernames and passwords. The authorization is done by the Kubernetes API based on Kubernetes role-based access controls . Your cluster administrator will grant you permissions as part of onboarding. You have administrator access to the user workload Kubernetes Namespaces by default. In order to follow the principle of least privilege , you as an user should only have sufficient access to manage resources required by your application. User access to the Kubernetes API may need to be restricted from case to case to follow the principle of least privilege. Note Regardless of your privilege, you will not be able to see components such as Harbor and Elasticsearch via the Kubernetes API. This is in order to comply with common logging policies, which requires logging to be sent to a tamper-proof environment. The tamper-proof environment needs to be separated from the production cluster. Usage guide \u00b6 This section focuses on using the kubeconfig. Using the kubeconfig file \u00b6 The kubeconfig file can be used with kubectl by: Setting and exporting the KUBECONFIG environment variable: Merging the configuration with your existing kubeconfig file, see Kubernetes documentation on merging kubeconfig files . Authenticating to the Kubernetes API \u00b6 To authenticate to the Kubernetes API, run a kubectl command. The oidc-login plugin will launch a browser where you log in to the cluster: This page contains the authentication options provided by your administrator. Select your log in method and log in: Once you have logged in through the browser, you are authenticated to the cluster: Your credentials will then be used by the Kubernetes API to make sure you are authorized. You are now logged in and can use kubectl to manage your Kubernetes resources! Running Example \u00b6 Pre-verification \u00b6 Make sure you are in the right namespace on the right cluster: kubectl get nodes kubectl config view --minify --output 'jsonpath={..namespace}' ; echo Configure an Image Pull Secret \u00b6 To start, make sure you configure the Kubernetes cluster with an image pull secret. Ideally, you should create a container registry Robot Account , which only has pull permissions and use its token. Important Using your own registry credentials as an image pull secret, instead of creating a robot account, is against best practices and may violate data privacy regulations. Your registry credentials identify you and allow you to both push and pull images. A robot account should identify the Kubernetes cluster and be only allowed to pull images. DOCKER_USER = 'robot$name' # enter robot account name DOCKER_PASSWORD = # enter robot secret Now create a pull secret and (optionally) use it by default in the current namespace. # Create a pull secret kubectl create secret docker-registry pull-secret \\ --docker-server = harbor. $DOMAIN \\ --docker-username = $DOCKER_USER \\ --docker-password = $DOCKER_PASSWORD # Set default pull secret in current namespace kubectl patch serviceaccount default -p '{\"imagePullSecrets\": [{\"name\": \"pull-secret\"}]}' Note For each Kubernetes namespace, you will have to create an image pull secret and configure it to be default. Aim to have a one-to-one-to-one mapping between Kubernetes namespaces, container registry projects and robot accounts. Deploy user demo \u00b6 Example Here is an example Helm Chart to get you started. If you haven't done so already, clone the user demo and ensure you are in the right folder: git clone https://github.com/elastisys/compliantkubernetes/ cd compliantkubernetes/user-demo Ensure you use the right registry project and image tag, i.e., those that you pushed in the previous example : REGISTRY_PROJECT = demo TAG = v1 You are ready to deploy the application. helm upgrade \\ --install \\ myapp \\ deploy/ck8s-user-demo/ \\ --set image.repository = harbor. $DOMAIN / $REGISTRY_PROJECT /ck8s-user-demo \\ --set image.tag = $TAG \\ --set ingress.hostname = demo. $DOMAIN Verification \u00b6 Verify that the application was deployed successfully: kubectl get pods # Wait until the status of your Pod is Running. Verify that the certificate was issued successfully: kubectl get certificate # Wait until your certificate shows READY True. Verify that your application is online. You may use your browser or curl : curl --include https://demo. $DOMAIN # First line should be HTTP/2 200 Do not expose $DOMAIN to your users. Although your administrator will set *.$DOMAIN to point to your applications, prefer to buy a branded domain. For example, register the domain myapp.com and point it via a CNAME or ALIAS record to myapp.$DOMAIN . Use topologySpreadConstraints if you want cross-data-center resilience If you want your application to tolerate a whole zone (data-center) to go down, you need to add topologySpreadConstraints by uncommenting the relevant section in values.yaml . In order for this to work, your administrator must configure the Nodes with zone labels. You can verify if this was performed correctly typing kubectl get nodes --show-labels and checking if Nodes feature the topology.kubernetes.io/zone label. Further reading \u00b6 dex on GitHub oidc-login/kubelogin on GitHub Organizing Cluster Access Using kubeconfig Files","title":"Kubernetes API"},{"location":"user-guide/kubernetes-ui/","text":"Kubernetes UI (Lens) \u00b6 Lens is a graphical user interface that you install locally on your machine. It provides an attractive and easy to use way of interacting with your Kubernetes cluster. Similar to kubectl , it can be used to manage multiple clusters. You can also install extensions that help you manage your clusters or applications deployed on them from a growing list of community-provided extensions. Because Lens is installed locally, and has no cluster-side component, it uses the exact same permissions as your Compliant Kubernetes user has. This makes it a perfectly safe and secure user interface to use and does not compromise your cluster's stability or security posture. Installing Lens \u00b6 Head over to the official Lens website and download the appropriate installation package for your operating system. Follow the instructions, and you should have Lens installed. On Debian or Ubuntu-based systems, once you've downloaded the .deb file for your architecture, you simply run the following command (assuming you called the downloaded file lens.deb ): sudo dpkg -i lens.deb Note for macOS and Linux users \u00b6 If you followed the Install Prerequisites steps of this documentation, you have probably installed the oidc-login plugin to kubectl via krew . If so, Lens will not be able to find it. That makes Lens fail to authenticate via Dex, the OpenID Connect provider in Compliant Kubernetes. You have two options for making the oidc-login plugin findable by Lens: Edit ~/.profile and add: if [ -d \"$HOME/.krew/bin\" ] ; then PATH=\"$HOME/.krew/bin:$PATH\" fi Run the following command: sudo ln -s ~/.krew/bin/kubectl-oidc_login /usr/local/bin Getting started with Lens \u00b6 Connecting to a cluster \u00b6 When Lens starts up, it will have located all the kubectl configuration files that are in your default directory ( $HOME/.kube ) and presents these in a list for you as clusters you can connect to. If you choose a cluster on this screen, you can then connect to it by hitting the big image for it above the \"Click to open\" text: Authentication with Dex does not happen within Lens itself, but it starts the correct component and will show the following screen: Go to the address it says, i.e., localhost:8000 to authenticate via Dex: Log in as usual with your identity provider and return to Lens. Hit the Reconnect button, and you're in! Cluster settings \u00b6 When you view your cluster's overview page, it comes up with an empty loading icon. That is because your user account does not have the permissions to view all Pods in all Namespaces. Lens assumes you do, but you do not. To remedy this, we can tell Lens which Namespaces to pull data from. Going back to the cluster selection screen, choose the Pen icon to configure the cluster's settings. You can then go to the setting called Namespaces and enter the names of the ones you have access to: Going back to the Overview of the cluster, you will now instead see information about it: Note that, for security reasons, monitoring metrics are not enabled, but you can read about that in the section below. Pod information and interaction \u00b6 In the Workload pane, you can see the Pods in your chosen Namespace: If you choose a Pod, you will see detailed information about that Pod, such as its labels, and other metadata. Again, note that for security reasons, the monitoring metrics part is empty. Among the things one can do with this overview, if the user has access to it, is to both manipulate the Pod as one would be able to do with kubectl edit , but also start an interactive shell session against it (like kubectl exec does): Optional permissions and steps required for Prometheus integration \u00b6 Lens can present monitoring information regarding your Pods if you have the appropriate permissions. To get the monitoring data for the graphs, it needs to query the Prometheus component in Compliant Kubernetes. However, your cluster administrator may have forbidden this access for security reasons. Why Prometheus integration can be a security risk With the Prometheus access Lens requires, one is able to exfiltrate the credentials that Prometheus itself uses for writing to long-term storage of metrics in Compliant Kubernetes. These could be used to send in junk data and possibly also overwrite the metrics that the real Prometheus process has written. For more information about this issue and how it develops, Compliant Kubernetes is tracking the implications of in this GitHub issue . That said, if you self-administer your cluster, are aware of the potential risks, and still would like to have Prometheus integration, you can do as follows. Download the lens-user-role.yaml file, the contents of which is shown below: Apply it, as-is, to your cluster by running kubectl apply -f lens-user-role.yaml . Assign this role to your user or the group your user belongs to by downloading and modifying it so that it lists your user name (typically that will be your email address) or the group you want to give these permissions to. Do that by downloading the lens-user-rolebinding-example.yaml file and editing it. The file is shown below: Apply it to your cluster after making your changes to a copy of the template via, for instance, kubectl apply -f lens-user-rolebinding.yaml . Next, we must configure Lens to connect to the Prometheus instance correctly: The address to your Prometheus instance is: monitoring/kube-prometheus-stack-prometheus:9090 and you should choose the Prometheus Stack option in the dropdown. With these settings in place, you will have metrics integration working, showing e.g. output like this for the overview and Pod information:","title":"Kubernetes UI (Lens)"},{"location":"user-guide/log-based-alerts/","text":"OpenSearch Alert \u00b6 The alerting feature notifies you when data from one or more OpenSearch indices meets certain conditions. For example, you might want to notify a Slack channel if your application logs more than five HTTP 404 errors in one hour/minute, or you might want to page a developer if no new documents have been indexed in the past 20 minutes. Alerting features have been enabled by default in Elastisys Compliant Kubernetes as of version 0.19.X OpenSearch Alert Demo \u00b6 When you log into OpenSearch Dashboards , you will start at the home page as shown below. From here click \"Visualize & analyze\" to continue and you will be greeted with the options to go forward to either Dashboard or Discover . Opening the sidebar in the top left will also provide navigation to OpenSearch Dashboards features, and here Alerting can be found in the page shown below. Once you click Alerting , it will navigate to the below page. To use OpenSearch alerting feature, it involves two steps described below. Create Destination - A reusable location for the information that you want the monitor to send out after being triggered. Supported locations are Amazon Chime, Email, Slack, or custom webhook. Create Monitor - A job that runs on a defined schedule and queries OpenSearch indices. The results of these queries are then used as input for one or more triggers (Conditions that, if met, generate alerts). Step 1 - Create Destination Go to Destination & Create Destinations as shown in the below. Name - Name of the destination, for example \u201c user-demo-404-slack-notify \u201d Type - choose Slack or any other available types you want to use it. Webhook URL - If using Slack, paste the webhook URL. Please refer for more information slack-webhook Step 2 - Create Monitors Go to Monitors & Create Monitor as shown in the below. Monitor Name - Name of the monitor, for example \u201cuser-demo-404-error\u201d Select Per query monitor or Per bucket monitor . - For more information Monitor-types Frequency - How often to monitor, for instance, to check every 1 minute Data source Index where your logs are stored, for instance, \u201ckubernetes\u201d (per default, Compliant Kubernetes will store all application logs indices that match the \u201ckubernetes*\u201d index pattern) Time field should be set to \u201c@timestamp\u201d Query Metrics - optional Time range for the last - Time frame of data the plugin should monitor . Ex- 1 minute(s) Data filter - status-code is 404 Triggers Trigger name - Name of the trigger. Ex- \u201c404-error occurred >5 times in last 1 minute\u201d Severity level - Select the severity level range with 1 (Highest) & 5 (Lowest) Trigger condition - Select the condition according to your applications . Ex- IS ABOVE = 5 Actions - Create an action with name , destination and customized message notification accordingly. Finally click - Create button to complete the creation of the monitor. You can see the status of the monitor under Alerting> Monitors> user-demo-404-error as shown below. Test alert notification to Slack. \u00b6 Demo application deployed and users get 404 errors many times (5 is the condition set before) as shown below. We get the Slack notifications as shown below. Users can view the alert status under the Alerting tab as shown below and accordingly take the required action. Users can acknowledge the alerts under the Alerting tab as shown below. Alert state \u00b6 Active - The alert is ongoing and unacknowledged. Alerts remain in this state until you acknowledge them, delete the trigger associated with the alert, or delete the monitor entirely. Acknowledged - Someone has acknowledged the alert, but not fixed the root cause. Completed - The alert is no longer ongoing. Alerts enter this state after the corresponding trigger evaluates to false. Error - An error occurred while executing the trigger\u2014usually the result of a bad trigger or destination. Deleted - Someone deleted the monitor or trigger associated with this alert while the alert was ongoing. You can find the more information about OpenSearch alerting by following the link.","title":"Log-based Alerts"},{"location":"user-guide/logs/","tags":["ISO 27001 A.12.4.1","ISO 27001 A.12.4.3","ISO 27001 A.16"],"text":"Logging \u00b6 Compliant Kubernetes (CK8s) provides the mechanism to manage your cluster as well as the lifecycle of thousands of containerized applications deployed in the cluster. The resources managed by CK8s are expected to be highly distributed with dynamic behaviors. An instance of CK8s cluster environment involves several components with nodes that host hundreds of containers that are constantly being spun up and destroyed based on workloads. When dealing with a large pool of containerized applications and workloads in CK8s, it is imperative to be proactive with continuous monitoring and debugging information in order to observe what is going on the cluster. These information can be seen at the container, node, or cluster level. Logging as one of the three pillars of observability is a crucial element to manage and monitor services and infrastructure. It allows you to track debugging information at different levels of granularity. Compliance needs \u00b6 The requirements to comply with ISO 27001 are stated in ISO 27001:2013 . The annexes that mostly concerns logging are: Annex 12 , article A.12.4.1 \"Event Logging\" and A.12.4.3 \"Administrator and Operator Logs\". Annex 16 which deals with incident management. In Compliant Kubernetes, OpenSearch is separate from the production workload, hence it complies with A.12.4.2 \"Protection of Log Information\". The cloud provider should ensure that the clock of Kubernetes nodes is synchronized, hence complying with A.12.4.4 \"Clock Synchronisation\". OpenSearch \u00b6 Raw logs in CK8s are normalized, filtered, and processed by fluentd and shipped to OpenSearch for storage and analysis. OpenSearch is derived from the fully open source version of Elasticsearch called Open Distro for Elasticsearch . OpenSearch provides a powerful, easy-to-use event monitoring and alerting system, enabling you to monitor, search, visualize your data among other things. OpenSearch Dashboards is used as visualization and analysis interface for OpenSearch for all your logs. Note Compliant Kubernetes v0.18 and earlier used Open Distro for Elasticsearch, providing fully open source versions of Elasticsearch and Kibana. This project has now reached end of life and continues through OpenSearch, replacing Elasticsearch with OpenSearch and Kibana with OpenSearch Dashboards. Although a big change for the project, it still remains highly compatible and with minor differences in features and user experience. Visualization using OpenSearch Dashboards \u00b6 OpenSearch Dashboards is used as a data visualization and exploration tool for log time-series and aggregate analytics. It offers powerful and easy-to-use features such as histograms, line graphs, pie charts, heat maps, and built-in geospatial support. When you log into OpenSearch Dashboards, you will start at the home page as shown below. From here click \"Visualize & analyze\" to continue and you will be greeted with the options to go forward to either Dashboard or Discover . Opening the sidebar in the top left will also provide navigation to OpenSearch Dashboards features, and here Visualize can be found in addition to the two former two outlined in the page shown below. Since we are concerned with searching logs and their visualization, we will focus on these three features indicated by the red rectangle in the figure above. If you are interested to know more about the rest please visit the official OpenSearch Dashboards documentation . Before we dive in further, let us discuss the type of logs ingested into OpenSearch. Logs in CK8s cluster are filtered and indexed by fluentd into four categories. Application level logs Kubeaudit logs related to Kubernetes audits to provide security-relevant chronological set of records documenting the sequence of activities that have affected system by individual users, administrators or other components of the system. This is mostly related to the ISO 27001 requirement A.12.4.3 \"Administrator and Operator Logs\". Kubernetes logs that provide insight into CK8s resources such as Nodes, Pods, Containers, Deployments and ReplicaSets. This allows you to observe the interactions between those resources and see the effects that one action has on another. Generally, logs in the CK8s ecosystem can be divided into the cluster level (logs outputted by components such as the kubelet, the API server, the scheduler) and the application level (logs generated by pods and containers). This is mostly related to the ISO 27001 requirement A.12.4.3 \"Administrator and Operator Logs\". Platform level logs Authlog includes information about system authorization, along with user logins and the authentication mechanism that were used. Such as SSH access to the Nodes. This is mostly related to the ISO 27001 requirement A.12.4.3 \"Administrator and Operator Logs\". Others logs other than the above two are indexed and shipped to OpenSearch as others . These logs are collected from the Node's journald logging system. Note Users can only view the logs of kubernetes and kubeaudit . authlog and others are for Compliant Kubernetes administrators. Let us dive into it then. Data Visualization and Exploration \u00b6 As you can see in the figure above, data visualization and exploration in OpenSearch Dashboards has three components: Discover , Visualize and Dashboard . The following section describes each components using examples. Note These following examples were created for Open Distro for Elasticsearch and Kibana, however the user experience is the same when using OpenSearch Dashboards. Discover \u00b6 The Discover component in OpenSearch Dashboards is used for exploring, searching and filtering logs. Navigate to Discover as shown previously to access the features provided by it. The figure below shows partial view of the page that you will get under Discover . As you can see in the above figure, the kubeaudit index logs are loaded by default. If you want to explore logs from either of the other two log indices please select the right index under the dropdown menu marked log index category . To appreciate the searching and filtering capability, let us get data for the following question: Get all logs that were collected for the past 20 hours in host 172.16.0.3 where the responseStatus reason is notfound We can use different ways to find the answer for the question. Below is one possible solution. Write sourceIPs: 172.16.0.3 in the search textbox . Click Add Filter and select responseStatus.reason and is under field and Operator dropdown menus respectively. Finally, enter notfound under Value input box and click Save . The following figure shows the details. To enter the 20 hours, click part that is labelled Time in the Discover figure above, then enter 20 under the input box and select hours in the dropdown menu. Make sure that you are under Relative tab. Finally, click update . The following figure shows how to set the hours. Note that the data will be automatically updated as time passes to reflect the past 20 hours data from the current time. Once you are done, you will see a result similar to the following figure. Visualize \u00b6 The Visualize component in OpenSearch Dashboards is to create different visualizations. Let us create a couple of visualizations. To create visualizations: Open the sidebar and click Visualize under OpenSearch Dashboards. Click Create visualization link located on the top right side of the page. Select a visualization type, we will use Pie here. Choose the index name or saved query name, if any, under New Pie / Choose a source . We will use the Kubernetes index here. By default a pie chart with the total number of logs will be provided by OpenSearch Dashboards. Let us divide the pie chart based on the number of logs contributed by each namespace . To do that perform the following steps: Under Buckets click add then Split Slices . See the figure below. Under aggregation select Significant Terms terms. see the figure below. Select Kubernetes.namespace_name.keyword under field . See the figure below. The final result will look like the following figure. Please save the pie chart as we will use it later. Let us create a similar pie chart using host instead of namespace . The chart will look like the following figure. Dashboard \u00b6 The Dashboard component in OpenSearch Dashboards is used for organizing related visualizations together. Let us bring the two visualizations that we created above together in a single dashboard. To do that: Open the sidebar and click Dashboard under OpenSearch Dashboards. Click Create Dashboard link located on the top right side of the page. Click Add existing link located on the left side. Select the name of the two charts/visualizations that you created above. The figure below shows the dashboard generated from the above steps showing the two pie charts in a single page. Accessing Falco and OPA Logs \u00b6 To access Falco or OPA logs, go to the Discover panel and write Falco or OPA on the search textbox . Make sure that the Kubernetes log index category is selected. The figure below shows the search result for Falco logs. The figure below shows the search result for OPA logs. Handling Mapping Conflicts \u00b6 If you get the following error: Mapping conflict! A field is defined as several types (string, integer, etc) across the indices that match this pattern. You may still be able to use these conflict fields in parts of Kibana, but they will be unavailable for functions that require Kibana to know their type. Correcting this issue will require re-indexing your data. This means that your application has changed the type of a field in your structured logs. For example, say version A of your application logs the HTTP request path in request . Later, version B logs the HTTP request path in request.path and the HTTP verb in request.verb . Essentially, request has changed from string to dictionary. As a first step, review your application change management policy to reduce the chance of a log field changing type. Second, ask your administrator to re-index the affected indices. Note Re-indexing requires a lot of permissions, including creating and deleting indices, and changing Index templates. This may interfere with audit logs and compromise platform security . Therefore, to ensure platform security, re-indexing can only be performed by Compliant Kubernetes administrators. Running Example \u00b6 The user demo application already includes structured logging: For each HTTP request, it logs the URL, the user agent, etc. Compliant Kubernetes further adds the Pod name, Helm Chart name, Helm Release name, etc. to each log entry. The screenshot below gives an example of log entries produced by the user demo application. It was obtained by using the index pattern kubernetes* and the filter kubernetes.labels.app_kubernetes_io/instance:myapp . Note You may want to save frequently used searches as dashboards. Compliant Kubernetes saves and backs these up for you. Exporting logs \u00b6 At the moment the reporting feature in OpenSearch doesn't work so instead we recommend you to use elasticsearch-dump . Example of exporting the kubernetes-* index pattern to a folder opensearch-dump : docker pull elasticdump/elasticsearch-dump mkdir opensearch-dump # OpenSearch username and password # This will be handed out from your Compliant Kubernetes administrator OPENSEARCH_USERNAME = \"your-username\" OPENSEARCH_PASSWORD = \"your-password\" # Your domain that is used for your cluster. # This is the same as the one you are using for your other services (grafana, harbor, etc.) DOMAIN = \"your-domain\" docker run --rm -ti -v $( pwd ) /opensearch-dump:/tmp elasticdump/elasticsearch-dump \\ --input = \"http:// ${ OPENSEARCH_USERNAME } : ${ OPENSEARCH_PASSWORD } @opensearch.ops. ${ DOMAIN } /kubernetes-*\" \\ --type = data \\ --output = /tmp/opensearch-dump.json \\ --searchBody = '{\"query\":{......}}' For more examples and how to use the tool, read the documentation in the repo . Further Reading \u00b6 OpenSearch OpenSearch Dashboards Open Distro for Elasticsearch Kibana Open Distro for Elasticsearch \u2013 How Different Is It? Fluentd","title":"Logs"},{"location":"user-guide/maintenance/","text":"What to expect from maintenance \u00b6 Different kinds of maintenance \u00b6 Patching the underlying OS on the nodes Upgrading the Compliant Kubernetes application stack Upgrading Kubernetes What impact could these kinds of maintenance have on your application? \u00b6 Let's go through them one by one. Patching the OS on the nodes \u00b6 Some service disruption is expected here, the nodes need to reboot in order to install the OS upgrades/security patches. This should be done automatically by Kured in almost all cases going forward, luckily Kured can be scheduled to perform these upgrades during night-time or whenever application traffic is expected to be low. Thanks to Kured these upgrades are not usually a problem. Upgrading the Compliant Kubernetes application stack \u00b6 There is barely any downtime expected from upgrading the base Compliant Kubernetes application stack. This is because most of the components being upgraded are not intertwined with your application, the only exception being NGINX Ingress Controller, which is not commonly upgraded. If you have any other managed services from us such as PostgreSQL, Redis, RabbitMQ or TimescaleDB, these services might be upgraded during the application maintenance windows. Upgrading these services can cause some short service disruptions and making them temporarily unreachable for your application. Upgrading Kubernetes \u00b6 The most impactful type of maintenance are the Kubernetes upgrades, which means that the nodes need to be rebooted. Currently there is no automatic process of doing this upgrade, so it has to be done manually. We do these upgrades on scheduled maintenance windows during office hours . The way we handle the upgrades is that we drain and reboot all nodes, one at the time. If parts of your application is running on just one node, then service disruptions are to be expected and parts of the application may become unreachable for short periods during the maintenance window. The worst case would be if the nodes were near full on resources, then the pods may not be able to be scheduled on another node while getting rebooted. This would mean that the pods running on that node would need to wait for its node to be ready again before it can be scheduled, which could be minutes of downtime. Note that this is not just a problem for Compliant Kubernetes, the same process would need to be followed when upgrading a \"vanilla\" Kubernetes cluster. To minimize the impact on the application you should use two replicas for your application and also set up topologySpreadConstraints to make sure that the replicas do not get scheduled on the same node.","title":"Maintenance"},{"location":"user-guide/metrics/","tags":["ISO 27001 A.12.1.3","ISO 27001 A.16.1"],"text":"Metrics \u00b6 This guide gives an introduction to Prometheus and Grafana and where they fit in Compliant Kubernetes, in terms of reducing the compliance burden. Why Prometheus and Grafana? \u00b6 Prometheus is an open-source solution for monitoring and alerting. It works by collecting and processing metrics from the various services in the cluster. It is widely used, stable, and a CNCF member. It is relatively easy to write ServiceMonitors for any custom services to get monitoring data from them into Prometheus. Grafana is the most widely used technology for visualization of metrics and analytics. It supports a multitude of data sources and it is easy to create custom dashboards. Grafana is created by Grafana Labs, a CNCF Silver Member. Compliance needs \u00b6 The requirements to comply with ISO 27001 are stated in ISO 27001:2013 The annexes that mostly concerns monitoring and alerting are Annex 12 , article A.12.1.3 \"capacity management\", and Annex 16 which deals with incident management. Capacity management \u00b6 Article A.12.1.3 states that \"The use of resources must be monitored, tuned and projections made of future capacity requirements to ensure the required system performance to meet the business objectives.\" Promethus and Grafana helps with this as the resource usage, such as storage capacity, CPU, and network usage can be monitored. Using visualization in Grafana, projections can be made as to future capacity requirements. The article goes on to say that \"Capacity management also needs to be: Pro-active \u2013 for example, using capacity considerations as part of change management; Re-active \u2013 e.g. triggers and alerts for when capacity usage is reaching a critical point so that timely increases, temporary or permanent can be made.\" Prometheus has a rich alerting functionality, allowing you to set up alerts to warn if, for example, thresholds are exceeded or performance is degraded. Incident management \u00b6 Annex A.16.1 is about management of information security incidents, events and weaknesses. The objective in this Annex A area is to ensure a consistent and effective approach to the lifecycle of incidents, events and weaknesses. Incidents needs to be tracked, reported, and lessons learned from them to improve processes and reduce the possibility of similar incidents occurring in the future. Prometheus and Grafana can help with this by making it easier to: collect evidence as soon as possible after the occurrence. conduct an information security forensics analysis communicate the existence of the information security incident or any relevant details to the leadership. Prometheus and Grafana in Compliant Kubernetes \u00b6 Prometheus \u00b6 Compliant Kubernetes installs the prometheus-operator by default. The Prometheus Operator for Kubernetes provides easy monitoring definitions for Kubernetes services and deployment and management of Prometheus instances as it can create/configure/manage Prometheus clusters atop Kubernetes. The following CRDs are installed by default. crd apigroup kind used by description alertmanagers monitoring.coreos.com Alertmanager prometheus-alerts podmonitors monitoring.coreos.com PodMonitor customer-rbac prometheuses monitoring.coreos.com Prometheus prometheusrules monitoring.coreos.com PrometheusRule customer-rbac, elasticsearch servicemonitors monitoring.coreos.com ServiceMonitor customer-rbac, dex, grafana, kibana, elastisearch, influxdb thanosrulers monitoring.coreos.com ThanosRuler Accessing Prometheus \u00b6 The web interface is not exposed by default in Compliant Kubernetes. In order to access it, the most straight-forward way is to use port forwarding via the Kubernetes API . kubectl -- -n monitoring port-forward prometheus-prometheus-operator-prometheus-0 9090:9090 Depending on your Compliant Kubernetes settings, access to the Prometheus server might have been disabled by the administrator. Grafana \u00b6 Grafana can be accessed at the endpoint provided by the Compliant Kubernetes install scripts. If you have configured dex you can login with a connected account. Compliant Kubernetes deploys Grafana with a selection of dashboards by default. Dashboards are accessed by clicking the Dashboard icon (for squares) at the lefthand side of the grafana window and selecting Manage. Some examples of useful dashboards are listed below. Node health \u00b6 The Nodes dashboard (Nodes) gives a quick overview of the status (health) of a node in the cluster. By selecting an instance in the \"instance\" dropdown metrics for CPU, Load, Memory, Disk and Network I/O is showed for that node. The time frame can be changed either by using the time dropdown or selecting directly in the graphs. Pod health \u00b6 The Pods dashboard (Kubernetes/Compute resources/Pods) gives a quick overview of the status (health) of a pod in the cluster. By selecting a pod in the \"pod\" dropdown metrics for CPU, Memory, and Network I/O is showed for that node. The time frame can be changed either by using the time dropdown or selecting directly in the graphs. Running Example \u00b6 The user demo already includes a ServiceMonitor , as required for Compliant Kubernetes to collect metrics from its /metrics endpoint: {{ - if .Values.serviceMonitor.enabled - }} apiVersion : monitoring.coreos.com/v1 kind : ServiceMonitor metadata : name : {{ include \"ck8s-user-demo.fullname\" . }} labels : {{ - include \"ck8s-user-demo.labels\" . | nindent 4 }} spec : selector : matchLabels : {{ - include \"ck8s-user-demo.selectorLabels\" . | nindent 6 }} endpoints : - port : http {{ - end }} The screenshot below shows Grafana in \"Explore\" mode (the compass icon to the left) featuring the query rate(http_request_duration_seconds_count[1m]) . It shows the request rate for the user demo application for each path and status code. As can be seen in the graph, the /users endpoint is getting more traffic than the other endpoints. The \"Explore\" mode is great for developing queries and exploring the data set. If you want to save a query so you can refer back to it, you can create a Dashboard instead. Dashboards consist of multiple Panels, each of which, can display the results of running queries. Learn more about Grafana panels . Note You may want to save frequently used Dashboards. Compliant Kubernetes saves and backs these up for you. Further reading \u00b6 For more information please refer to the official Prometheus and Grafana documentation.","title":"Metrics"},{"location":"user-guide/network-model/","tags":["ISO 27001 A.10.1.2","ISO 27001 A.13.1.1","ISO 27001 A.13.1.2","ISO 27001 A.13.1.3","BSI IT-Grundschutz APP.4.4.A7","BSI IT-Grundschutz APP.4.4.A18"],"text":"Network Model \u00b6 Note This section helps you implement ISO 27001, specifically: A.10.1.2 Key Management A.13.1.1 Network Controls A.13.1.2 Security of Network Services A.13.1.3 Segregation in Networks The diagram above present a useful model when reasoning about networking in Compliant Kubernetes. Note This is just a model and not an architectural diagram. Under the hood, things are a lot more complicated. Private Network \u00b6 Your application Pods, as well as Pods of additional services , can communicate on a secure private network, via RFC1918 private IP addresses. It is analogous to a VPC in VM-based workloads. In Compliant Kubernetes, it is the responsibility of the administrator to ensure the in-cluster private network is secure and trusted, either by performing an infrastructure audit or deploying Pod-to-Pod encryption . You should use NetworkPolicies to segregate your Pods. This improves your security posture by reducing the blast radius in case parts of your application are under attack. Example Feel free to take inspiration from the user demo . More example recipes for Kubernetes Network Policies that you can just copy paste can be found here . Private DNS \u00b6 The private network also features a private DNS. A Service my-svc in the namespace my-namespace can be accessed from within the Kubernetes cluster as my-svc.my-namespace . IP addresses of Pods are not stable. For example, the rollout of a new container image creates new Pods, which will have new IP addresses. Therefore, you should always use private DNS names of Services to connect your application Pods, as well as to connect your application to additional services . Ingress \u00b6 Your application users should never ever access the private network directly. Instead external access is enabled by creating Ingress objects. Compliant Kubernetes already comes with cert-manager and is already configured with a ClusterIssuer. A secure ACME protocol is used to issue and rotate certificates using the LetsEncrypt public service. Assuming you configured a Service and a Deployment for you application, making application users access your application involves two steps: Create the right DNS CNAME record. Create the right Ingress resource. Running Example \u00b6 Let us assume you want to host your application behind the nicely branded domain demo.example.com . Proceed as follows: For step 1, create a DNS CNAME as follows: demo.example.com. 900 CNAME app.$DOMAIN. where $DOMAIN is the environment-specific variable you received from the administrator . The line above is presented in DNS Zone file format and is widely accepted by DNS providers. After configuration, make sure the DNS record is properly configured and propagaged, by typing: host -a demo.example.com. Important In the above examples, the domain name is fully qualified , i.e., it ends with a dot. Make sure your DNS provider does not mis-interpret it as a relative domain name. Otherwise, you risk creating a DNS record like demo.example.com.example.com which is rarely what you want. Important Be cautious when using CNAMEs and apex domains (e.g., example.com ). See here for a long discussion of potential problems and current workarounds. For step 2, create an Ingress object with the right metadata.annotations and spec.tls , as exemplified below: apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : myapp-ck8s-user-demo annotations : # To list your current ClusterIssuers, simply use 'kubectl get ClusterIssuers'. cert-manager.io/cluster-issuer : letsencrypt-prod kubernetes.io/ingress.class : nginx ## Uncomment the line below to implement source IP allowlisting. ## Blocklisted IPs will get HTTP 403. # nginx.ingress.kubernetes.io/whitelist-source-range: 98.128.193.2/32 ## Uncomment the lines below to get OAuth authentication ## You will also need to configure and install oauth2-proxy. ## For an example and more details, see https://github.com/elastisys/compliantkubernetes/blob/main/user-demo/deploy/oauth2-proxy.yaml # nginx.ingress.kubernetes.io/auth-url: \"https://$host/oauth2/auth\" # nginx.ingress.kubernetes.io/auth-signin: \"https://$host/oauth2/start?rd=$escaped_request_uri\" # nginx.ingress.kubernetes.io/auth-response-headers: \"authorization\" spec : rules : - host : \"demo.example.com\" http : paths : - path : / pathType : Prefix backend : service : name : myapp-ck8s-user-demo port : number : 3000 tls : - hosts : - \"demo.example.com\" secretName : demo.example.com-tls Example Feel free to take inspiration from the user demo . If you want to protect your Ingress with OAuth2-based authentication, check out oauth2-proxy . Important The DNS name in spec.rules[0].host and spec.tls[0].hosts[0] must be the same as the DNS entry used by your application users, in the example above demo.example.com . Otherwise, the application users will get a \"Your connection is not private\" error. Important Some load-balancers fronting Compliant Kubernetes do not preserve source IP. This makes source IP allowlisting unusable. To check if source IP is preserved, check the HTTP request headers received by your application, specifically x-forwarded-for and x-real-ip . The user demo logs all HTTP request headers, as shown in the screenshot below. Demarcation of Responsibilities \u00b6 You are responsible for: creating Pods (via Deployments), Service and Ingress; segregating the private network via NetworkPolicies; configuring Ingresses as required to enable HTTPS encryption. The user demo already showcases the above. The Compliant Kubernetes administrator is responsible for: ensuring cert-manager works and is configured correctly; ensuring ClusterIssuers exist and are configured correctly; ensure the private network is secure or trusted. Further Reading \u00b6 DNS for Services and Pods Ingress NetworkPolicies","title":"Network Model"},{"location":"user-guide/operate/","text":"Step 3: Operate \u00b6 Welcome to the third and final step, Application Developer! In this step, you will learn how to operate your application on Elastisys Compliant Kubernetes. Configure Dashboards and Alerts \u00b6 Monitor your Application \u00b6 To monitor your application, you will log in to your Grafana. Recall how to log in to your web portals from Step 1: Prepare . Grafana visually displays the monitoring data that Prometheus has collected on your behalf. A significant amount of metrics are already collected for you, out of the box, on Elastisys Compliant Kubernetes. This means you can visualize data about the cluster immediately. But Prometheus can also be instructed to collect specific metrics from your own application. Perhaps this is more useful to you than monitoring metrics that relate to cluster health (in particular if somebody else managed Elastisys Compliant Kubernetes for you). To instruct Promethus on how to do this, you create a ServiceMonitor . This is a Kubernetes resource that configures Prometheus and specifies how to collect metrics from a particular application. The user demo already includes a ServiceMonitor , as required for Compliant Kubernetes to collect metrics from its /metrics endpoint: {{ - if .Values.serviceMonitor.enabled - }} apiVersion : monitoring.coreos.com/v1 kind : ServiceMonitor metadata : name : {{ include \"ck8s-user-demo.fullname\" . }} labels : {{ - include \"ck8s-user-demo.labels\" . | nindent 4 }} spec : selector : matchLabels : {{ - include \"ck8s-user-demo.selectorLabels\" . | nindent 6 }} endpoints : - port : http {{ - end }} The screenshot below shows Grafana in \"Explore\" mode (the compass icon to the left) featuring the query rate(http_request_duration_seconds_count[1m]) . It shows the request rate for the user demo application for each path and status code. As can be seen in the graph, the /users endpoint is getting more traffic than the other endpoints. The \"Explore\" mode is great for developing queries and exploring the data set. If you want to save a query so you can refer back to it, you can create a Dashboard instead. Dashboards consist of multiple Panels, each of which, can display the results of running queries. Learn more about Grafana panels . Note You may want to save frequently used Dashboards. Compliant Kubernetes saves and backs these up for you. Go deeper into metrics . Alert on Application Metrics \u00b6 Visualizing monitoring metrics is one thing. Sometimes, you may need to act on what they show, immediately. For that reason, the Prometheus monitoring system includes AlertManager. Prometheus is responsible for maintaining a set of Rules, which express trigger conditions via expressions. Once a rule has triggered, it has entered an alerting state. AlertManager is responsible for forwarding information about any rules in the alerting state to your chosen destination, which could be your company's Slack or similar. A number of integrations are available . If you wish to create rules based on application-specific monitoring metrics, you must first create appropriate ServiceMonitors as described above. The user demo already includes a PrometheusRule , to configure an alert: {{ - if .Values.prometheusRule.enabled - }} apiVersion : monitoring.coreos.com/v1 kind : PrometheusRule metadata : name : {{ include \"ck8s-user-demo.fullname\" . }} labels : {{ - include \"ck8s-user-demo.labels\" . | nindent 4 }} spec : groups : - name : ./example.rules rules : - alert : ApplicationIsActuallyUsed expr : rate(http_request_duration_seconds_count[1m])>1 {{ - end }} The screenshot below gives an example of the application alert, as seen in AlertManager. Go deeper into metric alerts . Alert on Log Contents \u00b6 Similar to alerting based on monitoring metrics, you may need to alert based on application log contents. For instance, it might make sense to send any log line of the FATAL log level to your Slack channel for immediate attention. The process of setting up log-based alerts is highly graphical, and supported by your OpenSearch Dashboards that is part of Elastisys Compliant Kubernetes. Recall how to log in to your web portals from Step 1: Prepare . Go deeper into log-based alerts . Test Backups and Capacity Management \u00b6 Disaster recovery is about so much more than backing up and restoring data. Backing up data is a necessary, but not sufficient, part of that. Not having sufficient capacity is also a kind of disaster, albeit, one that is easy to mitigate. Back up Application Data \u00b6 Compliant Kubernetes takes a daily backup of all Kubernetes Resources in all user namespaces. Persistent Volumes will be backed up if they are tied to a Pod. If backups are not wanted the label compliantkubernetes.io/nobackup can be added to opt-out of the daily backups. Application metrics (Grafana) and application log (Kibana) dashboards are also backup up by default. By default, backups are stored for 720 hours (30 days). Restoring from a backup with Velero is meant to be a type of disaster recovery. Velero will not overwrite existing Resources when restoring. As such, if you want to restore the state of a Resource that is still running, the Resource must be deleted first. To restore a backup on demand, contact your Compliant Kubernetes administrator. Go deeper into backups . Capacity Management \u00b6 Capacity management is about having sufficient capacity for your needs, be they in terms of storage or computational power. Your Elastisys Compliant Kubernetes administrator should perform capacity management of the platform , to ensure that there is a sufficient amount of spare capacity on a cluster level. As an application developer, you should perform capacity management on a Pod level. This primarily means setting resource requests correctly for containers inside Pods, making use of multiple instances in your Deployments and Stateful Sets (possibly via horizontal Pod autoscaling ). The use of resource requests and limits is enforced via an Elastisys Compliant Kubernetes safeguard . Automate with CI/CD \u00b6 Elastisys Compliant Kubernetes currently does not dictate or recommend any particular CI/CD solution over any other. It is, however, easy to integrate with various CI/CD solutions, such as GitHub Actions. The basic steps for a generic push-style CI/CD solution (such as GitHub Actions) are to: Create a limited Role , that has the least possible privileges required to deploy your application. Create a ServiceAccount and binding to the role created earlier via a RoleBinding , granting it the permissions needed for deploying the application. Getting the token for the ServiceAccount, so you can craft a KUBECONFIG to use with kubectl or helm in your CI/CD solution. Adding an in-cluster CI/CD solution is a work in progress, pending security reviews of alternatives in the ecosystem. Go deeper into CI/CD . Next step? Going deeper! \u00b6 By now, you're fully up and running! You have an application, updating it is a breeze, and you can monitor it and look at its logs. The next step is to open the \"Go deeper\" section of this documentation and read up on more topics that interest you. Thank you for starting your journey beyond the clouds with Elastisys Compliant Kubernetes!","title":"Step 3: Operate"},{"location":"user-guide/prepare-application/","tags":["ISO 27001 A.12.6.1","BSI IT-Grundschutz APP.4.4.A21"],"text":"Prepare Your Application \u00b6 To make the most out of Compliant Kubernetes, prepare your application so it features: some REST endpoints: NodeJS , .NET ; structured logging: NodeJS , .NET ; metrics endpoint: NodeJS , .NET ; Dockerfile, which showcases: How to run as non-root: NodeJS , .NET ; Helm Chart , which showcases: HTTPS Ingresses ; ServiceMonitor for metrics collection ; PrometheusRule for alerting ; topologySpreadConstraints for tolerating single Node or single Zone failure ; resources for capacity management ; NetworkPolicies for network segmentation ; Grafana dashboards for metrics visualization ; script for local development and testing ; Bonus: ability to make it crash ( /crash ). Feel free to clone our user demo for inspiration: git clone https://github.com/elastisys/compliantkubernetes/ cd compliantkubernetes/user-demo Make Sure Your Application Tolerates Nodes Replacement \u00b6 Important This section helps you implement ISO 27001, specifically: A.12.6.1 Management of Technical Vulnerabilities Compliant Kubernetes recommends against PodDisruptionBudgets (PDBs) . PDBs can easily be misconfigured to block draining Nodes, which interferes with automatic OS patching and compromises the security posture of the environment. Instead, prefer engineering your application to deal with disruptions. The user demo already showcases how to achieve this with replication and topologySpreadConstraints. Make sure to move state, even soft state, to specialized services . Further reading: Dealing with Disruptions","title":"Prepare Your Application"},{"location":"user-guide/prepare/","text":"Step 1: Prepare \u00b6 Hi there, Application Developer! Happy to have you on board with Elastisys Compliant Kubernetes! In this part, you will learn about the things you should do to prepare to get started with the platform. We assume somebody else, your administrator, has already set up the platform for you. You will therefore have received: URLs for the Elastisys Compliant Kubernetes UI components: OpenSearch Dashboards, Grafana, and Harbor; a kubeconfig file for configuring kubectl or Lens access to the underlying Kubernetes cluster; and (optionally and rarely) a static username and password. Note that normally, you should log in via a username and a password of your organization's Identity Provider, such as LDAP, Active Directory, or Google Workspaces account. Do you not already have an Elastisys Compliant Kubernetes platform up and running? Request one from a managed service provider and get started! Install Prerequisite Software \u00b6 Required software: oidc-login , which helps you log into your Kubernetes cluster via OpenID Connect integration with your Identity Provider of choice Your cluster management software of choice, of which you can choose either or both: kubectl , a command-line tool to help manage your Kubernetes resources Lens , a graphical user interface to help manage your Kubernetes resources (see also our dedicated page on Lens integration ) Optional, but very useful, tools for developers and DevOps engineers: docker , if you want to build (Docker) container images locally helm , if you want to manage your application with the Helm package manager Once installed, you can verify that configuration is correct by issuing a few simple commands . Access Your Web Portals \u00b6 Those URLs that your Elastisys Compliant Kubernetes administrator gave you all have a $DOMAIN , which will typically include your company name and perhaps the environment name. Your web portals are available at: harbor.$DOMAIN -- the Harbor container image registry, which will be the home to all your container images opensearch.$DOMAIN -- the OpenSearch Dashboards portal, where you will view your application and audit logs grafana.$DOMAIN -- the Grafana portal, where you will view your monitoring metrics for both the platform, as such, and your application-specific metrics Containerize Your Application \u00b6 Elastisys Compliant Kubernetes runs containerized applications in a Kubernetes platform. It is a Certified Kubernetes distribution, which means that if an application is possible to deploy on a standard Kubernetes environment, it can be deployed on Elastisys Compliant Kubernetes. However, there are some restrictions in place for security reasons. In particular, containers cannot be run as root . Following this best practice is a simple way to ensure additional security for your containerized applications deployed in Kubernetes. There are additional safeguards in place that reflect the security posture of Elastisys Compliant Kubernetes that impact your application. These prevent users from doing potentially unsafe things. In particular, users are not allowed to: change the Kubernetes API through CustomResourceDefinitions or Dynamic Webhooks ; gain more container execution permissions by mutating PodSecurityPolicies ; this implies that you cannot run container images as root or mount hostPaths ; mutate ClusterRoles or Roles so as to escalate privileges ; mutate Kubernetes resources in administrator-owned namespaces, such as monitoring or kube-system ; re-configure system Pods, such as Prometheus or fluentd; access the hosts directly. Next step? Deploying! \u00b6 Ready with a containerized application? Head over to the next step, where you learn how to deploy it!","title":"Step 1: Prepare"},{"location":"user-guide/registry/","text":"Harbor - private container registry \u00b6 This guide gives an introduction to Harbor and where it fits in Compliant Kubernetes, in terms of reducing the compliance burden. What is a container registry and why it is needed? \u00b6 A container registry is a system where you can store your container images in order to later use the images when you deploy your application (e.g. as a Pod in a Kubernetes cluster). The images need a permanent storage since they are used many times by different instances, especially in Kubernetes where Pods (which are using the images) are considered ephemeral, so it is not enough to just store images directly on nodes/virtual machines. There are many popular container registries available as services, e.g. Docker Hub and Google Container Registry. A common workflow with container registries is to build your images in a CI/CD pipeline, push the images to your registry, let the pipeline change your deployments that uses the images, and let the deployments pull down the new images from the repository. What is Harbor? \u00b6 Harbor is an open source container registry tool that allows you to host a registry privately. It also comes with some extra features such as vulnerability scanning and role based access control, this increases security and eases compliance with certain regulations. Harbor is also a CNCF Graduated project , proving that it is widely used and is well supported. Why is Harbor used in Compliant Kubernetes? \u00b6 Harbor is used in Compliant Kubernetes to provide a secure container registry and a way to manage container image vulnerabilities. Harbor comes packaged with a container image vulnerability scanner that can check if there are any known vulnerabilities in the images you upload to Harbor. The default scanner is Trivy, which provides a comprehensive vulnerability detection both at the OS package and language-specific package levels. Below you can see both an image that has not been scanned and the same image after it has been scanned. After the image is scanned you can see the description, vulnerable package, and severity of each vulnerability as well as if it has been fixed in a later version. You can either scan the images manually or enable automatic scanning whenever a new image is pushed to Harbor, we recommend automatic scanning. In Harbor you can then also restrict so that you can't pull down images that have vulnerabilities of a certain severity or higher. This ensures that you don't accidentally start to use vulnerable images. If you try to deploy a Pod that uses a vulnerable image it will fail to pull the image. When you then inspect the Pod with kubectl describe you will find an error message similar to this: Failed to pull image \"harbor.test.compliantkubernetes.io/test/ubuntu\": rpc error: code = Unknown desc = Error response from daemon: unknown: current image with 77 vulnerabilities cannot be pulled due to configured policy in 'Prevent images with vulnerability severity of \"Medium\" or higher from running.' To continue with pull, please contact your project administrator to exempt matched vulnerabilities through configuring the CVE whitelist. By default we also prevent you from running images from anywhere else than your Harbor instance. This is to ensure that you use all of these security features and don't accidentally pull down vulnerable images from other container registries. We are using Open Policy Agent and Gatekeeper to manage this prevention. If you try to deploy a Pod with an image from another registry you will get an error message similar to this: for: \"unsafe-image.yaml\": admission webhook \"validation.gatekeeper.sh\" denied the request: [denied by require-harbor-repo] container \"unsafe-container\" has an invalid image repo \"unsafe.registry.io/ubuntu\", allowed repos are [\"harbor.test.compliantkubernetes.io\"] Running Example \u00b6 Configure container registry credentials \u00b6 First, retrieve your Harbor CLI secret and configure your local Docker client. In your browser, type harbor.$DOMAIN where $DOMAIN is the information you retrieved from your administrator. Log into Harbor using Single Sign-On (SSO) via OpenID. In the right-top corner, click on your username, then \"User Profile\". Copy your CLI secret. Now log into the container registry: docker login harbor.$DOMAIN . You should see Login Succeeded . Create a registry project \u00b6 Example Here is an example Dockerfile and .dockerignore to get you started. Don't forget to run as non-root. If you haven't already done so, create a project called demo via the Harbor UI, which you have accessed in the previous step. Clone the user demo \u00b6 If you haven't done so already, clone the user demo: git clone https://github.com/elastisys/compliantkubernetes/ cd compliantkubernetes/user-demo Build and push the image \u00b6 REGISTRY_PROJECT = demo # Name of the project, created above TAG = v1 # Container image tag docker build -t harbor. $DOMAIN / $REGISTRY_PROJECT /ck8s-user-demo: $TAG . docker push harbor. $DOMAIN / $REGISTRY_PROJECT /ck8s-user-demo: $TAG You should see no error message. Note down the sha256 of the image. Verification \u00b6 Go to harbor.$DOMAIN . Choose the demo project. Check if the image was uploaded successfully, by comparing the tag's sha256 with the one returned by the docker push command above. (Optional) While you're at it, why not run the vulnerability scanner on the image you just pushed. User access \u00b6 If OIDC was enabled (e.g. DeX) your Harbor user will be created when you first login to the web interface. That user will not have admin privileges, if you need admin rights please contact the administrator by opening a support ticket. Further reading \u00b6 For more information please refer to the official Harbor , Trivy , Open Policy Agent and Gatekeeper documentation.","title":"Container registry"},{"location":"user-guide/setup/","text":"Install Prerequisites \u00b6 As a user, you will need the following before you get started with Compliant Kubernetes: Required software: oidc-login , which helps you log into your Kubernetes cluster via OpenID Connect integration with your Identity Provider of choice Your cluster management software of choice, of which you can choose either or both: kubectl , a command-line tool to help manage your Kubernetes resources Lens , a graphical user interface to help manage your Kubernetes resources (see also our dedicated page on Lens integration ) Optional, but very useful, tools for developers and DevOps engineers: docker , if you want to build (Docker) container images locally helm , if you want to manage your application with the Helm package manager Verify Your Prerequisite Software and its Configuration \u00b6 The easiest way to get started is to request a working installation from a managed Compliant Kubernetes provider . This means you will receive: URLs for the Elastisys Compliant Kubernetes UI components: OpenSearch Dashboards, Grafana, and Harbor; a kubeconfig file for configuring kubectl or Lens access to the underlying Kubernetes cluster; and (optionally and rarely) a static username and password. Note that normally, you should log in via a username and a password of your organization's Identity Provider, such as LDAP, Active Directory, or Google Workspaces account. Make sure you have configured your tools properly: export KUBECONFIG=path/of/kubeconfig.yaml # leave empty if you use the default of ~/.kube/config export DOMAIN= # the domain you received from the administrator To verify if the required tools are installed and work as expected, type: docker version kubectl version --client helm version # You should see the version number of installed tools and no errors. To verify the received KUBECONFIG, type: # Notice that you will be asked to complete browser-based single sign-on kubectl get nodes # You should see the Nodes of your Kubernetes cluster To verify the received URLs, type: curl --head https://dex. $DOMAIN /healthz curl --head https://harbor. $DOMAIN /healthz curl --head https://grafana. $DOMAIN /healthz curl --head https://opensearch. $DOMAIN /api/status curl --insecure --head https://app. $DOMAIN /healthz # Ingress Controller # All commands above should return 'HTTP/2 200'","title":"Install Prerequisites"},{"location":"user-guide/troubleshooting/","text":"Troubleshooting \u00b6 Going through these basic troubleshooting steps should help you as an application developer identify where a problem may lie. If any of these steps do not give the expected \"fine\" output, use kubectl describe to investigate. If you are using Lens instead of the kubectl command-line interface, clicking through your Deployments and Pods will reveal the same information as the commands given below. Is the Kubernetes cluster fine? \u00b6 All Nodes need to have status Ready . kubectl get nodes Are my application Pods fine? \u00b6 Pods should be Running or Completed , and fully Ready (e.g., 1/1 or 6/6 )? kubectl get pods Check your Pods for excessive resource usage: kubectl top pod Inspect application logs and metrics . Are my Deployments fine? \u00b6 Are all Deployments fine? Deployments should show all Pods Ready, Up-to-date and Available (e.g., 2/2 2 2 ). kubectl get deployments Are Helm Releases fine? \u00b6 All Releases should be deployed . helm list --all Are my Certificates fine? \u00b6 All Certificates needs to be Ready. kubectl get certificates","title":"Troubleshooting"},{"location":"user-guide/additional-services/","tags":["BSI IT-Grundschutz APP.4.4.A16"],"text":"Additional Services \u00b6 Compliant Kubernetes simplifies usage of a complex and diverse infrastructure. By exposing simple and uniform concepts, it allows you to focus on application development. However, your application needs more than just running stateless containers. At the very least, you will need a database -- such as PostgreSQL -- to persist data. More complex applications will require a distributed cache -- such as Redis -- to store session information or offload the database. Finally, background tasks are best handled by separate containers, connected to your user-facing backend code via a message queue -- such as RabbitMQ. These additional services need to be delivered as securely as the rest of the platform. Access control, business continuity, disaster recovery, security patching and maintenance need to be a core feature, not an afterthought. It turns out, the same simple and uniform concepts that benefit your application can also be used to simplify hosting additional services. And thanks to security-hardening included in Compliant Kubernetes, the burden of delivering additional services with the security you need is also reduced. Compliant Kubernetes is the \"hourglass waist\" of the platform. Think of it like HTTPS being the \"hourglass waist\" of the Internet: It unites the sprawl of wired and wireless network technologies to offer a uniform concept on which various web, gaming, chat and video streaming protocols can run. In the end, you win by having a feature-full platform to host your application. Not just VMs, but useful services. Administrators win by avoiding to re-invent the wheel and focus on the specifics of each additional service. This section of the user guide will help you benefit the most from the additional services hosted within Compliant Kubernetes.","title":"Overview"},{"location":"user-guide/additional-services/postgresql/","text":"PostgreSQL\u00ae \u00b6 This page will help you succeed in connecting your application to a primary relational database PostgreSQL which meets your security and compliance requirements. Provision a New PostgreSQL Cluster \u00b6 Ask your service-specific administrator to install a PostgreSQL cluster inside your Compliant Kubernetes environment. The service-specific administrator will ensure the PostgreSQL cluster complies with your security requirements, including: Business continuity : We recommend a highly available setup with at minimum a primary instance and a replica. Ideally, the PostgreSQL cluster should be configured with a primary and two replicas. Disaster recovery : Your service-specific administrator will configure the PostgreSQL cluster with physical backups, logical backups and Point-in-Time Recovery (PITR), as required to meet your Recovery Point Objectives . Capacity management : Your service-specific administrator will ensure PostgreSQL runs on dedicated (i.e., tainted) Kubernetes Nodes, as required to get the best performance. Incident management : Your administrator will set up the necessary Probes, dashboards and alerts, to discover issues and resolve them, before they become a problem. Access control : Your administrator will set up a \"root-like\" PostgreSQL account, which will allow you to create databases and PostgreSQL users, but not tamper will logging, business continuity or disaster recovery. Compliant Kubernetes recommends the Zalando PostgreSQL operator . Install Prerequisites \u00b6 Before continuing, make sure you have access to the Kubernetes API, as describe here . Make sure to install the PostgreSQL client on your workstation. On Ubuntu, this can be achieved as follows: sudo apt-get install postgresql-client Getting Access \u00b6 Your administrator will set up a Secret inside Compliant Kubernetes, which contains all information you need to access your PostgreSQL cluster. The Secret has the following shape: apiVersion : v1 kind : Secret metadata : name : $SECRET namespace : $NAMESPACE stringData : # PGHOST represents a cluster-scoped DNS name or IP, which only makes sense inside the Kubernetes cluster. # E.g., postgresql1.postgres-system.svc.cluster.local PGHOST : $PGHOST # These fields map to the environment variables consumed by psql. # Ref https://www.postgresql.org/docs/13/libpq-envars.html PGUSER : $PGUSER PGPASSWORD : $PGPASSWORD PGSSLMODE : $PGSSLMODE # This is the Kubernetes Service to which you need to 'kubectl port-forward' in order to get access to the PostgreSQL cluster from outside the Kubernetes cluster. # E.g., svc/postgresql1 # Ref https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/ USER_ACCESS : $USER_ACCESS Important The Secret is very precious! Prefer not to persist any information extracted from it, as shown below. To extract this information, proceed as follows: export SECRET = # Get this from your administrator export NAMESPACE = # Get this from your administrator export PGHOST = $( kubectl -n $NAMESPACE get secret $SECRET -o 'jsonpath={.data.PGHOST}' | base64 --decode ) export PGUSER = $( kubectl -n $NAMESPACE get secret $SECRET -o 'jsonpath={.data.PGUSER}' | base64 --decode ) export PGPASSWORD = $( kubectl -n $NAMESPACE get secret $SECRET -o 'jsonpath={.data.PGPASSWORD}' | base64 --decode ) export PGSSLMODE = $( kubectl -n $NAMESPACE get secret $SECRET -o 'jsonpath={.data.PGSSLMODE}' | base64 --decode ) export USER_ACCESS = $( kubectl -n $NAMESPACE get secret $SECRET -o 'jsonpath={.data.USER_ACCESS}' | base64 --decode ) Important Do not configure your application with the PostgreSQL admin username and password. Since the application will get too much permission, this will likely violate your access control policy. Create an Application User \u00b6 First, in one console, fetch the information from the access Secret as described above and port forward into the PostgreSQL master. kubectl -n $NAMESPACE port-forward $USER_ACCESS 5432 Important Since humans are bad at generating random passwords, we recommend using pwgen . Second, in another console, fetch the information from the access Secret again and run the PostgreSQL client to create the application database and user: export APP_DATABASE = myapp export APP_USERNAME = myapp export APP_PASSWORD = $( pwgen 32 ) cat <<EOF | psql -d postgres -h 127.0.0.1 \\ --set=APP_DATABASE=$APP_DATABASE \\ --set=APP_USERNAME=$APP_USERNAME \\ --set=APP_PASSWORD=$APP_PASSWORD create database :APP_DATABASE; create user :APP_USERNAME with encrypted password ':APP_PASSWORD'; grant all privileges on database :APP_DATABASE to :APP_USERNAME; EOF Continue with the second console in the next section to create a Secret with this information. Create an Application Secret \u00b6 First, check that you are on the right Compliant Kubernetes cluster, in the right application namespace: kubectl get nodes kubectl config view --minify --output 'jsonpath={..namespace}' ; echo Now, create a Kubernetes Secret in your application namespace to store the PostgreSQL application username and password. For consistency, prefer sticking to naming connection parameters as the environment variables consumed by psql . cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: app-postgresql-secret type: Opaque stringData: PGHOST: ${PGHOST} PGPORT: '5432' PGSSLMODE: ${PGSSLMODE} PGUSER: ${APP_USERNAME} PGPASSWORD: ${APP_PASSWORD} PGDATABASE: ${APP_DATABASE} EOF Warning Although most client libraries follow the libpq definition of these environment variables, some do not, and this will require changes to the application Secret. Notably node-postgres does not currently do so for PGSSLMODE . When this variable is set to require , it will do a full verification instead, requiring access to the PostgreSQL certificates to allow a connection. To get the intended mode for require set the variable to no-verify instead. Expose PostgreSQL credentials to Your Application \u00b6 To expose the PostgreSQL cluster credentials to your application, follow one of the following upstream documentation: Create a Pod that has access to the secret data through a Volume Define container environment variables using Secret data Follow the Go-Live Checklist \u00b6 You should be all set. Before going into production, don't forget to go through the go-live checklist . CK8S PostgreSQL Release Notes \u00b6 Check out the release notes for the PostgreSQL cluster that runs in Compliant Kubernetes environments! Further Reading \u00b6 Creating users Creating databases Granting permissions Kubernetes Secrets","title":"PostgreSQL"},{"location":"user-guide/additional-services/rabbitmq/","text":"RabbitMQ\u00ae \u00b6 This page will help you succeed in connecting your application to a RabbitMQ-based message queue which meets your security and compliance requirements. Provision a New RabbitMQ Cluster \u00b6 Ask your service-specific administrator to install a RabbitMQ cluster inside your Compliant Kubernetes environment. The service-specific administrator will ensure the RabbitMQ cluster complies with your security requirements, including: Business continuity : We recommend a highly available setup with three instances. Disaster recovery : Note that, because messages are often short-lived and possibly transient, backing them up from under a running node is highly discouraged and can lead to an inconsistent snapshot of the data. Generally, disaster recovery for message queue only makes sense for the dead letter exchange . Capacity management : Your service-specific administrator will ensure RabbitMQ runs on dedicated (i.e., tainted) Kubernetes Nodes, as required to get the best performance. Incident management : Your administrator will set up the necessary Probes, dashboards and alerts, to discover issues and resolve them, before they become a problem. Access control : Your service-specific administrator will hand you the RabbitMQ administrator username and password. This will allow you to declare exchanges, queues, bindings, users, virtual hosts and user permissions, as required. Compliant Kubernetes recommends the RabbitMQ Cluster Operator for Kubernetes . Install Prerequisites \u00b6 Before continuing, make sure you have access to the Kubernetes API, as describe here . Make sure to install the RabbitMQ client on your workstation. On Ubuntu, this can be achieved as follows: sudo apt install rabbitmq-server Getting Access to the Management UI \u00b6 Your administrator will set up a Secret inside Compliant Kubernetes, which contains all information you need to access your RabbitMQ cluster. The Secret has the following shape: apiVersion : v1 kind : Secret metadata : name : ${RABBITMQ_CLUSTER}-default-user namespace : ${RABBITMQ_NAMESPACE} stringData : username : ${RABBITMQ_ADMIN_USERNAME} password : ${RABBITMQ_ADMIN_PASSWORD} Important The Secret is very precious! Prefer not to persist any information extracted from it, as shown below. To access the management UI, proceed as follows: Retrieve the admin default username and password export RABBITMQ_CLUSTER = # Get this from your administrator export RABBITMQ_NAMESPACE = # Get this from your administrator echo -n \"RabbitMQ admin username: \" kubectl -n rabbitmq-system get secret rabbitmq-cluster-default-user -o jsonpath = \"{.data.username}\" | base64 --decode && echo echo -n \"RabbitMQ admin password: \" kubectl -n rabbitmq-system get secret rabbitmq-cluster-default-user -o jsonpath = \"{.data.password}\" | base64 --decode && echo Important Do not configure your application with the RabbitMQ admin username and password. Since the application will get too much permission, this will likely violate your access control policy. Start the port-forwarding: kubectl port-forward -n ${ RABBITMQ_NAMESPACE } svc/ ${ RABBITMQ_CLUSTER } 15672 Open the admin dashboard (at http://localhost:15672) and log in using the credentials retrieved in step 1. Create an application username, password and vhost, and store these in variables as named below: APP_USER = APP_PASS = APP_VHOST = Create a Application Secret \u00b6 First, check that you are on the right Compliant Kubernetes cluster, in the right application namespace: kubectl get nodes kubectl config view --minify --output 'jsonpath={..namespace}' ; echo Now, create a Kubernetes Secret in your application namespace to store the AMPQ URL: cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: app-amqp-secret type: Opaque stringData: AMQP_URL: amqp://${APP_USER}:${APP_PASS}@${RABBITMQ_CLUSTER}.${RABBITMQ_NAMESPACE}/${APP_VHOST} EOF Expose AMQP URL to Your Application \u00b6 To expose the AMQP URL to your application, follow one of the following upstream documentation: Create a Pod that has access to the secret data through a Volume Define container environment variables using Secret data Follow the Go-Live Checklist \u00b6 You should be all set. Before going into production, don't forget to go through the go-live checklist . CK8S RabbitMQ Release Notes \u00b6 Check out the release notes for the RabbitMQ cluster that runs in Compliant Kubernetes environments! Further Reading \u00b6 RabbitMQ Management UI AMQP URL spec AMQP Clients Kubernetes Secrets","title":"RabbitMQ"},{"location":"user-guide/additional-services/redis/","text":"Redis\u2122 \u00b6 This page will help you succeed in connecting your application to a low-latency in-memory cache Redis which meets your security and compliance requirements. Provision a New Redis Cluster \u00b6 Ask your service-specific administrator to install a Redis cluster inside your Compliant Kubernetes environment. The service-specific administrator will ensure the Redis cluster complies with your security requirements, including: Business continuity : We recommend a highly available setup with at minimum three instances. The Redis client library that you use in your application needs to support Redis Sentinel . Notice that clients with Sentinel support need extra steps to discover the Redis primary . Capacity management : Your service-specific administrator will ensure Redis has enough capacity to meet your needs. Incident management : Your administrator will set up the necessary Probes, dashboards and alerts, to discover issues and resolve them, before they become a problem. Important: Improve Access Control with NetworkPolicies Please note the follow information about Redis access control from the upstream documentation: Redis is designed to be accessed by trusted clients inside trusted environments. For improved security, discuss with your service-specific administrator what Pods and/or Namespaces need access to the Redis cluster. They can then set up the necessary NetworkPolicies . Important: No Disaster Recovery We do not recommend using Redis as primary database. Redis should be used to store: Cached data: If this is lost, this data can be quickly retrieved from the primary database, such as the PostgreSQL cluster. Session state: If this is lost, the user experience might be impacted -- e.g., the user needs to re-login -- but no data should be lost. Compliant Kubernetes recommends the Spotahome operator . Install Prerequisites \u00b6 Before continuing, make sure you have access to the Kubernetes API, as describe here . Make sure to install the Redis client on your workstation. On Ubuntu, this can be achieved as follows: sudo apt install redis-tools Getting Access \u00b6 Your administrator will set up a Secret inside Compliant Kubernetes, which contains all information you need to access your Redis cluster. The Secret has the following shape: apiVersion : v1 kind : Secret metadata : name : $SECRET namespace : $NAMESPACE stringData : # REDIS_SENTINEL_HOST represents a cluster-scoped Redis Sentinel host, which only makes sense inside the Kubernetes cluster. # E.g., rfs-redis-cluster.redis-system REDIS_SENTINEL_HOST : $REDIS_SENTINEL_HOST # REDIS_SENTINEL_PORT represents a cluster-scoped Redis Sentinel port, which only makes sense inside the Kubernetes cluster. # E.g., 26379 REDIS_SENTINEL_PORT : \"$REDIS_SENTINEL_PORT\" Important The Secret is very precious! Prefer not to persist any information extracted from it, as shown below. To extract this information, proceed as follows: export SECRET = # Get this from your administrator export NAMESPACE = # Get this from your administrator export REDIS_SENTINEL_HOST = $( kubectl -n $NAMESPACE get secret $SECRET -o 'jsonpath={.data.REDIS_SENTINEL_HOST}' | base64 -d ) export REDIS_SENTINEL_PORT = $( kubectl -n $NAMESPACE get secret $SECRET -o 'jsonpath={.data.REDIS_SENTINEL_PORT}' | base64 -d ) Important At the time of this writing, we do not recommend to use a Redis cluster in a multi-tenant fashion. One Redis cluster should have only one purpose. Create a Secret \u00b6 First, check that you are on the right Compliant Kubernetes cluster, in the right application namespace: kubectl get nodes kubectl config view --minify --output 'jsonpath={..namespace}' ; echo Now, create a Kubernetes Secret in your application namespace to store the Redis Sentinel connection parameters: cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: app-redis-secret type: Opaque stringData: REDIS_SENTINEL_HOST: $REDIS_SENTINEL_HOST REDIS_SENTINEL_PORT: \"$REDIS_SENTINEL_PORT\" EOF Expose Redis Connection Parameters to Your Application \u00b6 To expose the Redis cluster to your application, follow one of the following upstream documentation: Create a Pod that has access to the secret data through a Volume Define container environment variables using Secret data Important Make sure to use a Redis client library with Sentinel support. For example: Django-Redis Client that supports Sentinel Cluster HA If the linked code example doesn't work, try LOCATION: redis://mymaster/db . Follow the Go-Live Checklist \u00b6 You should be all set. Before going into production, don't forget to go through the go-live checklist . CK8S Redis Release Notes \u00b6 Check out the release notes for the Redis cluster that runs in Compliant Kubernetes environments! Further Reading \u00b6 Redis Sentinel Guidelines for Redis clients with support for Redis Sentinel Redis Commands Kubernetes Secrets","title":"Redis"},{"location":"user-guide/additional-services/timescaledb/","text":"TimescaleDB\u00ae \u00b6 This page will help you succeed in connecting your application to a primary relational database TimescaleDB which meets your security and compliance requirements. TimescaleDB is an extension on top of our managed PostgreSQL. This means that your administrator will be setting up a complete PostgreSQL cluster for you and you just use it for TimescaleDB via the TimescaleDB extension. Note TimescaleDB is not a viable option for collecting all metrics from the Kubernetes cluster. The data is uncompressed and would take a lot of space to store and use a lot of resources to analyze, unless you want to use it with a very short retention period. This is not usually a problem for collecting application specific metrics, since they are not as many as the metrics that are generated from the Kubernetes cluster. Important Due to very different performance-tuning characteristics, Timescale and PostgreSQL databases should never run on the same PostgreSQL cluster. To comply with this, it is essential that every PostgreSQL database that gets created on the PostgreSQL cluster also has the Timescale extension created for it. If you want to use TimescaleDB on your Compliant Kubernetes cluster, ask your administrator to provision a new standard PostgreSQL cluster inside your Compliant Kubernetes environment. Then set up the TimescaleDB extension. Provision a New PostgreSQL Cluster \u00b6 Ask your service-specific administrator to install a PostgreSQL cluster inside your Compliant Kubernetes environment. The service-specific administrator will ensure the PostgreSQL cluster complies with your security requirements, including: Business continuity : We recommend a highly available setup with at minimum a primary instance and a replica. Ideally, the PostgreSQL cluster should be configured with a primary and two replicas. Disaster recovery : Your service-specific administrator will configure the PostgreSQL cluster with physical backups, logical backups and Point-in-Time Recovery (PITR), as required to meet your Recovery Point Objectives . Capacity management : Your service-specific administrator will ensure PostgreSQL runs on dedicated (i.e., tainted) Kubernetes Nodes, as required to get the best performance. Incident management : Your administrator will set up the necessary Probes, dashboards and alerts, to discover issues and resolve them, before they become a problem. Access control : Your administrator will set up a \"root-like\" PostgreSQL account, which will allow you to create databases and PostgreSQL users, but not tamper will logging, business continuity or disaster recovery. Compliant Kubernetes recommends the Zalando PostgreSQL operator . Install Prerequisites \u00b6 Before continuing, make sure you have access to the Kubernetes API, as describe here . Make sure to install the PostgreSQL client on your workstation. On Ubuntu, this can be achieved as follows: sudo apt-get install postgresql-client Getting Access \u00b6 Your administrator will set up a Secret inside Compliant Kubernetes, which contains all information you need to access your PostgreSQL cluster. The Secret has the following shape: apiVersion : v1 kind : Secret metadata : name : $SECRET namespace : $NAMESPACE stringData : # PGHOST represents a cluster-scoped DNS name or IP, which only makes sense inside the Kubernetes cluster. # E.g., postgresql1.postgres-system.svc.cluster.local PGHOST : $PGHOST # These fields map to the environment variables consumed by psql. # Ref https://www.postgresql.org/docs/13/libpq-envars.html PGUSER : $PGUSER PGPASSWORD : $PGPASSWORD PGSSLMODE : $PGSSLMODE # This is the Kubernetes Service to which you need to 'kubectl port-forward' in order to get access to the PostgreSQL cluster from outside the Kubernetes cluster. # E.g., svc/postgresql1 # Ref https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/ USER_ACCESS : $USER_ACCESS Important The Secret is very precious! Prefer not to persist any information extracted from it, as shown below. To extract this information, proceed as follows: export SECRET = # Get this from your administrator export NAMESPACE = # Get this from your administrator export PGHOST = $( kubectl -n $NAMESPACE get secret $SECRET -o 'jsonpath={.data.PGHOST}' | base64 --decode ) export PGUSER = $( kubectl -n $NAMESPACE get secret $SECRET -o 'jsonpath={.data.PGUSER}' | base64 --decode ) export PGPASSWORD = $( kubectl -n $NAMESPACE get secret $SECRET -o 'jsonpath={.data.PGPASSWORD}' | base64 --decode ) export PGSSLMODE = $( kubectl -n $NAMESPACE get secret $SECRET -o 'jsonpath={.data.PGSSLMODE}' | base64 --decode ) export USER_ACCESS = $( kubectl -n $NAMESPACE get secret $SECRET -o 'jsonpath={.data.USER_ACCESS}' | base64 --decode ) Important Do not configure your application with the PostgreSQL admin username and password. Since the application will get too much permission, this will likely violate your access control policy. Create an Application User \u00b6 First, in one console, fetch the information from the access Secret as described above and port forward into the PostgreSQL master. kubectl -n $NAMESPACE port-forward $USER_ACCESS 5432 Important Since humans are bad at generating random passwords, we recommend using pwgen . Second, in another console, fetch the information from the access Secret again and run the PostgreSQL client to create the application database and user: export APP_DATABASE = myapp export APP_USERNAME = myapp export APP_PASSWORD = $( pwgen 32 ) cat <<EOF | psql -d postgres -h 127.0.0.1 \\ --set=APP_DATABASE=$APP_DATABASE \\ --set=APP_USERNAME=$APP_USERNAME \\ --set=APP_PASSWORD=$APP_PASSWORD create database :APP_DATABASE; create user :APP_USERNAME with encrypted password ':APP_PASSWORD'; grant all privileges on database :APP_DATABASE to :APP_USERNAME; EOF Continue with the second console in the next section to create a Secret with this information. Create an Application Secret \u00b6 First, check that you are on the right Compliant Kubernetes cluster, in the right application namespace: kubectl get nodes kubectl config view --minify --output 'jsonpath={..namespace}' ; echo Now, create a Kubernetes Secret in your application namespace to store the PostgreSQL application username and password. For consistency, prefer sticking to naming connection parameters as the environment variables consumed by psql . cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: app-postgresql-secret type: Opaque stringData: PGHOST: ${PGHOST} PGPORT: '5432' PGSSLMODE: ${PGSSLMODE} PGUSER: ${APP_USERNAME} PGPASSWORD: ${APP_PASSWORD} PGDATABASE: ${APP_DATABASE} EOF Warning Although most client libraries follow the libpq definition of these environment variables, some do not, and this will require changes to the application Secret. Notably node-postgres does not currently do so for PGSSLMODE . When this variable is set to require , it will do a full verification instead, requiring access to the PostgreSQL certificates to allow a connection. To get the intended mode for require set the variable to no-verify instead. Expose PostgreSQL credentials to Your Application \u00b6 To expose the PostgreSQL cluster credentials to your application, follow one of the following upstream documentation: Create a Pod that has access to the secret data through a Volume Define container environment variables using Secret data Set up the TimescaleDB extension on PostgreSQL \u00b6 Connect to the created database: \\c $APP_DATABASE Add the TimescaleDB extension: CREATE EXTENSION IF NOT EXISTS timescaledb ; Follow the Go-Live Checklist \u00b6 You should be all set. Before going into production, don't forget to go through the go-live checklist . CK8S TimescaleDB Release Notes \u00b6 Check out the release notes for the TimescaleDB/PostgreSQL cluster that runs in Compliant Kubernetes environments! Further Reading \u00b6 Getting started with Timescale Creating users Creating databases - Remember to create Timescale extension on the new databases. Granting permissions Kubernetes Secrets","title":"TimescaleDB"},{"location":"user-guide/safeguards/","text":"Safeguards \u00b6 \"Det ska vara l\u00e4tt att g\u00f6ra r\u00e4tt.\" (English: \"It should be easy to do it right.\") We know you care about the security and uptime of your application. But all that effort goes wasted if the platform allows you to make trivial mistakes. That is why Compliant Kubernetes is built with various safeguards, to allow you to make security and reliability easy for you. Relevant Regulations \u00b6 GDPR Article 32 : Taking into account the state of the art [...] the controller and the processor shall implement [...] as appropriate [...] a process for regularly testing, assessing and evaluating the effectiveness of technical and organisational measures for ensuring the security of the processing. In assessing the appropriate level of security account shall be taken in particular of the risks that are presented by processing, in particular from accidental or unlawful destruction, loss, alteration, unauthorised disclosure of, or access to personal data transmitted , stored or otherwise processed. [highlights added]","title":"Overview"},{"location":"user-guide/safeguards/enforce-networkpolicies/","tags":["ISO 27001 A.13.1.1","ISO 27001 A.13.1.2","ISO 27001 A.13.1.3","BSI IT-Grundschutz APP.4.4.A7","BSI IT-Grundschutz APP.4.4.A18"],"text":"Reduce blast radius: NetworkPolicies \u00b6 Note This section helps you implement ISO 27001, specifically: A.13.1.1 Network Controls A.13.1.2 Security of Network Services A.13.1.3 Segregation in Networks Important This safeguard is enabled by default since Compliant Kubernetes apps v0.19.0 . NetworkPolicies are useful in two cases: segregating tenants hosted in the same environment and further segregating application components. Both help you achieve better data protection. Segregating tenants hosted in the same environment \u00b6 Say you want to host a separate instance of your application for each tenant. For example, your end-users may belong to different -- potentially competing -- organizations, and you promised them to take extra care of not mixing their data. Say you want to reduce complexity by hosting all tenants inside the same environment, but without compromising data protection. Each application instance could be installed as a separate Helm Release, perhaps even in its own Namespace. These instances should be segregated from other application instances using NetworkPolicies. This insures that network traffic from one application instance cannot reach another application instance. Besides reducing attack surface, it also prevents embarrassing mistakes, like connecting one application to the database of another. Further segregation of application components \u00b6 If you run several applications -- e.g., frontend, backend, backoffice, database, message queue -- in a single Kubernetes cluster, it is a best practice to segregrate them. By segregating your applications and only allowing required ingress and egress network traffic, you further reduce blast radius in case of an attack. Compliant Kubernetes helps enforce segregation \u00b6 Compliant Kubernetes allows you to segregate applications by installing suitable NetworkPolicies. These are a bit like firewalls, but in the container world: Since containers are supposed to be deleted and recreated frequently, they change IP address a lot. Clearly the old \"allow/deny IP\" method does not scale. Therefore, NetworkPolicies select source and destination Pods based on labels or namespace labels. To make sure you don't forget to configure NetworkPolicies, the administrator can configure Compliant Kubernetes to deny creation of Pods with no matching NetworkPolicies. If you get the following error: Error: admission webhook \"validation.gatekeeper.sh\" denied the request: [denied by require-networkpolicy] No matching networkpolicy found Then you are missing NetworkPolicies which select your Pods. The user demo gives a good example to get you started. Further Reading \u00b6 NetworkPolicies","title":"Enforce NetworkPolicies"},{"location":"user-guide/safeguards/enforce-no-root/","tags":["ISO 27001 A.9.4.4","ISO 27001 A.12.6.1","ISO 27001 A.14.2.5"],"text":"Reduce blast radius: Preventing forgotten roots \u00b6 Note This section helps you implement ISO 27001, specifically: A.9.4.4 Use of Privileged Utility Programmes A.12.6.1 Management of Technical Vulnerabilities A.14.2.5 Secure System Engineering Principles Many container runtimes and operating system vulnerabilities need code running as root to become a threat. To minimize this risk, application should only run as root when strictly necessary. Unfortunately, many Dockerfiles -- and container base images -- today are shipped running as root by default. This makes it easy to slip code running as root into production, exposing data to unnecessary risks. To reduce blast radius, Compliant Kubernetes will protect you from accidentally deploying application running as root. How to solve: CreateContainerConfigError \u00b6 You may encounter the following issue: $ kubectl get pods NAME READY STATUS RESTARTS AGE myapp-ck8s-user-demo-564f8dd85-2bs8r 0/1 CreateContainerConfigError 0 84s myapp-ck8s-user-demo-bfbf9c459-dmk4l 0/1 CreateContainerConfigError 0 13m $ kubectl describe pods myapp-ck8s-user-demo-564f8dd85-2bs8r [...] Error: container has runAsNonRoot and image has non-numeric user (node), cannot verify user is non-root (pod: \"myapp-ck8s-user-demo-bfbf9c459-dmk4l_demo1(1b53b1a8-4845-4db5-aecf-6bebcc54e396)\", container: ck8s-user-demo) This means that your Dockerfile uses a non-numeric user and Kubernetes cannot validate whether the image truly runs as non-root. Alternatively, you may get: $ kubectl describe pods myapp-ck8s-user-demo-564f8dd85-2bs8r [...] Error: container has runAsNonRoot and image will run as root (pod: \"myapp-ck8s-user-demo-564f8dd85-2bs8r_demo1(a55a25f3-7b77-4fae-9f92-11e264446ecc)\", container: ck8s-user-demo) This means that your Dockerfile has no USER directive and your application would run as root. To ensure your application does not run as root, you have two options: Change the Dockerfile to USER 1000 or whatever numeric ID corresponds to your user. This is what the user demo does . Add the following snippet to the spec of your Pod manifest: securityContext : runAsUser : 1000 If possible, prefer changing the Dockerfile, to ensure your application runs as non-root not only in production, but also during development and testing. The smaller the difference between development, testing and production, the fewer surprises down the time. Further Reading \u00b6 Dockerfile USER SecurityContext","title":"Enforce No Root"},{"location":"user-guide/safeguards/enforce-resources/","tags":["ISO 27001 A.12.1.3"],"text":"Avoid downtime with Resource Requests and Limits \u00b6 Note This section helps you implement ISO 27001, specifically: A.12.1.3 Capacity Management Important This safeguard is enabled by default since Compliant Kubernetes apps v0.19.0 . Problem \u00b6 A major source of application downtime is insufficient capacity. For example, if a Node reaches 100% CPU utilization, then application Pods hosted on it will run slow, leading to bad end-user experience. If a Node runs into memory pressure, the application will run slower, as less memory is available for the page cache . High memory pressure may lead to the Node triggering the infamous Out-of-Memory (OOM) Killer , killing a victim, either your application or a platform component. Solution \u00b6 To avoid running into capacity issues, Kubernetes allows Pods to specify resource requests and limits for each of its containers. This achieves two benefits: It ensures that Pods are scheduled to Nodes that have the requested resources. It ensures that a Pod does not exceed its resource limits, hence limiting its blast radius and protecting other application or platform Pods. How Does Compliant Kubernetes Help? \u00b6 To make sure you don't forget to configure resource requests and limits, the administrator can configure Compliant Kubernetes to deny creation of Pods without explicit resource specifications. If you get the following error: Error: UPGRADE FAILED: failed to create resource: admission webhook \"validation.gatekeeper.sh\" denied the request: [denied by require-resource-requests] Container \"ck8s-user-demo\" has no resource requests Then you are missing resource requests for some containers of your Pods. The user demo gives a good example to get you started. Further Reading \u00b6 Managing Resources for Containers","title":"Enforce Resources"},{"location":"user-guide/safeguards/enforce-trusted-registries/","tags":["ISO 27001 A.12.6.1"],"text":"Avoid vulnerable container images \u00b6 Note This section helps you implement ISO 27001, specifically: A.12.6.1 Management of Technical Vulnerabilities Important This safeguard is enabled by default since Compliant Kubernetes apps v0.19.0 . Problem \u00b6 A healthy security posture requires you to ensure your code has no known vulnerabilities. Compliant Kubernetes comes with a registry which includes vulnerability scanning of container images. It can even be configured to prevent the Kubernetes cluster from pulling images with vulnerabilities above a set criticality. This is a per-project setting, so you could, for example, have a stricter policy for publicly facing application components -- e.g., the front office -- and a less strict policy for internal application components -- e.g., the back office. Public container registry, such as Docker Hub and Quay, might not stick to the vulnerability management you require, perhaps being at times too strict or too loose. Solution \u00b6 You can designate a set of registries, a project within a registry or specific container images as trusted. By this you declared that you did a risk analysis and determined that they fulfill your security requirements. How Does Compliant Kubernetes Help? \u00b6 Your administrator can configure Compliant Kubernetes to technically enforce a set of trusted container registries. This means that if you accidentally reference an image in an untrusted registry, you will get the following error: Error: admission webhook \"validation.gatekeeper.sh\" denied the request: [denied by require-harbor-repo] container \"ck8s-user-demo\" has an invalid image repo \"harbor.example.com/demo/ck8s-user-demo:1.16.0\", allowed repos are [\"harbor.cksc.a1ck.io\"] The resolution is rather simple. You have two options: Change the container image to point to a trusted registry. Get in touch with your administrator and discuss augmenting the set of trusted registries. Important Instead of adding a not-really-trusted registry to the set of trusted registries, prefer mirroring some public images in your Compliant Kubernetes registry. Further Reading \u00b6 Container Images Harbor Vulnerability Scanning","title":"Enforce Trusted Registries"},{"location":"ciso-guide/controls/","text":"ISO 27001 and BSI IT-Grundschutz Controls \u00b6 Click on the links below to navigate the documentation by ISO 27001 control: BSI IT-Grundschutz APP.4.4.A10 \u00b6 CI/CD BSI IT-Grundschutz APP.4.4.A14 \u00b6 Use Dedicated Nodes for Additional Services BSI IT-Grundschutz APP.4.4.A16 \u00b6 Overview BSI IT-Grundschutz APP.4.4.A18 \u00b6 Network Model Enforce NetworkPolicies BSI IT-Grundschutz APP.4.4.A2 \u00b6 CI/CD BSI IT-Grundschutz APP.4.4.A21 \u00b6 Maintenance Prepare Your Application BSI IT-Grundschutz APP.4.4.A3 \u00b6 How to Delegate Demarcation BSI IT-Grundschutz APP.4.4.A5 \u00b6 Backup Backups BSI IT-Grundschutz APP.4.4.A7 \u00b6 Network Model Enforce NetworkPolicies ISO 27001 A.10.1.2 \u00b6 Cryptography Network Model ISO 27001 A.12.1.3 \u00b6 Capacity Management Metrics Enforce Resources ISO 27001 A.12.2.1 \u00b6 Intrusion Detection ISO 27001 A.12.3.1 \u00b6 Backup Disaster Recovery Backups ISO 27001 A.12.4.1 \u00b6 Log Review Logs ISO 27001 A.12.4.3 \u00b6 Audit Logs Log Review Logs ISO 27001 A.12.4.4 \u00b6 Provider Audit ISO 27001 A.12.6.1 \u00b6 Intrusion Detection Vulnerability Management Overview Maintenance Prepare Your Application Enforce No Root Enforce Trusted Registries ISO 27001 A.13 \u00b6 Network Security ISO 27001 A.13.1.1 \u00b6 Network Model Enforce NetworkPolicies ISO 27001 A.13.1.2 \u00b6 Network Model Enforce NetworkPolicies ISO 27001 A.13.1.3 \u00b6 Network Model Enforce NetworkPolicies ISO 27001 A.14.1.1 \u00b6 Architectural Decision Log ISO 27001 A.14.2.5 \u00b6 Enforce No Root ISO 27001 A.15 \u00b6 Provider Audit ISO 27001 A.16 \u00b6 Metric Alerts Logs ISO 27001 A.16.1 \u00b6 Metrics ISO 27001 A.16.1.7 \u00b6 Intrusion Detection ISO 27001 A.17 \u00b6 We believe in community-driven open source ISO 27001 A.17.1.1 \u00b6 Backup Disaster Recovery ISO 27001 A.18.2.2 \u00b6 Policy-as-Code ISO 27001 A.18.2.3 \u00b6 Policy-as-Code ISO 27001 A.9.4.1 \u00b6 How to Delegate ISO 27001 A.9.4.4 \u00b6 Enforce No Root Other IT-Grundschutz Controls \u00b6 APP.4.4.A17 Attestierung von Nodes (H) \u00b6 The Kubespray layer in Compliant Kubernetes ensures that Data Plane Nodes and Control Plane Nodes are mutually authenticated via mutual TLS. BSI IT-Grundschutz Controls outside the scope of Compliant Kubernetes \u00b6 Pending official translation into English, the controls are written in German. APP.4.4.A1 Planung der Separierung der Anwendungen (B) \u00b6 Compliant Kubernetes recommends to setting up at least two separate environment: one for testing and one for production. APP.4.4.A6 Initialisierung von Pods (S) \u00b6 Application developers must make sure that initialization happens in init containers . APP.4.4.A11 \u00dcberwachung der Container (S) \u00b6 Application developers must ensure that their application has a liveliness and readiness probe, which are configured in the Deployment. This is illustrated by our user demo . APP.4.4.A12 Absicherung der Infrastruktur-Anwendungen (S) \u00b6 This requirement essentially states that the Compliant Kubernetes environments are only as secure as the infrastructure around them. Make sure you have a proper IT policy in place. Regularly review the systems where you store backups and configuration of Compliant Kubernetes. APP.4.4.A13 Automatisierte Auditierung der Konfiguration (H) \u00b6 Compliant Kubernetes administrators must regularly audit the configuration of their environments. We recommend doing this on a quarterly basis. APP.4.4.A20 Verschl\u00fcsselte Datenhaltung bei Pods (H) \u00b6 Compliant Kubernetes recommends disk encryption to be provided at the infrastructure level. If you have this requirement, check for full-disk encryption via the provider audit .","title":"ISO 27001 and BSI IT-Grundschutz"}]}